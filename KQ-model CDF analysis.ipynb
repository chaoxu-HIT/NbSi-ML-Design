{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1a3be9-50e6-4096-8c82-e5b1cbd2941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "æ¨¡å‹é¢„æµ‹è¯¯å·®CDFåˆ†æç¨‹åº - é€‚é…ä¿®å¤ç‰ˆ\n",
      "============================================================\n",
      "è¾“å…¥è·¯å¾„: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\æ¨¡å‹è®­ç»ƒ_K45_ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹_æ— è´å¶æ–¯ä¼˜åŒ–-7.29.3æœ€ç»ˆ_ä¿®å¤ç‰ˆ\n",
      "è¾“å‡ºè·¯å¾„: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\CDFåˆ†æ8.1\n",
      "æ—¶é—´æˆ³: 2025-08-01 11:09:28\n",
      "============================================================\n",
      "âœ“ è¾“å‡ºç›®å½•å·²åˆ›å»º: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\CDFåˆ†æ8.1\n",
      "=== æ­£åœ¨åŠ è½½æ¨¡å‹æ•°æ® ===\n",
      "åŠ è½½æ¨¡å‹æ–‡ä»¶: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\æ¨¡å‹è®­ç»ƒ_K45_ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹_æ— è´å¶æ–¯ä¼˜åŒ–-7.29.3æœ€ç»ˆ_ä¿®å¤ç‰ˆ\\best_models_physics_guided_no_bayesian_fixed.pkl\n",
      "åŠ è½½æ•°æ®æ–‡ä»¶: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\æ¨¡å‹è®­ç»ƒ_K45_ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹_æ— è´å¶æ–¯ä¼˜åŒ–-7.29.3æœ€ç»ˆ_ä¿®å¤ç‰ˆ\\data_info_physics_guided_no_bayesian_fixed.pkl\n",
      "åŠ è½½ç‰¹å¾æ–‡ä»¶: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\æ¨¡å‹è®­ç»ƒ_K45_ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹_æ— è´å¶æ–¯ä¼˜åŒ–-7.29.3æœ€ç»ˆ_ä¿®å¤ç‰ˆ\\features_name_physics_guided_no_bayesian_fixed.pkl\n",
      "âœ“ æˆåŠŸåŠ è½½ 11 ä¸ªæ¨¡å‹\n",
      "âœ“ æ•°æ®å½¢çŠ¶ä¿¡æ¯: X(221, 40), y(221,)\n",
      "âœ“ ç‰¹å¾æ•°é‡: 40\n",
      "\n",
      "=== æ­£åœ¨è®¡ç®—æ¨¡å‹é¢„æµ‹è¯¯å·® ===\n",
      "  [1/11] å¤„ç†æ¨¡å‹: PCC_k=5_GradientBoosting\n",
      "    âœ“ RÂ²=0.9318, MAE=2.0637, å¹³å‡ç›¸å¯¹è¯¯å·®=9.16%\n",
      "  [2/11] å¤„ç†æ¨¡å‹: F-Regression_k=5_GradientBoosting\n",
      "    âœ“ RÂ²=0.9197, MAE=2.0991, å¹³å‡ç›¸å¯¹è¯¯å·®=9.16%\n",
      "  [3/11] å¤„ç†æ¨¡å‹: RFE_k=4_GradientBoosting\n",
      "    âœ“ RÂ²=0.9177, MAE=2.2332, å¹³å‡ç›¸å¯¹è¯¯å·®=9.44%\n",
      "  [4/11] å¤„ç†æ¨¡å‹: Mutual Information_k=5_GradientBoosting\n",
      "    âœ“ RÂ²=0.9157, MAE=2.1871, å¹³å‡ç›¸å¯¹è¯¯å·®=9.29%\n",
      "  [5/11] å¤„ç†æ¨¡å‹: Lasso_k=4_GradientBoosting\n",
      "    âœ“ RÂ²=0.9149, MAE=2.1957, å¹³å‡ç›¸å¯¹è¯¯å·®=10.98%\n",
      "  [6/11] å¤„ç†æ¨¡å‹: SFS_k=4_GradientBoosting\n",
      "    âœ“ RÂ²=0.9103, MAE=2.1015, å¹³å‡ç›¸å¯¹è¯¯å·®=9.14%\n",
      "  [7/11] å¤„ç†æ¨¡å‹: RandomForest_k=4_GradientBoosting\n",
      "    âœ“ RÂ²=0.9082, MAE=2.1780, å¹³å‡ç›¸å¯¹è¯¯å·®=9.82%\n",
      "  [8/11] å¤„ç†æ¨¡å‹: SFS_k=4_Bagging\n",
      "    âœ“ RÂ²=0.9080, MAE=2.2937, å¹³å‡ç›¸å¯¹è¯¯å·®=8.12%\n",
      "  [9/11] å¤„ç†æ¨¡å‹: SFS_k=5_GradientBoosting\n",
      "    âœ“ RÂ²=0.9071, MAE=2.0734, å¹³å‡ç›¸å¯¹è¯¯å·®=9.13%\n",
      "  [10/11] å¤„ç†æ¨¡å‹: SBS_k=4_GradientBoosting\n",
      "    âœ“ RÂ²=0.9054, MAE=2.5126, å¹³å‡ç›¸å¯¹è¯¯å·®=9.49%\n",
      "  [11/11] å¤„ç†æ¨¡å‹: GA_k=4_GradientBoosting\n",
      "    âœ“ RÂ²=0.9051, MAE=2.1520, å¹³å‡ç›¸å¯¹è¯¯å·®=9.45%\n",
      "âœ“ æˆåŠŸå¤„ç† 11 ä¸ªæ¨¡å‹\n",
      "\n",
      "=== ä¿å­˜æ•°æ®æ–‡ä»¶ ===\n",
      "âœ“ ä¿å­˜ç›¸å¯¹è¯¯å·®æ•°æ®: model_relative_errors.pkl\n",
      "âœ“ ä¿å­˜æ¨¡å‹ä¿¡æ¯: model_info.pkl\n",
      "âœ“ ä¿å­˜è¯¦ç»†ç»Ÿè®¡: detailed_statistics.pkl\n",
      "âœ“ ä¿å­˜æ±‡æ€»CSV: model_summary_with_errors.csv\n",
      "âœ“ ä¿å­˜ä¸ªä½“è¯¯å·®æ–‡ä»¶åˆ°: individual_errors/\n",
      "  ä¸ºOriginè½¯ä»¶å‡†å¤‡Excelæ•°æ®...\n",
      "    æ­£åœ¨ç”ŸæˆOriginå¯ç”¨çš„Excelæ–‡ä»¶...\n",
      "    ç”ŸæˆCDFæ•°æ®è¡¨...\n",
      "    âœ“ ä¸»CDFæ•°æ®è¡¨: 01_All_Models_CDF_Data.xlsx\n",
      "    ç”Ÿæˆé¡¶çº§æ¨¡å‹å¯¹æ¯”æ•°æ®...\n",
      "    âœ“ é¡¶çº§æ¨¡å‹å¯¹æ¯”: 02_Top_Models_Comparison.xlsx\n",
      "    ç”Ÿæˆè¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾æ•°æ®...\n",
      "    âœ“ è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾: 03_Error_Distribution_Histograms.xlsx\n",
      "    ç”Ÿæˆé›·è¾¾å›¾æ•°æ®...\n",
      "    âœ“ é›·è¾¾å›¾æ•°æ®: 04_Radar_Chart_Data.xlsx\n",
      "    ç”Ÿæˆå•ç‹¬æ¨¡å‹åˆ†ææ•°æ®...\n",
      "      å¤„ç†æ¨¡å‹ 1/5: PCC_k=5_GradientBoosting\n",
      "      å¤„ç†æ¨¡å‹ 2/5: F-Regression_k=5_GradientBoosting\n",
      "      å¤„ç†æ¨¡å‹ 3/5: RFE_k=4_GradientBoosting\n",
      "      å¤„ç†æ¨¡å‹ 4/5: Mutual Information_k=5_GradientBoosting\n",
      "      å¤„ç†æ¨¡å‹ 5/5: Lasso_k=4_GradientBoosting\n",
      "    âœ“ å•ç‹¬æ¨¡å‹åˆ†ææ•°æ®: individual_models/ (å‰5ä¸ªæ¨¡å‹)\n",
      "    ç”ŸæˆOriginç»˜å›¾æŒ‡å—...\n",
      "    âœ“ Originç»˜å›¾æŒ‡å—: 00_Origin_Plotting_Guide.xlsx\n",
      "    âœ… æ‰€æœ‰Excelæ•°æ®æ–‡ä»¶å·²ç”Ÿæˆå®Œæˆ!\n",
      "\n",
      "=== åˆ›å»ºç»¼åˆCDFåˆ†æå›¾ ===\n",
      "  ç»˜åˆ¶å®Œæ•´CDFæ›²çº¿...\n",
      "  ç»˜åˆ¶é¡¶çº§æ¨¡å‹è¯¦ç»†å¯¹æ¯”...\n",
      "  ç»˜åˆ¶è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾...\n",
      "  ç»˜åˆ¶æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾...\n",
      "âœ“ ç»¼åˆCDFå›¾å·²ä¿å­˜: comprehensive_cdf_analysis.png\n",
      "\n",
      "=== åˆ›å»ºå•ç‹¬æ¨¡å‹åˆ†æå›¾ ===\n",
      "  [1/11] åˆ›å»º PCC_k=5_GradientBoosting çš„åˆ†æå›¾...\n",
      "  [2/11] åˆ›å»º F-Regression_k=5_GradientBoosting çš„åˆ†æå›¾...\n",
      "  [3/11] åˆ›å»º RFE_k=4_GradientBoosting çš„åˆ†æå›¾...\n",
      "  [4/11] åˆ›å»º Mutual Information_k=5_GradientBoosting çš„åˆ†æå›¾...\n",
      "  [5/11] åˆ›å»º Lasso_k=4_GradientBoosting çš„åˆ†æå›¾...\n",
      "  [6/11] åˆ›å»º SFS_k=4_GradientBoosting çš„åˆ†æå›¾...\n",
      "  [7/11] åˆ›å»º RandomForest_k=4_GradientBoosting çš„åˆ†æå›¾...\n",
      "  [8/11] åˆ›å»º SFS_k=4_Bagging çš„åˆ†æå›¾...\n",
      "  [9/11] åˆ›å»º SFS_k=5_GradientBoosting çš„åˆ†æå›¾...\n",
      "  [10/11] åˆ›å»º SBS_k=4_GradientBoosting çš„åˆ†æå›¾...\n",
      "  [11/11] åˆ›å»º GA_k=4_GradientBoosting çš„åˆ†æå›¾...\n",
      "âœ“ å·²åˆ›å»º 11 ä¸ªå•ç‹¬æ¨¡å‹åˆ†æå›¾\n",
      "\n",
      "=== åˆ›å»ºæ±‡æ€»ç»Ÿè®¡æŠ¥å‘Š ===\n",
      "âœ“ è¯¦ç»†ç»Ÿè®¡è¡¨å·²ä¿å­˜: detailed_model_statistics.csv\n",
      "âœ“ æ’åç»Ÿè®¡å·²ä¿å­˜: ranking_statistics.txt\n",
      "\n",
      "============================================================\n",
      "=== CDFåˆ†æå®Œæˆ ===\n",
      "============================================================\n",
      "âœ“ æˆåŠŸåˆ†æ 11 ä¸ªæ¨¡å‹\n",
      "âœ“ æœ€ä¼˜æ¨¡å‹: PCC_k=5_GradientBoosting (RÂ²=0.9318)\n",
      "âœ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\CDFåˆ†æ8.1\n",
      "\n",
      "ç”Ÿæˆçš„æ–‡ä»¶:\n",
      "ğŸ“Š figures/comprehensive_cdf_analysis.png - ç»¼åˆCDFåˆ†æå›¾\n",
      "ğŸ“Š figures/individual_models/ - å•ç‹¬æ¨¡å‹åˆ†æå›¾ (11ä¸ª)\n",
      "ğŸ“„ data/model_summary_with_errors.csv - æ¨¡å‹æ±‡æ€»è¡¨\n",
      "ğŸ“„ data/detailed_statistics.pkl - è¯¦ç»†ç»Ÿè®¡æ•°æ®\n",
      "ğŸ“„ statistics/detailed_model_statistics.csv - è¯¦ç»†ç»Ÿè®¡è¡¨\n",
      "ğŸ“„ statistics/ranking_statistics.txt - æ’åç»Ÿè®¡\n",
      "ğŸ“„ analysis_config.txt - åˆ†æé…ç½®\n",
      "\n",
      "ğŸ“ˆ Originç»˜å›¾ä¸“ç”¨Excelæ–‡ä»¶:\n",
      "ğŸ“Š excel_for_origin/00_Origin_Plotting_Guide.xlsx - ç»˜å›¾æŒ‡å—\n",
      "ğŸ“Š excel_for_origin/01_All_Models_CDF_Data.xlsx - æ‰€æœ‰æ¨¡å‹CDFæ•°æ®\n",
      "ğŸ“Š excel_for_origin/02_Top_Models_Comparison.xlsx - é¡¶çº§æ¨¡å‹å¯¹æ¯”æ•°æ®\n",
      "ğŸ“Š excel_for_origin/03_Error_Distribution_Histograms.xlsx - è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾æ•°æ®\n",
      "ğŸ“Š excel_for_origin/04_Radar_Chart_Data.xlsx - é›·è¾¾å›¾æ•°æ®\n",
      "ğŸ“Š excel_for_origin/individual_models/ - å•ç‹¬æ¨¡å‹åˆ†ææ•°æ® (å‰5ä¸ªæ¨¡å‹)\n",
      "\n",
      "ğŸ¯ é‡ç‚¹å‘ç°:\n",
      "   æœ€ä½³RÂ²: 0.9318 (PCC_k=5_GradientBoosting)\n",
      "   æœ€ä½MAE: 2.0637 (PCC_k=5_GradientBoosting)\n",
      "   å¹³å‡ä¼˜ç§€é¢„æµ‹ç‡(â‰¤5%è¯¯å·®): 0.389\n",
      "   é«˜çº§æ–¹æ³•æ¨¡å‹å æ¯”: 5/11\n",
      "\n",
      "ğŸ’¡ Originä½¿ç”¨è¯´æ˜:\n",
      "   1. é¦–å…ˆæ‰“å¼€ 00_Origin_Plotting_Guide.xlsx æŸ¥çœ‹ç»˜å›¾æŒ‡å—\n",
      "   2. ä¸»è¦CDFå›¾ä½¿ç”¨ 01_All_Models_CDF_Data.xlsx\n",
      "   3. æ¯ä¸ªExcelæ–‡ä»¶åŒ…å«å¤šä¸ªå·¥ä½œè¡¨ï¼Œé€‰æ‹©åˆé€‚çš„æ•°æ®å·¥ä½œè¡¨\n",
      "   4. æ‰€æœ‰æ•°æ®å·²ç»é¢„å¤„ç†ï¼Œå¯ç›´æ¥ç”¨äºOriginç»˜å›¾\n",
      "   5. æ–‡ä»¶åŒ…å«é¢œè‰²å’Œæ ·å¼å»ºè®®ï¼Œä¾¿äºåˆ›å»ºä¸“ä¸šå›¾è¡¨\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ç¡®ä¿openpyxlå¯ç”¨äºExcelæ–‡ä»¶æ“ä½œ\n",
    "try:\n",
    "    import openpyxl\n",
    "except ImportError:\n",
    "    print(\"è­¦å‘Š: openpyxlæœªå®‰è£…ï¼ŒExcelåŠŸèƒ½å¯èƒ½ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install openpyxl\")\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.style.use('default')\n",
    "\n",
    "# é…ç½®å‚æ•°\n",
    "CONFIG = {\n",
    "    # è¾“å…¥è·¯å¾„ - æ¨¡å‹è®­ç»ƒç»“æœçš„è·¯å¾„\n",
    "    'input_path': r\"D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\æ¨¡å‹è®­ç»ƒ_K45_ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹_æ— è´å¶æ–¯ä¼˜åŒ–-7.29.3æœ€ç»ˆ_ä¿®å¤ç‰ˆ\",\n",
    "    # è¾“å‡ºè·¯å¾„ - CDFåˆ†æç»“æœä¿å­˜è·¯å¾„\n",
    "    'output_path': r\"D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\CDFåˆ†æ8.1\",\n",
    "    # æ–‡ä»¶åç¼€ï¼ˆé€‚é…ä¿®å¤ç‰ˆï¼‰\n",
    "    'file_suffix': '_physics_guided_no_bayesian_fixed',\n",
    "    # åˆ†æå‚æ•°\n",
    "    'max_error': 50,  # æœ€å¤§è¯¯å·®é˜ˆå€¼ï¼ˆ%ï¼‰\n",
    "    'top_models_count': 8,  # è¯¦ç»†å¯¹æ¯”å›¾ä¸­æ˜¾ç¤ºçš„æ¨¡å‹æ•°é‡\n",
    "    'error_thresholds': [5, 10, 15, 20, 25, 30],  # è¯¯å·®é˜ˆå€¼åˆ—è¡¨\n",
    "    'figure_dpi': 300,  # å›¾ç‰‡åˆ†è¾¨ç‡\n",
    "    'enable_detailed_stats': True,  # æ˜¯å¦ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡\n",
    "}\n",
    "\n",
    "def setup_output_directory():\n",
    "    \"\"\"åˆ›å»ºè¾“å‡ºç›®å½•\"\"\"\n",
    "    output_dir = CONFIG['output_path']\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # åˆ›å»ºå­ç›®å½•\n",
    "    subdirs = ['figures', 'data', 'statistics']\n",
    "    for subdir in subdirs:\n",
    "        os.makedirs(os.path.join(output_dir, subdir), exist_ok=True)\n",
    "    \n",
    "    print(f\"âœ“ è¾“å‡ºç›®å½•å·²åˆ›å»º: {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "def load_model_data():\n",
    "    \"\"\"åŠ è½½ä¿å­˜çš„æ¨¡å‹å’Œæ•°æ®\"\"\"\n",
    "    print(\"=== æ­£åœ¨åŠ è½½æ¨¡å‹æ•°æ® ===\")\n",
    "    input_path = CONFIG['input_path']\n",
    "    suffix = CONFIG['file_suffix']\n",
    "    \n",
    "    try:\n",
    "        # åŠ è½½æ¨¡å‹åˆ—è¡¨\n",
    "        models_file = os.path.join(input_path, f'best_models{suffix}.pkl')\n",
    "        print(f\"åŠ è½½æ¨¡å‹æ–‡ä»¶: {models_file}\")\n",
    "        with open(models_file, 'rb') as f:\n",
    "            best_models = pickle.load(f)\n",
    "        \n",
    "        # åŠ è½½æ•°æ®ä¿¡æ¯\n",
    "        data_file = os.path.join(input_path, f'data_info{suffix}.pkl')\n",
    "        print(f\"åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}\")\n",
    "        with open(data_file, 'rb') as f:\n",
    "            data_info = pickle.load(f)\n",
    "        \n",
    "        # åŠ è½½ç‰¹å¾åç§°\n",
    "        features_file = os.path.join(input_path, f'features_name{suffix}.pkl')\n",
    "        print(f\"åŠ è½½ç‰¹å¾æ–‡ä»¶: {features_file}\")\n",
    "        with open(features_file, 'rb') as f:\n",
    "            features_name = pickle.load(f)\n",
    "        \n",
    "        print(f\"âœ“ æˆåŠŸåŠ è½½ {len(best_models)} ä¸ªæ¨¡å‹\")\n",
    "        print(f\"âœ“ æ•°æ®å½¢çŠ¶ä¿¡æ¯: X{data_info['X_all_scaled'].shape}, y{data_info['y_all_np'].shape}\")\n",
    "        print(f\"âœ“ ç‰¹å¾æ•°é‡: {len(features_name)}\")\n",
    "        \n",
    "        return best_models, data_info, features_name\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}\")\n",
    "        print(f\"è¯·æ£€æŸ¥è·¯å¾„: {input_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        raise\n",
    "\n",
    "def calculate_model_errors(best_models, data_info):\n",
    "    \"\"\"ä¸ºæ¯ä¸ªæ¨¡å‹è®¡ç®—é¢„æµ‹è¯¯å·®å’Œè¯¦ç»†ç»Ÿè®¡\"\"\"\n",
    "    print(\"\\n=== æ­£åœ¨è®¡ç®—æ¨¡å‹é¢„æµ‹è¯¯å·® ===\")\n",
    "    \n",
    "    model_errors = {}\n",
    "    model_info = {}\n",
    "    detailed_stats = {}\n",
    "    \n",
    "    for i, model_data in enumerate(best_models):\n",
    "        model_key = model_data['model_key']\n",
    "        method = model_data['method']\n",
    "        ml_model = model_data['model']\n",
    "        k = model_data['k']\n",
    "        \n",
    "        print(f\"  [{i+1}/{len(best_models)}] å¤„ç†æ¨¡å‹: {model_key}\")\n",
    "        \n",
    "        try:\n",
    "            # è·å–æ¨¡å‹å®ä¾‹å’Œå¯¹åº”æ•°æ®\n",
    "            model_instance = model_data['model_instance']\n",
    "            X_data = model_data['X_data']\n",
    "            y_data = model_data['y_data']\n",
    "            \n",
    "            # è¿›è¡Œé¢„æµ‹\n",
    "            y_pred = model_instance.predict(X_data)\n",
    "            \n",
    "            # è®¡ç®—å„ç§è¯¯å·®æŒ‡æ ‡\n",
    "            absolute_errors = np.abs(y_data - y_pred)\n",
    "            \n",
    "            # è®¡ç®—ç›¸å¯¹è¯¯å·®ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œé¿å…é™¤é›¶\n",
    "            y_true_safe = np.where(np.abs(y_data) < 1e-6, 1e-6, y_data)\n",
    "            relative_errors = np.abs((y_data - y_pred) / y_true_safe) * 100\n",
    "            \n",
    "            # è®¡ç®—æ ‡å‡†åŒ–æ®‹å·®\n",
    "            residuals = y_data - y_pred\n",
    "            std_residuals = residuals / np.std(residuals) if np.std(residuals) > 0 else residuals\n",
    "            \n",
    "            # å­˜å‚¨è¯¯å·®\n",
    "            model_errors[model_key] = relative_errors\n",
    "            \n",
    "            # å­˜å‚¨æ¨¡å‹ä¿¡æ¯\n",
    "            model_info[model_key] = {\n",
    "                'method': method,\n",
    "                'ml_model': ml_model,\n",
    "                'k': k,\n",
    "                'train_r2': model_data['train_r2'],\n",
    "                'test_r2': model_data['test_r2'],\n",
    "                'train_mae': model_data['train_mae'],\n",
    "                'test_mae': model_data['test_mae'],\n",
    "                'train_rmsle': model_data.get('train_rmsle', 0),\n",
    "                'test_rmsle': model_data.get('test_rmsle', 0),\n",
    "                'features': model_data['features'],\n",
    "                'is_advanced': model_data['is_advanced'],\n",
    "                'is_focus_combination': model_data['is_focus_combination'],\n",
    "                'data_type': model_data.get('data_type', 'Unknown')\n",
    "            }\n",
    "            \n",
    "            # è®¡ç®—è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯\n",
    "            if CONFIG['enable_detailed_stats']:\n",
    "                detailed_stats[model_key] = {\n",
    "                    'y_true': y_data,\n",
    "                    'y_pred': y_pred,\n",
    "                    'absolute_errors': absolute_errors,\n",
    "                    'relative_errors': relative_errors,\n",
    "                    'residuals': residuals,\n",
    "                    'std_residuals': std_residuals,\n",
    "                    'rmse': np.sqrt(mean_squared_error(y_data, y_pred)),\n",
    "                    'mae': mean_absolute_error(y_data, y_pred),\n",
    "                    'r2': r2_score(y_data, y_pred),\n",
    "                    'mean_relative_error': np.mean(relative_errors),\n",
    "                    'median_relative_error': np.median(relative_errors),\n",
    "                    'std_relative_error': np.std(relative_errors),\n",
    "                    'max_relative_error': np.max(relative_errors),\n",
    "                    'min_relative_error': np.min(relative_errors)\n",
    "                }\n",
    "            \n",
    "            print(f\"    âœ“ RÂ²={model_data['test_r2']:.4f}, MAE={model_data['test_mae']:.4f}, \"\n",
    "                  f\"å¹³å‡ç›¸å¯¹è¯¯å·®={np.mean(relative_errors):.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    âŒ å¤„ç†æ¨¡å‹ {model_key} æ—¶å‡ºé”™: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"âœ“ æˆåŠŸå¤„ç† {len(model_errors)} ä¸ªæ¨¡å‹\")\n",
    "    return model_errors, model_info, detailed_stats\n",
    "\n",
    "def save_data_files(model_errors, model_info, detailed_stats, output_dir):\n",
    "    \"\"\"ä¿å­˜æ‰€æœ‰æ•°æ®æ–‡ä»¶\"\"\"\n",
    "    print(\"\\n=== ä¿å­˜æ•°æ®æ–‡ä»¶ ===\")\n",
    "    data_dir = os.path.join(output_dir, 'data')\n",
    "    excel_dir = os.path.join(output_dir, 'excel_for_origin')\n",
    "    os.makedirs(excel_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. ä¿å­˜æ¨¡å‹è¯¯å·®æ•°æ®\n",
    "    errors_file = os.path.join(data_dir, 'model_relative_errors.pkl')\n",
    "    with open(errors_file, 'wb') as f:\n",
    "        pickle.dump(model_errors, f)\n",
    "    print(f\"âœ“ ä¿å­˜ç›¸å¯¹è¯¯å·®æ•°æ®: {os.path.basename(errors_file)}\")\n",
    "    \n",
    "    # 2. ä¿å­˜æ¨¡å‹ä¿¡æ¯\n",
    "    info_file = os.path.join(data_dir, 'model_info.pkl')\n",
    "    with open(info_file, 'wb') as f:\n",
    "        pickle.dump(model_info, f)\n",
    "    print(f\"âœ“ ä¿å­˜æ¨¡å‹ä¿¡æ¯: {os.path.basename(info_file)}\")\n",
    "    \n",
    "    # 3. ä¿å­˜è¯¦ç»†ç»Ÿè®¡æ•°æ®\n",
    "    if detailed_stats:\n",
    "        stats_file = os.path.join(data_dir, 'detailed_statistics.pkl')\n",
    "        with open(stats_file, 'wb') as f:\n",
    "            pickle.dump(detailed_stats, f)\n",
    "        print(f\"âœ“ ä¿å­˜è¯¦ç»†ç»Ÿè®¡: {os.path.basename(stats_file)}\")\n",
    "    \n",
    "    # 4. ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰\n",
    "    # åˆ›å»ºæ±‡æ€»è¡¨\n",
    "    summary_data = []\n",
    "    for model_key, info in model_info.items():\n",
    "        features_str = ', '.join(info['features']) if isinstance(info['features'], list) else str(info['features'])\n",
    "        \n",
    "        row = {\n",
    "            'Model_Key': model_key,\n",
    "            'Method': info['method'],\n",
    "            'ML_Model': info['ml_model'],\n",
    "            'K': info['k'],\n",
    "            'Train_R2': info['train_r2'],\n",
    "            'Test_R2': info['test_r2'],\n",
    "            'Train_MAE': info['train_mae'],\n",
    "            'Test_MAE': info['test_mae'],\n",
    "            'Train_RMSLE': info['train_rmsle'],\n",
    "            'Test_RMSLE': info['test_rmsle'],\n",
    "            'Features': features_str,\n",
    "            'Is_Advanced': info['is_advanced'],\n",
    "            'Is_Focus_Combination': info['is_focus_combination'],\n",
    "            'Data_Type': info['data_type']\n",
    "        }\n",
    "        \n",
    "        # æ·»åŠ è¯¯å·®ç»Ÿè®¡\n",
    "        if model_key in model_errors:\n",
    "            errors = model_errors[model_key]\n",
    "            row.update({\n",
    "                'Mean_Relative_Error': np.mean(errors),\n",
    "                'Median_Relative_Error': np.median(errors),\n",
    "                'Std_Relative_Error': np.std(errors),\n",
    "                'Max_Relative_Error': np.max(errors),\n",
    "                'Min_Relative_Error': np.min(errors)\n",
    "            })\n",
    "            \n",
    "            # æ·»åŠ ä¸åŒé˜ˆå€¼ä¸‹çš„ç´¯ç§¯é¢‘ç‡\n",
    "            for threshold in CONFIG['error_thresholds']:\n",
    "                row[f'Freq_Below_{threshold}pct'] = np.mean(errors <= threshold)\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    # ä¿å­˜æ±‡æ€»CSV\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.sort_values('Test_R2', ascending=False)\n",
    "    summary_csv = os.path.join(data_dir, 'model_summary_with_errors.csv')\n",
    "    summary_df.to_csv(summary_csv, index=False, encoding='utf-8')\n",
    "    print(f\"âœ“ ä¿å­˜æ±‡æ€»CSV: {os.path.basename(summary_csv)}\")\n",
    "    \n",
    "    # 5. ä¿å­˜æ¯ä¸ªæ¨¡å‹çš„è¯¦ç»†è¯¯å·®æ•°æ®\n",
    "    errors_detail_dir = os.path.join(data_dir, 'individual_errors')\n",
    "    os.makedirs(errors_detail_dir, exist_ok=True)\n",
    "    \n",
    "    for model_key, errors in model_errors.items():\n",
    "        error_df = pd.DataFrame({\n",
    "            'Sample_Index': range(len(errors)),\n",
    "            'Relative_Error_Percent': errors\n",
    "        })\n",
    "        error_file = os.path.join(errors_detail_dir, f'{model_key}_errors.csv')\n",
    "        error_df.to_csv(error_file, index=False)\n",
    "    \n",
    "    print(f\"âœ“ ä¿å­˜ä¸ªä½“è¯¯å·®æ–‡ä»¶åˆ°: individual_errors/\")\n",
    "    \n",
    "    # 6. ä¸ºOriginè½¯ä»¶å‡†å¤‡Excelæ•°æ®\n",
    "    print(\"  ä¸ºOriginè½¯ä»¶å‡†å¤‡Excelæ•°æ®...\")\n",
    "    save_excel_for_origin(model_errors, model_info, detailed_stats, excel_dir)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def save_excel_for_origin(model_errors, model_info, detailed_stats, excel_dir):\n",
    "    \"\"\"ä¸ºOriginè½¯ä»¶ä¿å­˜æ‰€æœ‰å¯è§†åŒ–æ•°æ®çš„Excelæ–‡ä»¶\"\"\"\n",
    "    print(\"    æ­£åœ¨ç”ŸæˆOriginå¯ç”¨çš„Excelæ–‡ä»¶...\")\n",
    "    \n",
    "    # 1. ä¿å­˜æ‰€æœ‰æ¨¡å‹çš„CDFæ•°æ® - ä¸»è¦æ•°æ®è¡¨\n",
    "    print(\"    ç”ŸæˆCDFæ•°æ®è¡¨...\")\n",
    "    cdf_excel_file = os.path.join(excel_dir, '01_All_Models_CDF_Data.xlsx')\n",
    "    \n",
    "    with pd.ExcelWriter(cdf_excel_file, engine='openpyxl') as writer:\n",
    "        # å·¥ä½œè¡¨1: æ‰€æœ‰æ¨¡å‹çš„CDFåŸå§‹æ•°æ®\n",
    "        all_cdf_data = {}\n",
    "        model_labels = []\n",
    "        \n",
    "        for model_key, errors in model_errors.items():\n",
    "            info = model_info[model_key]\n",
    "            errors_filtered = errors[errors <= CONFIG['max_error']]\n",
    "            errors_sorted = np.sort(errors_filtered)\n",
    "            \n",
    "            if len(errors_sorted) > 0:\n",
    "                cumulative_prob = np.arange(1, len(errors_sorted) + 1) / len(errors_sorted)\n",
    "                \n",
    "                # åˆ›å»ºæ ‡ç­¾\n",
    "                advanced_mark = \"â˜…\" if info['is_advanced'] else \"\"\n",
    "                focus_mark = \"ğŸ¯\" if info['is_focus_combination'] else \"\"\n",
    "                label = f\"{advanced_mark}{focus_mark}{info['method']}+{info['ml_model']}_K{info['k']}\"\n",
    "                model_labels.append(label)\n",
    "                \n",
    "                # ç»Ÿä¸€è¯¯å·®èŒƒå›´åˆ°0-50ï¼Œæ’å€¼åˆ°ç›¸åŒçš„xè½´\n",
    "                x_common = np.linspace(0, CONFIG['max_error'], 1000)\n",
    "                y_interpolated = np.interp(x_common, errors_sorted, cumulative_prob)\n",
    "                \n",
    "                all_cdf_data[f'{label}_Error'] = x_common\n",
    "                all_cdf_data[f'{label}_CumProb'] = y_interpolated\n",
    "        \n",
    "        # ä¿å­˜CDFæ’å€¼æ•°æ®\n",
    "        cdf_df = pd.DataFrame(all_cdf_data)\n",
    "        cdf_df.to_excel(writer, sheet_name='CDF_Interpolated_Data', index=False)\n",
    "        \n",
    "        # å·¥ä½œè¡¨2: åŸå§‹CDFæ•°æ®ç‚¹ï¼ˆæ¯ä¸ªæ¨¡å‹ä¸€åˆ—ï¼‰\n",
    "        max_length = max(len(errors[errors <= CONFIG['max_error']]) for errors in model_errors.values())\n",
    "        raw_cdf_data = {'Common_Index': range(max_length)}\n",
    "        \n",
    "        for model_key, errors in model_errors.items():\n",
    "            info = model_info[model_key]\n",
    "            errors_filtered = errors[errors <= CONFIG['max_error']]\n",
    "            errors_sorted = np.sort(errors_filtered)\n",
    "            \n",
    "            if len(errors_sorted) > 0:\n",
    "                cumulative_prob = np.arange(1, len(errors_sorted) + 1) / len(errors_sorted)\n",
    "                label = f\"{info['method']}+{info['ml_model']}_K{info['k']}\"\n",
    "                \n",
    "                # å¡«å……åˆ°æœ€å¤§é•¿åº¦\n",
    "                padded_errors = np.full(max_length, np.nan)\n",
    "                padded_probs = np.full(max_length, np.nan)\n",
    "                \n",
    "                padded_errors[:len(errors_sorted)] = errors_sorted\n",
    "                padded_probs[:len(cumulative_prob)] = cumulative_prob\n",
    "                \n",
    "                raw_cdf_data[f'{label}_Error'] = padded_errors\n",
    "                raw_cdf_data[f'{label}_CumProb'] = padded_probs\n",
    "        \n",
    "        raw_cdf_df = pd.DataFrame(raw_cdf_data)\n",
    "        raw_cdf_df.to_excel(writer, sheet_name='CDF_Raw_Data_Points', index=False)\n",
    "        \n",
    "        # å·¥ä½œè¡¨3: æ¨¡å‹ä¿¡æ¯å’Œç»Ÿè®¡\n",
    "        model_stats = []\n",
    "        for model_key, info in model_info.items():\n",
    "            if model_key in model_errors:\n",
    "                errors = model_errors[model_key]\n",
    "                model_stats.append({\n",
    "                    'Model_Key': model_key,\n",
    "                    'Display_Name': f\"{info['method']}+{info['ml_model']}_K{info['k']}\",\n",
    "                    'Method': info['method'],\n",
    "                    'ML_Model': info['ml_model'],\n",
    "                    'K': info['k'],\n",
    "                    'Test_R2': info['test_r2'],\n",
    "                    'Test_MAE': info['test_mae'],\n",
    "                    'Is_Advanced': info['is_advanced'],\n",
    "                    'Is_Focus_Combination': info['is_focus_combination'],\n",
    "                    'Mean_Error': np.mean(errors),\n",
    "                    'Median_Error': np.median(errors),\n",
    "                    'Std_Error': np.std(errors),\n",
    "                    'Sample_Count': len(errors),\n",
    "                    'Freq_5pct': np.mean(errors <= 5),\n",
    "                    'Freq_10pct': np.mean(errors <= 10),\n",
    "                    'Freq_20pct': np.mean(errors <= 20),\n",
    "                    'Color_Code': f'Color_{list(model_errors.keys()).index(model_key) + 1}'\n",
    "                })\n",
    "        \n",
    "        stats_df = pd.DataFrame(model_stats)\n",
    "        stats_df = stats_df.sort_values('Test_R2', ascending=False)\n",
    "        stats_df.to_excel(writer, sheet_name='Model_Info_and_Stats', index=False)\n",
    "    \n",
    "    print(f\"    âœ“ ä¸»CDFæ•°æ®è¡¨: {os.path.basename(cdf_excel_file)}\")\n",
    "    \n",
    "    # 2. ä¿å­˜é¡¶çº§æ¨¡å‹å¯¹æ¯”æ•°æ®\n",
    "    print(\"    ç”Ÿæˆé¡¶çº§æ¨¡å‹å¯¹æ¯”æ•°æ®...\")\n",
    "    top_models_file = os.path.join(excel_dir, '02_Top_Models_Comparison.xlsx')\n",
    "    \n",
    "    sorted_models = sorted(model_errors.items(), \n",
    "                          key=lambda x: model_info[x[0]]['test_r2'], \n",
    "                          reverse=True)\n",
    "    top_models = sorted_models[:CONFIG['top_models_count']]\n",
    "    \n",
    "    with pd.ExcelWriter(top_models_file, engine='openpyxl') as writer:\n",
    "        # é¡¶çº§æ¨¡å‹CDFæ•°æ®\n",
    "        x_common = np.linspace(0, 35, 500)\n",
    "        top_cdf_data = {'Error_Percent': x_common}\n",
    "        \n",
    "        for i, (model_key, errors) in enumerate(top_models):\n",
    "            info = model_info[model_key]\n",
    "            errors_filtered = errors[errors <= 35]\n",
    "            errors_sorted = np.sort(errors_filtered)\n",
    "            \n",
    "            if len(errors_sorted) > 0:\n",
    "                cumulative_prob = np.arange(1, len(errors_sorted) + 1) / len(errors_sorted)\n",
    "                y_interpolated = np.interp(x_common, errors_sorted, cumulative_prob)\n",
    "                \n",
    "                method_short = info['method'].replace('F-Regression', 'F-Reg').replace('Mutual Information', 'MutInfo')\n",
    "                model_short = info['ml_model'].replace('GradientBoosting', 'GB').replace('RandomForest', 'RF')\n",
    "                advanced_mark = \"â˜…\" if info['is_advanced'] else \"\"\n",
    "                \n",
    "                label = f\"{advanced_mark}{method_short}+{model_short}\"\n",
    "                top_cdf_data[f'{label}_CumProb'] = y_interpolated\n",
    "                top_cdf_data[f'{label}_R2'] = [info['test_r2']] * len(x_common)  # ç”¨äºæ ‡æ³¨\n",
    "        \n",
    "        top_cdf_df = pd.DataFrame(top_cdf_data)\n",
    "        top_cdf_df.to_excel(writer, sheet_name='Top_Models_CDF', index=False)\n",
    "        \n",
    "        # é¡¶çº§æ¨¡å‹ç»Ÿè®¡å¯¹æ¯”\n",
    "        top_stats = []\n",
    "        for model_key, errors in top_models:\n",
    "            info = model_info[model_key]\n",
    "            top_stats.append({\n",
    "                'Rank': len(top_stats) + 1,\n",
    "                'Model': f\"{info['method']}+{info['ml_model']}\",\n",
    "                'K': info['k'],\n",
    "                'Test_R2': info['test_r2'],\n",
    "                'Test_MAE': info['test_mae'],\n",
    "                'Mean_Error': np.mean(errors),\n",
    "                'Freq_5pct': np.mean(errors <= 5),\n",
    "                'Freq_10pct': np.mean(errors <= 10),\n",
    "                'Freq_20pct': np.mean(errors <= 20),\n",
    "                'Is_Advanced': info['is_advanced'],\n",
    "                'LineStyle': ['solid', 'dashed', 'dashdot', 'dotted'][len(top_stats) % 4]\n",
    "            })\n",
    "        \n",
    "        top_stats_df = pd.DataFrame(top_stats)\n",
    "        top_stats_df.to_excel(writer, sheet_name='Top_Models_Stats', index=False)\n",
    "    \n",
    "    print(f\"    âœ“ é¡¶çº§æ¨¡å‹å¯¹æ¯”: {os.path.basename(top_models_file)}\")\n",
    "    \n",
    "    # 3. ä¿å­˜è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾æ•°æ®\n",
    "    print(\"    ç”Ÿæˆè¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾æ•°æ®...\")\n",
    "    histogram_file = os.path.join(excel_dir, '03_Error_Distribution_Histograms.xlsx')\n",
    "    \n",
    "    with pd.ExcelWriter(histogram_file, engine='openpyxl') as writer:\n",
    "        # æœ€ä¼˜æ¨¡å‹çš„è¯¯å·®åˆ†å¸ƒ\n",
    "        best_model_key = max(model_errors.keys(), key=lambda x: model_info[x]['test_r2'])\n",
    "        best_errors = model_errors[best_model_key]\n",
    "        best_errors_filtered = best_errors[best_errors <= 30]\n",
    "        \n",
    "        # è®¡ç®—ç›´æ–¹å›¾\n",
    "        counts, bin_edges = np.histogram(best_errors_filtered, bins=30)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        \n",
    "        hist_data = {\n",
    "            'Bin_Centers': bin_centers,\n",
    "            'Counts': counts,\n",
    "            'Bin_Edges_Left': bin_edges[:-1],\n",
    "            'Bin_Edges_Right': bin_edges[1:],\n",
    "            'Bin_Width': bin_edges[1] - bin_edges[0]\n",
    "        }\n",
    "        \n",
    "        # æ·»åŠ ç»Ÿè®¡çº¿æ•°æ®\n",
    "        hist_data['Mean_Error'] = [np.mean(best_errors_filtered)] * len(bin_centers)\n",
    "        hist_data['Median_Error'] = [np.median(best_errors_filtered)] * len(bin_centers)\n",
    "        hist_data['Max_Count_for_Lines'] = [max(counts)] * len(bin_centers)\n",
    "        \n",
    "        hist_df = pd.DataFrame(hist_data)\n",
    "        hist_df.to_excel(writer, sheet_name='Best_Model_Histogram', index=False)\n",
    "        \n",
    "        # æ‰€æœ‰æ¨¡å‹çš„ç›´æ–¹å›¾æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "        all_hist_data = {'Error_Range': bin_centers}\n",
    "        for model_key, errors in list(model_errors.items())[:8]:  # åªä¿å­˜å‰8ä¸ªæ¨¡å‹\n",
    "            info = model_info[model_key]\n",
    "            errors_filtered = errors[errors <= 30]\n",
    "            \n",
    "            if len(errors_filtered) > 0:\n",
    "                counts, _ = np.histogram(errors_filtered, bins=bin_edges)\n",
    "                model_name = f\"{info['method']}+{info['ml_model']}_K{info['k']}\"\n",
    "                all_hist_data[f'{model_name}_Counts'] = counts\n",
    "        \n",
    "        all_hist_df = pd.DataFrame(all_hist_data)\n",
    "        all_hist_df.to_excel(writer, sheet_name='All_Models_Histograms', index=False)\n",
    "    \n",
    "    print(f\"    âœ“ è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾: {os.path.basename(histogram_file)}\")\n",
    "    \n",
    "    # 4. ä¿å­˜é›·è¾¾å›¾æ•°æ®\n",
    "    print(\"    ç”Ÿæˆé›·è¾¾å›¾æ•°æ®...\")\n",
    "    radar_file = os.path.join(excel_dir, '04_Radar_Chart_Data.xlsx')\n",
    "    \n",
    "    with pd.ExcelWriter(radar_file, engine='openpyxl') as writer:\n",
    "        # é€‰æ‹©å‰6ä¸ªæ¨¡å‹\n",
    "        radar_models = sorted_models[:6]\n",
    "        metrics = ['Test_R2', 'Freq_5pct', 'Freq_10pct', 'Freq_20pct', 'Low_Error_Rate']\n",
    "        metric_labels = ['RÂ²', '5%è¯¯å·®ç‡', '10%è¯¯å·®ç‡', '20%è¯¯å·®ç‡', 'ä½è¯¯å·®ç‡']\n",
    "        \n",
    "        # é›·è¾¾å›¾æ•°æ®\n",
    "        radar_data = []\n",
    "        for model_key, _ in radar_models:\n",
    "            info = model_info[model_key]\n",
    "            errors = model_errors[model_key]\n",
    "            \n",
    "            row = {\n",
    "                'Model': f\"{info['method']}+{info['ml_model']}_K{info['k']}\",\n",
    "                'Test_R2': info['test_r2'],\n",
    "                'Freq_5pct': np.mean(errors <= 5),\n",
    "                'Freq_10pct': np.mean(errors <= 10),\n",
    "                'Freq_20pct': np.mean(errors <= 20),\n",
    "                'Low_Error_Rate': np.mean(errors <= 15),\n",
    "                'Is_Advanced': info['is_advanced'],\n",
    "                'Is_Focus': info['is_focus_combination']\n",
    "            }\n",
    "            radar_data.append(row)\n",
    "        \n",
    "        radar_df = pd.DataFrame(radar_data)\n",
    "        radar_df.to_excel(writer, sheet_name='Radar_Data', index=False)\n",
    "        \n",
    "        # é›·è¾¾å›¾é…ç½®ï¼ˆè§’åº¦ç­‰ï¼‰\n",
    "        angles_deg = np.linspace(0, 360, len(metrics) + 1)[:-1]  # ä¸åŒ…å«360åº¦ç‚¹\n",
    "        angles_rad = np.deg2rad(angles_deg)\n",
    "        \n",
    "        config_data = {\n",
    "            'Metric': metric_labels,\n",
    "            'Angle_Degrees': angles_deg,\n",
    "            'Angle_Radians': angles_rad,\n",
    "            'Max_Value': [1.0] * len(metrics),  # æ‰€æœ‰æŒ‡æ ‡æœ€å¤§å€¼ä¸º1\n",
    "            'Min_Value': [0.0] * len(metrics),  # æ‰€æœ‰æŒ‡æ ‡æœ€å°å€¼ä¸º0\n",
    "        }\n",
    "        \n",
    "        config_df = pd.DataFrame(config_data)\n",
    "        config_df.to_excel(writer, sheet_name='Radar_Config', index=False)\n",
    "    \n",
    "    print(f\"    âœ“ é›·è¾¾å›¾æ•°æ®: {os.path.basename(radar_file)}\")\n",
    "    \n",
    "    # 5. ä¿å­˜æ¯ä¸ªæ¨¡å‹çš„è¯¦ç»†åˆ†ææ•°æ®\n",
    "    print(\"    ç”Ÿæˆå•ç‹¬æ¨¡å‹åˆ†ææ•°æ®...\")\n",
    "    individual_dir = os.path.join(excel_dir, 'individual_models')\n",
    "    os.makedirs(individual_dir, exist_ok=True)\n",
    "    \n",
    "    for i, (model_key, errors) in enumerate(list(model_errors.items())[:5]):  # åªä¸ºå‰5ä¸ªæ¨¡å‹ç”Ÿæˆè¯¦ç»†æ•°æ®\n",
    "        info = model_info[model_key]\n",
    "        stats = detailed_stats.get(model_key, {})\n",
    "        \n",
    "        print(f\"      å¤„ç†æ¨¡å‹ {i+1}/5: {model_key}\")\n",
    "        \n",
    "        try:\n",
    "            safe_filename = model_key.replace('/', '_').replace('\\\\', '_').replace(':', '_')\n",
    "            individual_file = os.path.join(individual_dir, f'{safe_filename}_analysis.xlsx')\n",
    "            \n",
    "            with pd.ExcelWriter(individual_file, engine='openpyxl') as writer:\n",
    "                # CDFæ•°æ®\n",
    "                errors_filtered = errors[errors <= 40]\n",
    "                errors_sorted = np.sort(errors_filtered)\n",
    "                if len(errors_sorted) > 0:\n",
    "                    cumulative_prob = np.arange(1, len(errors_sorted) + 1) / len(errors_sorted)\n",
    "                    \n",
    "                    cdf_data = {\n",
    "                        'Error_Percent': errors_sorted,\n",
    "                        'Cumulative_Probability': cumulative_prob,\n",
    "                        'Mean_Line_X': [np.mean(errors_filtered)] * len(errors_sorted),\n",
    "                        'Mean_Line_Y': np.linspace(0, 1, len(errors_sorted)),\n",
    "                        'Median_Line_X': [np.median(errors_filtered)] * len(errors_sorted),\n",
    "                        'Median_Line_Y': np.linspace(0, 1, len(errors_sorted))\n",
    "                    }\n",
    "                    \n",
    "                    cdf_df = pd.DataFrame(cdf_data)\n",
    "                    cdf_df.to_excel(writer, sheet_name='CDF_Data', index=False)\n",
    "                \n",
    "                # ç›´æ–¹å›¾æ•°æ®\n",
    "                counts, bin_edges = np.histogram(errors_filtered, bins=25)\n",
    "                bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "                \n",
    "                hist_data = {\n",
    "                    'Bin_Centers': bin_centers,\n",
    "                    'Counts': counts,\n",
    "                    'Bin_Left': bin_edges[:-1],\n",
    "                    'Bin_Right': bin_edges[1:],\n",
    "                    'Mean_Value': [np.mean(errors_filtered)] * len(bin_centers),\n",
    "                    'Median_Value': [np.median(errors_filtered)] * len(bin_centers)\n",
    "                }\n",
    "                \n",
    "                hist_df = pd.DataFrame(hist_data)\n",
    "                hist_df.to_excel(writer, sheet_name='Histogram_Data', index=False)\n",
    "                \n",
    "                # é¢„æµ‹vså®é™…æ•£ç‚¹å›¾æ•°æ®\n",
    "                if 'y_true' in stats and 'y_pred' in stats:\n",
    "                    y_true = stats['y_true']\n",
    "                    y_pred = stats['y_pred']\n",
    "                    \n",
    "                    # æ•£ç‚¹æ•°æ®\n",
    "                    scatter_data = {\n",
    "                        'Actual_Values': y_true,\n",
    "                        'Predicted_Values': y_pred,\n",
    "                        'R2_Value': [stats.get('r2', 0)] * len(y_true)\n",
    "                    }\n",
    "                    \n",
    "                    scatter_df = pd.DataFrame(scatter_data)\n",
    "                    scatter_df.to_excel(writer, sheet_name='Prediction_Scatter', index=False)\n",
    "                    \n",
    "                    # å®Œç¾é¢„æµ‹çº¿æ•°æ®ï¼ˆå•ç‹¬å·¥ä½œè¡¨ï¼‰\n",
    "                    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "                    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "                    perfect_line_data = {\n",
    "                        'Perfect_Line_X': [min_val, max_val],\n",
    "                        'Perfect_Line_Y': [min_val, max_val]\n",
    "                    }\n",
    "                    \n",
    "                    perfect_line_df = pd.DataFrame(perfect_line_data)\n",
    "                    perfect_line_df.to_excel(writer, sheet_name='Perfect_Prediction_Line', index=False)\n",
    "                \n",
    "                # æ®‹å·®å›¾æ•°æ®\n",
    "                if 'residuals' in stats:\n",
    "                    residuals = stats['residuals']\n",
    "                    y_pred = stats['y_pred']\n",
    "                    \n",
    "                    # æ®‹å·®æ•£ç‚¹æ•°æ®\n",
    "                    residual_data = {\n",
    "                        'Predicted_Values': y_pred,\n",
    "                        'Residuals': residuals\n",
    "                    }\n",
    "                    \n",
    "                    residual_df = pd.DataFrame(residual_data)\n",
    "                    residual_df.to_excel(writer, sheet_name='Residual_Data', index=False)\n",
    "                    \n",
    "                    # é›¶çº¿æ•°æ®ï¼ˆå•ç‹¬å·¥ä½œè¡¨ï¼‰\n",
    "                    zero_line_data = {\n",
    "                        'Zero_Line_X': [np.min(y_pred), np.max(y_pred)],\n",
    "                        'Zero_Line_Y': [0, 0]\n",
    "                    }\n",
    "                    \n",
    "                    zero_line_df = pd.DataFrame(zero_line_data)\n",
    "                    zero_line_df.to_excel(writer, sheet_name='Zero_Reference_Line', index=False)\n",
    "                \n",
    "                # æ¨¡å‹ä¿¡æ¯\n",
    "                model_info_data = {\n",
    "                    'Property': ['Model_Key', 'Method', 'ML_Model', 'K', 'Test_R2', 'Test_MAE', \n",
    "                               'Mean_Error', 'Median_Error', 'Std_Error', 'Sample_Count'],\n",
    "                    'Value': [model_key, info['method'], info['ml_model'], info['k'], \n",
    "                             info['test_r2'], info['test_mae'], np.mean(errors), \n",
    "                             np.median(errors), np.std(errors), len(errors)]\n",
    "                }\n",
    "                \n",
    "                info_df = pd.DataFrame(model_info_data)\n",
    "                info_df.to_excel(writer, sheet_name='Model_Info', index=False)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"        âŒ å¤„ç†æ¨¡å‹ {model_key} æ—¶å‡ºé”™: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"    âœ“ å•ç‹¬æ¨¡å‹åˆ†ææ•°æ®: individual_models/ (å‰5ä¸ªæ¨¡å‹)\")\n",
    "    \n",
    "    # 6. åˆ›å»ºOriginç»˜å›¾æŒ‡å—\n",
    "    print(\"    ç”ŸæˆOriginç»˜å›¾æŒ‡å—...\")\n",
    "    guide_file = os.path.join(excel_dir, '00_Origin_Plotting_Guide.xlsx')\n",
    "    \n",
    "    with pd.ExcelWriter(guide_file, engine='openpyxl') as writer:\n",
    "        # æ–‡ä»¶è¯´æ˜\n",
    "        guide_data = {\n",
    "            'File_Name': [\n",
    "                '01_All_Models_CDF_Data.xlsx',\n",
    "                '02_Top_Models_Comparison.xlsx', \n",
    "                '03_Error_Distribution_Histograms.xlsx',\n",
    "                '04_Radar_Chart_Data.xlsx',\n",
    "                'individual_models/[model]_analysis.xlsx'\n",
    "            ],\n",
    "            'Chart_Type': [\n",
    "                'Line Chart (CDF)',\n",
    "                'Line Chart (Top Models)',\n",
    "                'Histogram',\n",
    "                'Radar Chart',\n",
    "                'Multiple Charts'\n",
    "            ],\n",
    "            'Key_Worksheet': [\n",
    "                'CDF_Interpolated_Data',\n",
    "                'Top_Models_CDF',\n",
    "                'Best_Model_Histogram',\n",
    "                'Radar_Data',\n",
    "                'Multiple sheets'\n",
    "            ],\n",
    "            'X_Column': [\n",
    "                '[Model]_Error columns',\n",
    "                'Error_Percent',\n",
    "                'Bin_Centers',\n",
    "                'Radar chart (polar)',\n",
    "                'Varies by chart'\n",
    "            ],\n",
    "            'Y_Column': [\n",
    "                '[Model]_CumProb columns',\n",
    "                '[Model]_CumProb columns',\n",
    "                'Counts',\n",
    "                'Metric values',\n",
    "                'Varies by chart'\n",
    "            ],\n",
    "            'Notes': [\n",
    "                'Multiple lines, one per model',\n",
    "                'Top performing models only',\n",
    "                'Add mean/median lines',\n",
    "                'Use polar coordinates',\n",
    "                'Individual model analysis'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        guide_df = pd.DataFrame(guide_data)\n",
    "        guide_df.to_excel(writer, sheet_name='Plotting_Guide', index=False)\n",
    "        \n",
    "        # é¢œè‰²å’Œæ ·å¼å»ºè®®\n",
    "        color_data = []\n",
    "        for i, (model_key, info) in enumerate(list(model_info.items())[:15]):\n",
    "            color_data.append({\n",
    "                'Model': f\"{info['method']}+{info['ml_model']}_K{info['k']}\",\n",
    "                'Suggested_Color': f'RGB({50 + i*15}, {100 + i*10}, {200 - i*8})',\n",
    "                'Line_Style': ['Solid', 'Dash', 'Dot', 'DashDot'][i % 4],\n",
    "                'Line_Width': 2 if info['is_focus_combination'] else 1.5,\n",
    "                'Priority': 'High' if info['is_focus_combination'] else 'Normal'\n",
    "            })\n",
    "        \n",
    "        color_df = pd.DataFrame(color_data)\n",
    "        color_df.to_excel(writer, sheet_name='Style_Suggestions', index=False)\n",
    "    \n",
    "    print(f\"    âœ“ Originç»˜å›¾æŒ‡å—: {os.path.basename(guide_file)}\")\n",
    "    print(f\"    âœ… æ‰€æœ‰Excelæ•°æ®æ–‡ä»¶å·²ç”Ÿæˆå®Œæˆ!\")\n",
    "\n",
    "def create_comprehensive_cdf_plot(model_errors, model_info, output_dir):\n",
    "    \"\"\"åˆ›å»ºç»¼åˆCDFåˆ†æå›¾\"\"\"\n",
    "    print(\"\\n=== åˆ›å»ºç»¼åˆCDFåˆ†æå›¾ ===\")\n",
    "    \n",
    "    # è®¾ç½®å›¾å½¢\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # å®šä¹‰é¢œè‰²æ˜ å°„\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(model_errors)))\n",
    "    \n",
    "    # ç»Ÿè®¡æ•°æ®å­˜å‚¨\n",
    "    stats_data = []\n",
    "    \n",
    "    # å­å›¾1: å®Œæ•´CDFæ›²çº¿\n",
    "    print(\"  ç»˜åˆ¶å®Œæ•´CDFæ›²çº¿...\")\n",
    "    for i, (model_key, errors) in enumerate(model_errors.items()):\n",
    "        info = model_info[model_key]\n",
    "        \n",
    "        # è¿‡æ»¤å¼‚å¸¸å€¼\n",
    "        errors_filtered = errors[errors <= CONFIG['max_error']]\n",
    "        errors_sorted = np.sort(errors_filtered)\n",
    "        \n",
    "        # è®¡ç®—ç´¯ç§¯æ¦‚ç‡\n",
    "        n = len(errors_sorted)\n",
    "        if n > 0:\n",
    "            cumulative_prob = np.arange(1, n + 1) / n\n",
    "            \n",
    "            # åˆ›å»ºæ ‡ç­¾\n",
    "            advanced_mark = \"â˜…\" if info['is_advanced'] else \"\"\n",
    "            focus_mark = \"ğŸ¯\" if info['is_focus_combination'] else \"\"\n",
    "            label = f\"{advanced_mark}{focus_mark}{info['method']}+{info['ml_model']} (K={info['k']})\"\n",
    "            \n",
    "            # ç»˜åˆ¶CDFæ›²çº¿\n",
    "            ax1.plot(errors_sorted, cumulative_prob, \n",
    "                    label=label, linewidth=2, color=colors[i], alpha=0.8)\n",
    "            \n",
    "            # è®¡ç®—ç»Ÿè®¡æ•°æ®\n",
    "            for threshold in CONFIG['error_thresholds']:\n",
    "                freq = np.mean(errors_filtered <= threshold) if len(errors_filtered) > 0 else 0\n",
    "                stats_data.append({\n",
    "                    'Model': model_key,\n",
    "                    'Method': info['method'],\n",
    "                    'ML_Model': info['ml_model'],\n",
    "                    'K': info['k'],\n",
    "                    'Test_R2': info['test_r2'],\n",
    "                    'Threshold': threshold,\n",
    "                    'Frequency': freq,\n",
    "                    'Advanced': info['is_advanced'],\n",
    "                    'Focus': info['is_focus_combination']\n",
    "                })\n",
    "    \n",
    "    # è®¾ç½®å­å›¾1\n",
    "    ax1.set_xlabel('ç›¸å¯¹è¯¯å·® (%)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('ç´¯ç§¯é¢‘ç‡', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('æ‰€æœ‰æ¨¡å‹é¢„æµ‹è¯¯å·®ç´¯ç§¯åˆ†å¸ƒ (CDF)', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax1.set_xlim(0, CONFIG['max_error'])\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # æ·»åŠ å‚è€ƒçº¿\n",
    "    for threshold in [5, 10, 20]:\n",
    "        ax1.axvline(x=threshold, color='red', linestyle='--', alpha=0.5)\n",
    "        ax1.text(threshold, 0.95, f'{threshold}%', rotation=90, \n",
    "                verticalalignment='top', color='red', fontsize=9)\n",
    "    \n",
    "    # å­å›¾2: é¡¶çº§æ¨¡å‹è¯¦ç»†å¯¹æ¯”\n",
    "    print(\"  ç»˜åˆ¶é¡¶çº§æ¨¡å‹è¯¦ç»†å¯¹æ¯”...\")\n",
    "    sorted_models = sorted(model_errors.items(), \n",
    "                          key=lambda x: model_info[x[0]]['test_r2'], \n",
    "                          reverse=True)\n",
    "    top_models = sorted_models[:CONFIG['top_models_count']]\n",
    "    \n",
    "    line_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']\n",
    "    linestyles = ['-', '--', '-.', ':', '-', '--', '-.', ':']\n",
    "    \n",
    "    for i, (model_key, errors) in enumerate(top_models):\n",
    "        info = model_info[model_key]\n",
    "        errors_filtered = errors[errors <= 35]\n",
    "        errors_sorted = np.sort(errors_filtered)\n",
    "        \n",
    "        n = len(errors_sorted)\n",
    "        if n > 0:\n",
    "            cumulative_prob = np.arange(1, n + 1) / n\n",
    "            \n",
    "            method_short = info['method'].replace('F-Regression', 'F-Reg').replace('Mutual Information', 'MutInfo')\n",
    "            model_short = info['ml_model'].replace('GradientBoosting', 'GB').replace('RandomForest', 'RF')\n",
    "            advanced_mark = \"â˜…\" if info['is_advanced'] else \"\"\n",
    "            \n",
    "            label = f\"{advanced_mark}{method_short}+{model_short} (RÂ²={info['test_r2']:.3f})\"\n",
    "            \n",
    "            ax2.plot(errors_sorted, cumulative_prob, \n",
    "                    label=label, \n",
    "                    color=line_colors[i % len(line_colors)], \n",
    "                    linestyle=linestyles[i % len(linestyles)],\n",
    "                    linewidth=2.5, alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel('ç›¸å¯¹è¯¯å·® (%)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('ç´¯ç§¯é¢‘ç‡', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title(f'é¡¶çº§{CONFIG[\"top_models_count\"]}ä¸ªæ¨¡å‹è¯¯å·®å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(loc='lower right', fontsize=9)\n",
    "    ax2.set_xlim(0, 35)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # æ·»åŠ å‚è€ƒçº¿\n",
    "    for threshold in [5, 10, 20]:\n",
    "        ax2.axvline(x=threshold, color='gray', linestyle=':', alpha=0.7)\n",
    "        ax2.text(threshold, 0.02, f'{threshold}%', horizontalalignment='center', \n",
    "                verticalalignment='bottom', color='gray', fontsize=9)\n",
    "    \n",
    "    # å­å›¾3: è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆä»¥æœ€ä¼˜æ¨¡å‹ä¸ºä¾‹ï¼‰\n",
    "    print(\"  ç»˜åˆ¶è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾...\")\n",
    "    best_model_key = max(model_errors.keys(), key=lambda x: model_info[x]['test_r2'])\n",
    "    best_errors = model_errors[best_model_key]\n",
    "    \n",
    "    ax3.hist(best_errors[best_errors <= 30], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax3.axvline(np.mean(best_errors), color='red', linestyle='--', linewidth=2, label=f'å¹³å‡å€¼: {np.mean(best_errors):.2f}%')\n",
    "    ax3.axvline(np.median(best_errors), color='orange', linestyle='--', linewidth=2, label=f'ä¸­ä½æ•°: {np.median(best_errors):.2f}%')\n",
    "    ax3.set_xlabel('ç›¸å¯¹è¯¯å·® (%)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('é¢‘æ¬¡', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title(f'æœ€ä¼˜æ¨¡å‹è¯¯å·®åˆ†å¸ƒ\\n({model_info[best_model_key][\"method\"]}+{model_info[best_model_key][\"ml_model\"]})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # å­å›¾4: æ¨¡å‹æ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾\n",
    "    print(\"  ç»˜åˆ¶æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾...\")\n",
    "    \n",
    "    # é€‰æ‹©å‰6ä¸ªæ¨¡å‹è¿›è¡Œé›·è¾¾å›¾å¯¹æ¯”\n",
    "    radar_models = sorted_models[:6]\n",
    "    metrics = ['Test_R2', 'Freq_5pct', 'Freq_10pct', 'Freq_20pct', 'Low_Error_Rate']\n",
    "    \n",
    "    # è®¡ç®—æŒ‡æ ‡\n",
    "    radar_data = []\n",
    "    model_names = []\n",
    "    \n",
    "    for model_key, _ in radar_models:\n",
    "        info = model_info[model_key]\n",
    "        errors = model_errors[model_key]\n",
    "        \n",
    "        # è®¡ç®—å„é¡¹æŒ‡æ ‡ï¼ˆå½’ä¸€åŒ–åˆ°0-1ï¼‰\n",
    "        test_r2 = info['test_r2']\n",
    "        freq_5 = np.mean(errors <= 5)\n",
    "        freq_10 = np.mean(errors <= 10) \n",
    "        freq_20 = np.mean(errors <= 20)\n",
    "        low_error_rate = np.mean(errors <= 15)  # ä½è¯¯å·®ç‡\n",
    "        \n",
    "        radar_data.append([test_r2, freq_5, freq_10, freq_20, low_error_rate])\n",
    "        \n",
    "        # ç®€åŒ–æ¨¡å‹åç§°\n",
    "        method_short = info['method'][:5]\n",
    "        model_short = info['ml_model'][:5]\n",
    "        model_names.append(f\"{method_short}+{model_short}\")\n",
    "    \n",
    "    # åˆ›å»ºé›·è¾¾å›¾\n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # é—­åˆ\n",
    "    \n",
    "    ax4 = plt.subplot(2, 2, 4, projection='polar')\n",
    "    \n",
    "    for i, (data, name) in enumerate(zip(radar_data, model_names)):\n",
    "        values = data + data[:1]  # é—­åˆæ•°æ®\n",
    "        ax4.plot(angles, values, 'o-', linewidth=2, label=name, color=line_colors[i % len(line_colors)])\n",
    "        ax4.fill(angles, values, alpha=0.25, color=line_colors[i % len(line_colors)])\n",
    "    \n",
    "    ax4.set_xticks(angles[:-1])\n",
    "    ax4.set_xticklabels(['RÂ²', '5%è¯¯å·®', '10%è¯¯å·®', '20%è¯¯å·®', 'ä½è¯¯å·®ç‡'])\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.set_title('æ¨¡å‹ç»¼åˆæ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜å›¾å½¢\n",
    "    output_path = os.path.join(output_dir, 'figures', 'comprehensive_cdf_analysis.png')\n",
    "    plt.savefig(output_path, dpi=CONFIG['figure_dpi'], bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    print(f\"âœ“ ç»¼åˆCDFå›¾å·²ä¿å­˜: {os.path.basename(output_path)}\")\n",
    "    \n",
    "    plt.close()\n",
    "    return stats_data\n",
    "\n",
    "def create_individual_model_plots(model_errors, model_info, detailed_stats, output_dir):\n",
    "    \"\"\"ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºå•ç‹¬çš„åˆ†æå›¾\"\"\"\n",
    "    print(\"\\n=== åˆ›å»ºå•ç‹¬æ¨¡å‹åˆ†æå›¾ ===\")\n",
    "    \n",
    "    individual_dir = os.path.join(output_dir, 'figures', 'individual_models')\n",
    "    os.makedirs(individual_dir, exist_ok=True)\n",
    "    \n",
    "    for i, (model_key, errors) in enumerate(model_errors.items()):\n",
    "        info = model_info[model_key]\n",
    "        stats = detailed_stats.get(model_key, {})\n",
    "        \n",
    "        print(f\"  [{i+1}/{len(model_errors)}] åˆ›å»º {model_key} çš„åˆ†æå›¾...\")\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # å­å›¾1: CDFæ›²çº¿\n",
    "        errors_filtered = errors[errors <= 40]\n",
    "        errors_sorted = np.sort(errors_filtered)\n",
    "        n = len(errors_sorted)\n",
    "        if n > 0:\n",
    "            cumulative_prob = np.arange(1, n + 1) / n\n",
    "            ax1.plot(errors_sorted, cumulative_prob, linewidth=3, color='blue')\n",
    "            ax1.fill_between(errors_sorted, cumulative_prob, alpha=0.3, color='lightblue')\n",
    "        \n",
    "        ax1.set_xlabel('ç›¸å¯¹è¯¯å·® (%)', fontweight='bold')\n",
    "        ax1.set_ylabel('ç´¯ç§¯é¢‘ç‡', fontweight='bold')\n",
    "        ax1.set_title('è¯¯å·®ç´¯ç§¯åˆ†å¸ƒå‡½æ•° (CDF)', fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_xlim(0, 40)\n",
    "        \n",
    "        # æ·»åŠ ç»Ÿè®¡çº¿\n",
    "        mean_error = np.mean(errors_filtered)\n",
    "        median_error = np.median(errors_filtered)\n",
    "        ax1.axvline(mean_error, color='red', linestyle='--', label=f'å¹³å‡å€¼: {mean_error:.2f}%')\n",
    "        ax1.axvline(median_error, color='orange', linestyle='--', label=f'ä¸­ä½æ•°: {median_error:.2f}%')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # å­å›¾2: è¯¯å·®ç›´æ–¹å›¾\n",
    "        ax2.hist(errors_filtered, bins=25, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        ax2.axvline(mean_error, color='red', linestyle='--', linewidth=2)\n",
    "        ax2.axvline(median_error, color='orange', linestyle='--', linewidth=2)\n",
    "        ax2.set_xlabel('ç›¸å¯¹è¯¯å·® (%)', fontweight='bold')\n",
    "        ax2.set_ylabel('é¢‘æ¬¡', fontweight='bold')\n",
    "        ax2.set_title('è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾', fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # å­å›¾3: é¢„æµ‹vså®é™…å€¼æ•£ç‚¹å›¾\n",
    "        if 'y_true' in stats and 'y_pred' in stats:\n",
    "            y_true = stats['y_true']\n",
    "            y_pred = stats['y_pred']\n",
    "            \n",
    "            ax3.scatter(y_true, y_pred, alpha=0.6, color='purple')\n",
    "            \n",
    "            # æ·»åŠ å®Œç¾é¢„æµ‹çº¿\n",
    "            min_val = min(np.min(y_true), np.min(y_pred))\n",
    "            max_val = max(np.max(y_true), np.max(y_pred))\n",
    "            ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹')\n",
    "            \n",
    "            ax3.set_xlabel('å®é™…å€¼', fontweight='bold')\n",
    "            ax3.set_ylabel('é¢„æµ‹å€¼', fontweight='bold')\n",
    "            ax3.set_title(f'é¢„æµ‹vså®é™…å€¼ (RÂ²={stats.get(\"r2\", 0):.4f})', fontweight='bold')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            ax3.legend()\n",
    "        \n",
    "        # å­å›¾4: æ®‹å·®å›¾\n",
    "        if 'residuals' in stats:\n",
    "            residuals = stats['residuals']\n",
    "            y_pred = stats['y_pred']\n",
    "            \n",
    "            ax4.scatter(y_pred, residuals, alpha=0.6, color='brown')\n",
    "            ax4.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "            ax4.set_xlabel('é¢„æµ‹å€¼', fontweight='bold')\n",
    "            ax4.set_ylabel('æ®‹å·®', fontweight='bold')\n",
    "            ax4.set_title('æ®‹å·®åˆ†å¸ƒå›¾', fontweight='bold')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ æ€»æ ‡é¢˜å’Œä¿¡æ¯\n",
    "        advanced_mark = \"â˜…é«˜çº§æ–¹æ³• \" if info['is_advanced'] else \"\"\n",
    "        focus_mark = \"ğŸ¯é‡ç‚¹ç»„åˆ \" if info['is_focus_combination'] else \"\"\n",
    "        suptitle = f\"{advanced_mark}{focus_mark}{info['method']} + {info['ml_model']} (K={info['k']})\\n\"\n",
    "        suptitle += f\"Test RÂ²={info['test_r2']:.4f}, MAE={info['test_mae']:.4f}, å¹³å‡ç›¸å¯¹è¯¯å·®={np.mean(errors):.2f}%\"\n",
    "        \n",
    "        fig.suptitle(suptitle, fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # ä¿å­˜å›¾å½¢\n",
    "        safe_filename = model_key.replace('/', '_').replace('\\\\', '_').replace(':', '_')\n",
    "        output_path = os.path.join(individual_dir, f'{safe_filename}_analysis.png')\n",
    "        plt.savefig(output_path, dpi=CONFIG['figure_dpi'], bbox_inches='tight', \n",
    "                    facecolor='white', edgecolor='none')\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"âœ“ å·²åˆ›å»º {len(model_errors)} ä¸ªå•ç‹¬æ¨¡å‹åˆ†æå›¾\")\n",
    "\n",
    "def create_summary_statistics(model_errors, model_info, output_dir):\n",
    "    \"\"\"åˆ›å»ºæ±‡æ€»ç»Ÿè®¡æŠ¥å‘Š\"\"\"\n",
    "    print(\"\\n=== åˆ›å»ºæ±‡æ€»ç»Ÿè®¡æŠ¥å‘Š ===\")\n",
    "    \n",
    "    stats_dir = os.path.join(output_dir, 'statistics')\n",
    "    \n",
    "    # åˆ›å»ºæ±‡æ€»ç»Ÿè®¡è¡¨\n",
    "    summary_stats = []\n",
    "    \n",
    "    for model_key, errors in model_errors.items():\n",
    "        info = model_info[model_key]\n",
    "        \n",
    "        # åŸºæœ¬ç»Ÿè®¡\n",
    "        basic_stats = {\n",
    "            'Model_Key': model_key,\n",
    "            'Method': info['method'],\n",
    "            'ML_Model': info['ml_model'],\n",
    "            'K': info['k'],\n",
    "            'Test_R2': info['test_r2'],\n",
    "            'Test_MAE': info['test_mae'],\n",
    "            'Test_RMSLE': info['test_rmsle'],\n",
    "            'Is_Advanced': info['is_advanced'],\n",
    "            'Is_Focus_Combination': info['is_focus_combination'],\n",
    "            'Sample_Count': len(errors),\n",
    "            'Mean_Relative_Error': np.mean(errors),\n",
    "            'Median_Relative_Error': np.median(errors),\n",
    "            'Std_Relative_Error': np.std(errors),\n",
    "            'Min_Relative_Error': np.min(errors),\n",
    "            'Max_Relative_Error': np.max(errors),\n",
    "            'Q25_Relative_Error': np.percentile(errors, 25),\n",
    "            'Q75_Relative_Error': np.percentile(errors, 75),\n",
    "            'IQR_Relative_Error': np.percentile(errors, 75) - np.percentile(errors, 25)\n",
    "        }\n",
    "        \n",
    "        # ä¸åŒé˜ˆå€¼ä¸‹çš„é¢‘ç‡\n",
    "        for threshold in CONFIG['error_thresholds']:\n",
    "            basic_stats[f'Freq_Below_{threshold}pct'] = np.mean(errors <= threshold)\n",
    "        \n",
    "        # ç‰¹æ®Šç»Ÿè®¡\n",
    "        basic_stats['Freq_Above_30pct'] = np.mean(errors > 30)\n",
    "        basic_stats['Freq_Above_50pct'] = np.mean(errors > 50)\n",
    "        basic_stats['Excellent_Predictions'] = np.mean(errors <= 5)  # ä¼˜ç§€é¢„æµ‹ç‡\n",
    "        basic_stats['Good_Predictions'] = np.mean(errors <= 10)      # è‰¯å¥½é¢„æµ‹ç‡\n",
    "        basic_stats['Acceptable_Predictions'] = np.mean(errors <= 20) # å¯æ¥å—é¢„æµ‹ç‡\n",
    "        \n",
    "        summary_stats.append(basic_stats)\n",
    "    \n",
    "    # è½¬æ¢ä¸ºDataFrameå¹¶æ’åº\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    summary_df = summary_df.sort_values('Test_R2', ascending=False)\n",
    "    \n",
    "    # ä¿å­˜è¯¦ç»†ç»Ÿè®¡è¡¨\n",
    "    detailed_stats_file = os.path.join(stats_dir, 'detailed_model_statistics.csv')\n",
    "    summary_df.to_csv(detailed_stats_file, index=False, encoding='utf-8')\n",
    "    print(f\"âœ“ è¯¦ç»†ç»Ÿè®¡è¡¨å·²ä¿å­˜: {os.path.basename(detailed_stats_file)}\")\n",
    "    \n",
    "    # åˆ›å»ºæ’åç»Ÿè®¡\n",
    "    ranking_stats = {\n",
    "        'Best_R2_Model': summary_df.iloc[0]['Model_Key'],\n",
    "        'Best_R2_Value': summary_df.iloc[0]['Test_R2'],\n",
    "        'Best_MAE_Model': summary_df.loc[summary_df['Test_MAE'].idxmin()]['Model_Key'],\n",
    "        'Best_MAE_Value': summary_df['Test_MAE'].min(),\n",
    "        'Lowest_Mean_Error_Model': summary_df.loc[summary_df['Mean_Relative_Error'].idxmin()]['Model_Key'],\n",
    "        'Lowest_Mean_Error_Value': summary_df['Mean_Relative_Error'].min(),\n",
    "        'Most_Stable_Model': summary_df.loc[summary_df['Std_Relative_Error'].idxmin()]['Model_Key'],\n",
    "        'Most_Stable_Value': summary_df['Std_Relative_Error'].min(),\n",
    "        'Best_Excellent_Rate_Model': summary_df.loc[summary_df['Excellent_Predictions'].idxmax()]['Model_Key'],\n",
    "        'Best_Excellent_Rate_Value': summary_df['Excellent_Predictions'].max(),\n",
    "        'Total_Models': len(summary_stats),\n",
    "        'Advanced_Methods_Count': sum(1 for s in summary_stats if s['Is_Advanced']),\n",
    "        'Focus_Combinations_Count': sum(1 for s in summary_stats if s['Is_Focus_Combination'])\n",
    "    }\n",
    "    \n",
    "    # ä¿å­˜æ’åç»Ÿè®¡\n",
    "    ranking_file = os.path.join(stats_dir, 'ranking_statistics.txt')\n",
    "    with open(ranking_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== æ¨¡å‹æ’åç»Ÿè®¡ ===\\n\\n\")\n",
    "        f.write(f\"æœ€é«˜RÂ²æ¨¡å‹: {ranking_stats['Best_R2_Model']} (RÂ²={ranking_stats['Best_R2_Value']:.4f})\\n\")\n",
    "        f.write(f\"æœ€ä½MAEæ¨¡å‹: {ranking_stats['Best_MAE_Model']} (MAE={ranking_stats['Best_MAE_Value']:.4f})\\n\")\n",
    "        f.write(f\"æœ€ä½å¹³å‡è¯¯å·®æ¨¡å‹: {ranking_stats['Lowest_Mean_Error_Model']} (å¹³å‡è¯¯å·®={ranking_stats['Lowest_Mean_Error_Value']:.2f}%)\\n\")\n",
    "        f.write(f\"æœ€ç¨³å®šæ¨¡å‹: {ranking_stats['Most_Stable_Model']} (è¯¯å·®æ ‡å‡†å·®={ranking_stats['Most_Stable_Value']:.2f}%)\\n\")\n",
    "        f.write(f\"æœ€é«˜ä¼˜ç§€é¢„æµ‹ç‡æ¨¡å‹: {ranking_stats['Best_Excellent_Rate_Model']} (ä¼˜ç§€ç‡={ranking_stats['Best_Excellent_Rate_Value']:.3f})\\n\")\n",
    "        f.write(f\"\\n=== æ•´ä½“ç»Ÿè®¡ ===\\n\")\n",
    "        f.write(f\"æ¨¡å‹æ€»æ•°: {ranking_stats['Total_Models']}\\n\")\n",
    "        f.write(f\"é«˜çº§æ–¹æ³•æ¨¡å‹æ•°: {ranking_stats['Advanced_Methods_Count']}\\n\")\n",
    "        f.write(f\"é‡ç‚¹ç»„åˆæ¨¡å‹æ•°: {ranking_stats['Focus_Combinations_Count']}\\n\")\n",
    "        \n",
    "        # æ·»åŠ æ•´ä½“æ€§èƒ½ç»Ÿè®¡\n",
    "        f.write(f\"\\n=== æ•´ä½“æ€§èƒ½ç»Ÿè®¡ ===\\n\")\n",
    "        f.write(f\"å¹³å‡RÂ²: {summary_df['Test_R2'].mean():.4f}\\n\")\n",
    "        f.write(f\"å¹³å‡MAE: {summary_df['Test_MAE'].mean():.4f}\\n\")\n",
    "        f.write(f\"å¹³å‡ç›¸å¯¹è¯¯å·®: {summary_df['Mean_Relative_Error'].mean():.2f}%\\n\")\n",
    "        f.write(f\"å¹³å‡ä¼˜ç§€é¢„æµ‹ç‡: {summary_df['Excellent_Predictions'].mean():.3f}\\n\")\n",
    "        f.write(f\"å¹³å‡è‰¯å¥½é¢„æµ‹ç‡: {summary_df['Good_Predictions'].mean():.3f}\\n\")\n",
    "        f.write(f\"å¹³å‡å¯æ¥å—é¢„æµ‹ç‡: {summary_df['Acceptable_Predictions'].mean():.3f}\\n\")\n",
    "    \n",
    "    print(f\"âœ“ æ’åç»Ÿè®¡å·²ä¿å­˜: {os.path.basename(ranking_file)}\")\n",
    "    \n",
    "    return summary_df, ranking_stats\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"æ¨¡å‹é¢„æµ‹è¯¯å·®CDFåˆ†æç¨‹åº - é€‚é…ä¿®å¤ç‰ˆ\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"è¾“å…¥è·¯å¾„: {CONFIG['input_path']}\")\n",
    "    print(f\"è¾“å‡ºè·¯å¾„: {CONFIG['output_path']}\")\n",
    "    print(f\"æ—¶é—´æˆ³: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # 1. è®¾ç½®è¾“å‡ºç›®å½•\n",
    "        output_dir = setup_output_directory()\n",
    "        \n",
    "        # 2. åŠ è½½æ•°æ®\n",
    "        best_models, data_info, features_name = load_model_data()\n",
    "        \n",
    "        # 3. è®¡ç®—è¯¯å·®\n",
    "        model_errors, model_info, detailed_stats = calculate_model_errors(best_models, data_info)\n",
    "        \n",
    "        if not model_errors:\n",
    "            print(\"âŒ æ²¡æœ‰å¯åˆ†æçš„æ¨¡å‹è¯¯å·®æ•°æ®\")\n",
    "            return\n",
    "        \n",
    "        # 4. ä¿å­˜æ•°æ®æ–‡ä»¶\n",
    "        summary_df = save_data_files(model_errors, model_info, detailed_stats, output_dir)\n",
    "        \n",
    "        # 5. åˆ›å»ºç»¼åˆCDFå›¾\n",
    "        stats_data = create_comprehensive_cdf_plot(model_errors, model_info, output_dir)\n",
    "        \n",
    "        # 6. åˆ›å»ºå•ç‹¬æ¨¡å‹åˆ†æå›¾\n",
    "        create_individual_model_plots(model_errors, model_info, detailed_stats, output_dir)\n",
    "        \n",
    "        # 7. åˆ›å»ºæ±‡æ€»ç»Ÿè®¡\n",
    "        summary_df, ranking_stats = create_summary_statistics(model_errors, model_info, output_dir)\n",
    "        \n",
    "        # 8. ä¿å­˜åˆ†æé…ç½®\n",
    "        config_file = os.path.join(output_dir, 'analysis_config.txt')\n",
    "        with open(config_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=== CDFåˆ†æé…ç½® ===\\n\")\n",
    "            f.write(f\"è¾“å…¥è·¯å¾„: {CONFIG['input_path']}\\n\")\n",
    "            f.write(f\"æ–‡ä»¶åç¼€: {CONFIG['file_suffix']}\\n\")\n",
    "            f.write(f\"æœ€å¤§è¯¯å·®é˜ˆå€¼: {CONFIG['max_error']}%\\n\")\n",
    "            f.write(f\"é¡¶çº§æ¨¡å‹æ•°é‡: {CONFIG['top_models_count']}\\n\")\n",
    "            f.write(f\"è¯¯å·®é˜ˆå€¼åˆ—è¡¨: {CONFIG['error_thresholds']}\\n\")\n",
    "            f.write(f\"å›¾ç‰‡åˆ†è¾¨ç‡: {CONFIG['figure_dpi']} DPI\\n\")\n",
    "            f.write(f\"è¯¦ç»†ç»Ÿè®¡: {'å¯ç”¨' if CONFIG['enable_detailed_stats'] else 'ç¦ç”¨'}\\n\")\n",
    "            f.write(f\"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"\\n=== Origin Excelæ–‡ä»¶è¯´æ˜ ===\\n\")\n",
    "            f.write(f\"00_Origin_Plotting_Guide.xlsx - åŒ…å«ç»˜å›¾æŒ‡å—å’Œæ ·å¼å»ºè®®\\n\")\n",
    "            f.write(f\"01_All_Models_CDF_Data.xlsx - æ‰€æœ‰æ¨¡å‹çš„CDFæ•°æ®\\n\")\n",
    "            f.write(f\"  â”œâ”€ CDF_Interpolated_Data: æ’å€¼åçš„ç»Ÿä¸€æ•°æ® (æ¨èç”¨äºç»˜å›¾)\\n\")\n",
    "            f.write(f\"  â”œâ”€ CDF_Raw_Data_Points: åŸå§‹CDFæ•°æ®ç‚¹\\n\")\n",
    "            f.write(f\"  â””â”€ Model_Info_and_Stats: æ¨¡å‹ä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®\\n\")\n",
    "            f.write(f\"02_Top_Models_Comparison.xlsx - é¡¶çº§æ¨¡å‹å¯¹æ¯”æ•°æ®\\n\")\n",
    "            f.write(f\"  â”œâ”€ Top_Models_CDF: é¡¶çº§æ¨¡å‹CDFæ•°æ®\\n\")\n",
    "            f.write(f\"  â””â”€ Top_Models_Stats: é¡¶çº§æ¨¡å‹ç»Ÿè®¡å¯¹æ¯”\\n\")\n",
    "            f.write(f\"03_Error_Distribution_Histograms.xlsx - è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾æ•°æ®\\n\")\n",
    "            f.write(f\"  â”œâ”€ Best_Model_Histogram: æœ€ä¼˜æ¨¡å‹çš„è¯¯å·®åˆ†å¸ƒ\\n\")\n",
    "            f.write(f\"  â””â”€ All_Models_Histograms: æ‰€æœ‰æ¨¡å‹çš„ç›´æ–¹å›¾æ•°æ®\\n\")\n",
    "            f.write(f\"04_Radar_Chart_Data.xlsx - é›·è¾¾å›¾æ•°æ®\\n\")\n",
    "            f.write(f\"  â”œâ”€ Radar_Data: é›·è¾¾å›¾æ•°å€¼æ•°æ®\\n\")\n",
    "            f.write(f\"  â””â”€ Radar_Config: é›·è¾¾å›¾é…ç½®ä¿¡æ¯\\n\")\n",
    "            f.write(f\"individual_models/[model]_analysis.xlsx - å•ç‹¬æ¨¡å‹åˆ†ææ•°æ®\\n\")\n",
    "            f.write(f\"  â”œâ”€ CDF_Data: å•ä¸ªæ¨¡å‹çš„CDFæ•°æ®\\n\")\n",
    "            f.write(f\"  â”œâ”€ Histogram_Data: è¯¯å·®åˆ†å¸ƒæ•°æ®\\n\")\n",
    "            f.write(f\"  â”œâ”€ Prediction_Scatter: é¢„æµ‹vså®é™…æ•£ç‚¹å›¾æ•°æ®\\n\")\n",
    "            f.write(f\"  â”œâ”€ Perfect_Prediction_Line: å®Œç¾é¢„æµ‹çº¿æ•°æ®\\n\")\n",
    "            f.write(f\"  â”œâ”€ Residual_Data: æ®‹å·®æ•£ç‚¹å›¾æ•°æ®\\n\")\n",
    "            f.write(f\"  â”œâ”€ Zero_Reference_Line: é›¶çº¿å‚è€ƒæ•°æ®\\n\")\n",
    "            f.write(f\"  â””â”€ Model_Info: æ¨¡å‹åŸºæœ¬ä¿¡æ¯\\n\")\n",
    "            f.write(f\"\\n=== Originç»˜å›¾å»ºè®® ===\\n\")\n",
    "            f.write(f\"1. CDFå›¾: ä½¿ç”¨01æ–‡ä»¶çš„CDF_Interpolated_Dataå·¥ä½œè¡¨\\n\")\n",
    "            f.write(f\"2. é¢œè‰²æ–¹æ¡ˆ: å‚è€ƒ00æ–‡ä»¶çš„Style_Suggestionså·¥ä½œè¡¨\\n\")\n",
    "            f.write(f\"3. çº¿å‹è®¾ç½®: é«˜çº§æ–¹æ³•ç”¨å®çº¿ï¼Œé‡ç‚¹ç»„åˆç”¨ç²—çº¿\\n\")\n",
    "            f.write(f\"4. åæ ‡è½´: Xè½´0-{CONFIG['max_error']}%ï¼ŒYè½´0-1\\n\")\n",
    "            f.write(f\"5. å‚è€ƒçº¿: åœ¨5%, 10%, 20%å¤„æ·»åŠ å‚ç›´å‚è€ƒçº¿\\n\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"=== CDFåˆ†æå®Œæˆ ===\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"âœ“ æˆåŠŸåˆ†æ {len(model_errors)} ä¸ªæ¨¡å‹\")\n",
    "        print(f\"âœ“ æœ€ä¼˜æ¨¡å‹: {ranking_stats['Best_R2_Model']} (RÂ²={ranking_stats['Best_R2_Value']:.4f})\")\n",
    "        print(f\"âœ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}\")\n",
    "        \n",
    "        print(f\"\\nç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "        print(f\"ğŸ“Š figures/comprehensive_cdf_analysis.png - ç»¼åˆCDFåˆ†æå›¾\")\n",
    "        print(f\"ğŸ“Š figures/individual_models/ - å•ç‹¬æ¨¡å‹åˆ†æå›¾ ({len(model_errors)}ä¸ª)\")\n",
    "        print(f\"ğŸ“„ data/model_summary_with_errors.csv - æ¨¡å‹æ±‡æ€»è¡¨\")\n",
    "        print(f\"ğŸ“„ data/detailed_statistics.pkl - è¯¦ç»†ç»Ÿè®¡æ•°æ®\")\n",
    "        print(f\"ğŸ“„ statistics/detailed_model_statistics.csv - è¯¦ç»†ç»Ÿè®¡è¡¨\")\n",
    "        print(f\"ğŸ“„ statistics/ranking_statistics.txt - æ’åç»Ÿè®¡\")\n",
    "        print(f\"ğŸ“„ analysis_config.txt - åˆ†æé…ç½®\")\n",
    "        print(f\"\\nğŸ“ˆ Originç»˜å›¾ä¸“ç”¨Excelæ–‡ä»¶:\")\n",
    "        print(f\"ğŸ“Š excel_for_origin/00_Origin_Plotting_Guide.xlsx - ç»˜å›¾æŒ‡å—\")\n",
    "        print(f\"ğŸ“Š excel_for_origin/01_All_Models_CDF_Data.xlsx - æ‰€æœ‰æ¨¡å‹CDFæ•°æ®\")\n",
    "        print(f\"ğŸ“Š excel_for_origin/02_Top_Models_Comparison.xlsx - é¡¶çº§æ¨¡å‹å¯¹æ¯”æ•°æ®\")\n",
    "        print(f\"ğŸ“Š excel_for_origin/03_Error_Distribution_Histograms.xlsx - è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾æ•°æ®\")\n",
    "        print(f\"ğŸ“Š excel_for_origin/04_Radar_Chart_Data.xlsx - é›·è¾¾å›¾æ•°æ®\")\n",
    "        print(f\"ğŸ“Š excel_for_origin/individual_models/ - å•ç‹¬æ¨¡å‹åˆ†ææ•°æ® (å‰5ä¸ªæ¨¡å‹)\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ é‡ç‚¹å‘ç°:\")\n",
    "        print(f\"   æœ€ä½³RÂ²: {ranking_stats['Best_R2_Value']:.4f} ({ranking_stats['Best_R2_Model']})\")\n",
    "        print(f\"   æœ€ä½MAE: {ranking_stats['Best_MAE_Value']:.4f} ({ranking_stats['Best_MAE_Model']})\")\n",
    "        print(f\"   å¹³å‡ä¼˜ç§€é¢„æµ‹ç‡(â‰¤5%è¯¯å·®): {summary_df['Excellent_Predictions'].mean():.3f}\")\n",
    "        print(f\"   é«˜çº§æ–¹æ³•æ¨¡å‹å æ¯”: {ranking_stats['Advanced_Methods_Count']}/{ranking_stats['Total_Models']}\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ Originä½¿ç”¨è¯´æ˜:\")\n",
    "        print(f\"   1. é¦–å…ˆæ‰“å¼€ 00_Origin_Plotting_Guide.xlsx æŸ¥çœ‹ç»˜å›¾æŒ‡å—\")\n",
    "        print(f\"   2. ä¸»è¦CDFå›¾ä½¿ç”¨ 01_All_Models_CDF_Data.xlsx\")\n",
    "        print(f\"   3. æ¯ä¸ªExcelæ–‡ä»¶åŒ…å«å¤šä¸ªå·¥ä½œè¡¨ï¼Œé€‰æ‹©åˆé€‚çš„æ•°æ®å·¥ä½œè¡¨\")\n",
    "        print(f\"   4. æ‰€æœ‰æ•°æ®å·²ç»é¢„å¤„ç†ï¼Œå¯ç›´æ¥ç”¨äºOriginç»˜å›¾\")\n",
    "        print(f\"   5. æ–‡ä»¶åŒ…å«é¢œè‰²å’Œæ ·å¼å»ºè®®ï¼Œä¾¿äºåˆ›å»ºä¸“ä¸šå›¾è¡¨\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # ä¿å­˜é”™è¯¯æ—¥å¿—\n",
    "        error_log = os.path.join(CONFIG['output_path'], 'error_log.txt')\n",
    "        os.makedirs(os.path.dirname(error_log), exist_ok=True)\n",
    "        with open(error_log, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"é”™è¯¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"é”™è¯¯ä¿¡æ¯: {str(e)}\\n\\n\")\n",
    "            f.write(\"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\\n\")\n",
    "            f.write(traceback.format_exc())\n",
    "        print(f\"é”™è¯¯æ—¥å¿—å·²ä¿å­˜: {error_log}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bdf63-a78f-4e49-a5bc-d968721dd42a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
