import pandas as pd
from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_squared_log_error
from sklearn.linear_model import (Lasso, Ridge, ElasticNet, BayesianRidge,
                                  HuberRegressor, LinearRegression)
from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,
                              HistGradientBoostingRegressor, AdaBoostRegressor,
                              BaggingRegressor, VotingRegressor)
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from sklearn.feature_selection import (RFE, mutual_info_regression, f_regression,
                                        SelectKBest, SelectFromModel, f_classif)
from sklearn.preprocessing import StandardScaler
import numpy as np
from scipy.stats import pearsonr
import os
import warnings
import pickle
import joblib
from joblib import Parallel, delayed
import multiprocessing
import random
import time
import hashlib  # ç”¨äºæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥

warnings.filterwarnings('ignore')

# è·å–CPUæ ¸å¿ƒæ•°
n_cores = multiprocessing.cpu_count()
print(f"æ£€æµ‹åˆ° {n_cores} ä¸ªCPUæ ¸å¿ƒï¼Œå°†ä½¿ç”¨å¹¶è¡Œè®¡ç®—")

# ============== é…ç½®å‚æ•° ==============
CONFIG = {
    'data_file': "D:\\æ–‡æˆæ•°æ®åº“\\Nb-Siæ•°æ®åº“3.4-æˆåˆ†-æ€§èƒ½.xlsx",
    'output_dir': "D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\æ¨¡å‹è®­ç»ƒ_K4567_9.3",
    'target_col': 'KQ',
    'exclude_cols': ['ID', 'Sample_Name', 'Notes'],  # éœ€è¦æ’é™¤çš„åˆ—
    'forbidden_features_for_final_model': ['Nb', 'Si', 'Ti', 'Al', 'Cr', 'Hf', 'Zr', 'Mo', 'V', 'W', 'Ta', 'Sn'],  # æœ€ç»ˆæ¨¡å‹ä¸­ç¦æ­¢çš„å…ƒç´ ç‰¹å¾
    'max_k': 40,  # Kçš„æœ€å¤§å€¼é™åˆ¶ï¼ˆä¿®æ”¹ä¸º40ï¼‰
    'target_k_range': [4, 5, 6, 7],  # å¤„ç†K=4,5,6,7
    'min_r2_threshold': 0.905,  # ç›®æ ‡RÂ²ä¸‹é™é˜ˆå€¼
    'max_r2_threshold': 0.99,  # ç›®æ ‡RÂ²ä¸Šé™é˜ˆå€¼ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
    'cv_folds': 5,  # äº¤å‰éªŒè¯æŠ˜æ•°æ”¹ä¸º5æŠ˜
    'random_state': 2023,
    'n_jobs': 4,  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„CPUæ ¸å¿ƒ
    # æ–°å¢é«˜çº§æ–¹æ³•çš„å‚æ•°
    'sfs_max_iter': 20,    # SFSæœ€å¤§è¿­ä»£æ¬¡æ•°
    'sbs_max_iter': 20,    # SBSæœ€å¤§è¿­ä»£æ¬¡æ•°
    'ga_population': 20,   # GAç§ç¾¤å¤§å°
    'ga_generations': 15,  # GAè¿­ä»£ä»£æ•°
    'ga_mutation_rate': 0.1,  # GAå˜å¼‚ç‡
    'ga_crossover_rate': 0.8,  # GAäº¤å‰ç‡
    # ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹å‚æ•°
    'physics_guided_enhancement': True,
    'exact_feature_names': True,           # ä½¿ç”¨ç²¾ç¡®çš„ç‰¹å¾å
    'auto_feature_detection': True,        # è‡ªåŠ¨æ£€æµ‹å¯ç”¨ç‰¹å¾
    'quality_check_enabled': True,         # å¯ç”¨è´¨é‡æ£€æŸ¥
    'correlation_threshold': 0.001,        # æä½ç›¸å…³æ€§é˜ˆå€¼
    'variance_threshold': 1e-12,          # æå°æ–¹å·®é˜ˆå€¼
    # è´å¶æ–¯ä¼˜åŒ–å‚æ•° - å·²ç¦ç”¨
    'bayesian_optimization': False,        # ç¦ç”¨è´å¶æ–¯ä¼˜åŒ–
    'print_all_satisfactory': True,       # æ‰“å°æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„æ¨¡å‹
    # æ–°å¢ï¼šè°ƒè¯•å’Œæ–‡ä»¶ä¿å­˜å‚æ•°
    'save_detailed_predictions': True,    # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
    'enable_data_consistency_check': True, # å¯ç”¨æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
    'debug_mode': True,                    # å¯ç”¨è°ƒè¯•æ¨¡å¼
}

# é‡ç‚¹ä¼˜åŒ–ç­–ç•¥ï¼ˆä¿ç•™ç”¨äºç»Ÿè®¡ï¼Œä½†ä¸å†è¿›è¡Œè´å¶æ–¯ä¼˜åŒ–ï¼‰
FOCUS_MODELS = ['KNeighbors', 'GradientBoosting', 'XGBoost', 'CatBoost', 'RandomForest', 'AdaBoost', 'Bagging']
FOCUS_METHODS = ['SFS', 'GA', 'RandomForest', 'F-Regression', 'PCC']

# ============== ç‰¹å¾åç§°ç®€ç§°æ˜ å°„å­—å…¸ ==============
def create_feature_name_mapping():
    """æ ¹æ®PDFæ–‡æ¡£åˆ›å»ºç‰¹å¾åç§°ç®€ç§°æ˜ å°„"""
    mapping = {
        # æˆåˆ†ç‰¹å¾ (CC)
        'Nb': 'CC1',
        'Si': 'CC2', 
        'Ti': 'CC3',
        'Hf': 'CC4',
        'Zr': 'CC5',
        'Al': 'CC6',
        'Cr': 'CC7',
        'Mo': 'CC8',
        'V': 'CC9',
        'W': 'CC10',
        'Ta': 'CC11',
        'Sn': 'CC12',
        
        # åŸå­ç”µå­æ€§è´¨ (AE)
        'mean_A1 atomic number': 'AE1m',
        'var_A1 atomic number': 'AE1v',
        'mean_E2 electronegativity (Pauling)': 'AE2m',
        'var_E2 electronegativity (Pauling)': 'AE2v',
        'mean_Electrophilicity Index': 'AE3m',
        'var_Electrophilicity Index': 'AE3v',
        'mean_F13 Electron affinity': 'AE4m',
        'var_F13 Electron affinity': 'AE4v',
        'mean_Glawe Number': 'AE5m',
        'var_Glawe Number': 'AE5v',
        
        # åŸå­åŠå¾„ (AR) - æ ¹æ®æ–‡æ¡£æ¨æ–­
        'mean_S13 radii atomic (coordination number 12) (pm)': 'AR1m',
        'var_S13 radii atomic (coordination number 12) (pm)': 'AR1v',
        
        # åŸå­ç£æ€§ (AM) - æ ¹æ®æ–‡æ¡£æ¨æ–­
        'mean_Mass Magnetic Susceptibility': 'AM1m',
        'var_Mass Magnetic Susceptibility': 'AM1v',
        
        # åŸå­æ ¸ (AN) - æ ¹æ®æ–‡æ¡£æ¨æ–­
        'mean_A2 atomic mass': 'AN1m',
        'var_A2 atomic mass': 'AN1v',
        
        # åŸå­é‡é‡ (AW) - æ ¹æ®æ–‡æ¡£æ¨æ–­
        'mean_A3 atomic weight': 'AW1m',
        'var_A3 atomic weight': 'AW1v',
        
        # çƒ­åŠ›å­¦ (TH) - æ ¹æ®æ–‡æ¡£æ¨æ–­
        'mean_C1 temperature melting': 'TH1m',
        'var_C1 temperature melting': 'TH1v',
        'mean_C2 temperature boiling': 'TH2m',
        'var_C2 temperature boiling': 'TH2v',
        'mean_C4 enthalpy melting': 'TH3m',
        'var_C4 enthalpy melting': 'TH3v',
        'mean_C5 enthalpy atomization': 'TH4m',
        'var_C5 enthalpy atomization': 'TH4v',
        
        # æœºæ¢°æ€§èƒ½ (PM) - æ ¹æ®æ–‡æ¡£æ¨æ–­
        'mean_C12 modulus Young': 'PM1m',
        'var_C12 modulus Young': 'PM1v',
        
        # çƒ­å­¦æ€§èƒ½ (PT) - æ ¹æ®æ–‡æ¡£æ¨æ–­
        'mean_C7 thermal conductivity': 'PT1m',
        'var_C7 thermal conductivity': 'PT1v',
        
        # ç”µå­¦æ€§èƒ½ (PE) - æ ¹æ®æ–‡æ¡£æ¨æ–­
        'mean_E1 electrical resistivity': 'PE1m',
        'var_E1 electrical resistivity': 'PE1v',
        
        # å¯†åº¦æ€§èƒ½ (PD) - æ ¹æ®æ–‡æ¡£æ¨æ–­
        'mean_å¯†åº¦': 'PD1m',
        'var_å¯†åº¦': 'PD1v',
        
        # æ™¶æ ¼ç»“æ„ (SL) - æ ¹æ®æ–‡æ¡£æ¨æ–­
        'mean_S10 Lattice Constants a': 'SL1m',
        'var_S10 Lattice Constants a': 'SL1v',
        'mean_S11 Lattice Constants b': 'SL2m',
        'var_S11 Lattice Constants b': 'SL2v',
        'mean_S12 Lattice Constants c': 'SL3m',
        'var_S12 Lattice Constants c': 'SL3v',
        
        # ç©ºé—´ç¾¤ (SS) - æ ¹æ®æ–‡æ¡£æ¨æ–­
        'mean_S14 space group number': 'SS1m',
        'var_S14 space group number': 'SS1v',
        
        # ç‰©ç†å·¥ç¨‹ç‰¹å¾
        # éŸ§æ€§åˆ¤æ® (JT)
        'Pugh_modulus_ratio': 'JT1',
        'Poisson_ratio_predicted': 'JT2', 
        'Cauchy_pressure_indicator': 'JT3',
        'Si_supersaturation': 'JT4',
        'atomic_size_mismatch_NbSi': 'JT5',
        
        # ç”µå­ç»“æ„åˆ¤æ® (JE)
        'VEC_Omega_coupling': 'JE1',
        'VEC_stabilization': 'JE2',
        'VEC_deviation_from_ideal': 'JE3',
        'electron_localization': 'JE4',
        
        # çƒ­åŠ›å­¦åˆ¤æ® (JH)
        'phase_stability_index': 'JH1',
        'thermodynamic_driving_force': 'JH2',
        'entropy_stabilization': 'JH3',
        'thermal_stability_factor': 'JH4',
        
        # Nb-Siä¸“ç”¨åˆ¤æ® (JN)
        'solubility_limit_factor': 'JN1',
        'eutectic_distance': 'JN2',
        'Nb_Si_ratio': 'JN3',
        'silicide_volume_fraction': 'JN4',
        'hypereutectic_indicator': 'JN5',
        
        # åŠ›å­¦åˆ¤æ® (JM)
        'stress_concentration_factor': 'JM1',
        'lattice_distortion_energy': 'JM2',
        'toughness_indicator': 'JM3',
        'crack_resistance_factor': 'JM4',
        'plastic_deformation_capacity': 'JM5',
        
        # æ‰©æ•£åˆ¤æ® (JD)
        'diffusion_resistance': 'JD1',
        'diffusion_activation_energy': 'JD2',
        
        # åˆé‡‘åˆ¤æ® (JA)
        'refractory_strengthening': 'JA1',
        'Ti_Al_synergy': 'JA2',
        'Cr_oxidation_resistance': 'JA3',
        
        # ç»“æ„åˆ¤æ® (JS)
        'interface_coherency': 'JS1',
        'lattice_mismatch_effect': 'JS2',
        'phase_separation_tendency': 'JS3',
        'precipitation_hardening_potential': 'JS4',
        'elastic_anisotropy': 'JS5',
        
        # å…¶ä»–åˆ¤æ® (JX)
        'enthalpy_entropy_competition': 'JX1',
        'silicide_forming_tendency': 'JX2',
        'VEC_squared_deviation': 'JX3',
        
        # å¸¸è§çš„çƒ­åŠ›å­¦å‚æ•°
        'VEC': 'VEC',
        'Î´': 'Delta',
        'Î©': 'Omega',
        'Î”Hmix': 'dHmix',
        'Î”Smix': 'dSmix',
        'Î”G': 'dG',
        'Tm': 'Tm',
        'Î”Tm': 'dTm',
    }
    return mapping

def apply_feature_name_mapping(feature_names, mapping):
    """åº”ç”¨ç‰¹å¾åç§°æ˜ å°„ï¼Œå°†é•¿åç§°è½¬æ¢ä¸ºç®€ç§°"""
    mapped_names = []
    for name in feature_names:
        if name in mapping:
            mapped_names.append(mapping[name])
        else:
            # å¦‚æœæ²¡æœ‰æ˜ å°„ï¼Œä¿ç•™åŸåç§°ä½†è¿›è¡Œç®€åŒ–
            simplified_name = name.replace('mean_', 'm_').replace('var_', 'v_')
            if len(simplified_name) > 15:
                simplified_name = simplified_name[:15] + '...'
            mapped_names.append(simplified_name)
    return mapped_names

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(CONFIG['output_dir'], exist_ok=True)

# åˆ›å»ºè¯¦ç»†é¢„æµ‹ç»“æœç›®å½•
predictions_dir = os.path.join(CONFIG['output_dir'], 'detailed_predictions')
os.makedirs(predictions_dir, exist_ok=True)

# ============== æ–°å¢ï¼šæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å‡½æ•° ==============
def calculate_data_checksum(data, label=""):
    """è®¡ç®—æ•°æ®çš„æ ¡éªŒå’Œç”¨äºä¸€è‡´æ€§æ£€æŸ¥"""
    if isinstance(data, np.ndarray):
        data_str = str(data.sum()) + str(data.shape) + str(data.mean())
    else:
        data_str = str(data)
    checksum = hashlib.md5(data_str.encode()).hexdigest()[:8]
    if CONFIG['debug_mode']:
        print(f"    ğŸ” {label} æ•°æ®æ ¡éªŒå’Œ: {checksum}")
    return checksum

def verify_model_independence(model1, model2, model_name):
    """éªŒè¯ä¸¤ä¸ªæ¨¡å‹å®ä¾‹æ˜¯å¦ç‹¬ç«‹"""
    is_independent = id(model1) != id(model2)
    if CONFIG['debug_mode']:
        print(f"    ğŸ” æ¨¡å‹ç‹¬ç«‹æ€§æ£€æŸ¥ {model_name}: {'âœ… ç‹¬ç«‹' if is_independent else 'âŒ ç›¸åŒå®ä¾‹'}")
    return is_independent

# ============== æ–°å¢ï¼šä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœå‡½æ•° ==============
def save_detailed_predictions(model_key, y_train, pred_train, y_test, pred_test, 
                            train_indices, test_indices, timestamp):
    """ä¿å­˜æ¨¡å‹çš„è¯¦ç»†é¢„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶"""
    if not CONFIG['save_detailed_predictions']:
        return
    
    # åˆ›å»ºè®­ç»ƒé›†ç»“æœDataFrame
    train_results = pd.DataFrame({
        'Dataset': ['Training'] * len(y_train),
        'Dataset_Code': [1] * len(y_train),
        'Sample_Index': train_indices + 1,  # ä»1å¼€å§‹ç¼–å·
        'Experimental_KQ': y_train,
        'Predicted_KQ': pred_train,
        'Error': pred_train - y_train,
        'Absolute_Error': np.abs(pred_train - y_train),
        'Relative_Error_Percent': np.abs(pred_train - y_train) / np.abs(y_train) * 100
    })
    
    # åˆ›å»ºæµ‹è¯•é›†ç»“æœDataFrame
    test_results = pd.DataFrame({
        'Dataset': ['Test'] * len(y_test),
        'Dataset_Code': [2] * len(y_test),
        'Sample_Index': test_indices + 1,  # ä»1å¼€å§‹ç¼–å·
        'Experimental_KQ': y_test,
        'Predicted_KQ': pred_test,
        'Error': pred_test - y_test,
        'Absolute_Error': np.abs(pred_test - y_test),
        'Relative_Error_Percent': np.abs(pred_test - y_test) / np.abs(y_test) * 100
    })
    
    # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç»“æœ
    all_results = pd.concat([train_results, test_results], ignore_index=True)
    
    # ä¿å­˜åˆ°å”¯ä¸€çš„æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³é¿å…è¦†ç›–ï¼‰
    filename = f"{model_key}_predictions_{timestamp}.csv"
    filepath = os.path.join(predictions_dir, filename)
    
    all_results.to_csv(filepath, index=False, encoding='utf-8')
    
    if CONFIG['debug_mode']:
        print(f"    ğŸ’¾ è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜: {filename}")
        # éªŒè¯ä¿å­˜çš„æ•°æ®
        saved_data = pd.read_csv(filepath)
        if len(saved_data) == len(all_results):
            print(f"    âœ… æ–‡ä»¶ä¿å­˜éªŒè¯æˆåŠŸï¼Œå…±{len(saved_data)}è¡Œæ•°æ®")
        else:
            print(f"    âŒ æ–‡ä»¶ä¿å­˜éªŒè¯å¤±è´¥ï¼ŒæœŸæœ›{len(all_results)}è¡Œï¼Œå®é™…{len(saved_data)}è¡Œ")
    
    return filepath

# å®šä¹‰RMSLEè®¡ç®—å‡½æ•°ï¼ˆå¤„ç†è´Ÿå€¼æƒ…å†µï¼‰
def calculate_rmsle(y_true, y_pred):
    """
    è®¡ç®—Root Mean Squared Logarithmic Error
    å¤„ç†è´Ÿå€¼ï¼šå°†è´Ÿå€¼è½¬æ¢ä¸ºå¾ˆå°çš„æ­£å€¼
    """
    try:
        # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ­£æ•°ï¼Œå°†è´Ÿå€¼å’Œé›¶è½¬æ¢ä¸ºä¸€ä¸ªå¾ˆå°çš„æ­£æ•°
        y_true_pos = np.where(y_true <= 0, 1e-6, y_true)
        y_pred_pos = np.where(y_pred <= 0, 1e-6, y_pred)
        
        rmsle = np.sqrt(mean_squared_log_error(y_true_pos, y_pred_pos))
        return rmsle
    except:
        # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›NaN
        return np.nan

# å®šä¹‰æ‰‹åŠ¨äº¤å‰éªŒè¯å‡½æ•°ï¼ˆä»…ç”¨äºç‰¹å¾ç­›é€‰é˜¶æ®µï¼‰
def manual_cross_val(model, X, y, n_splits=5):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=CONFIG['random_state'])
    r2_scores = []
    rmse_scores = []
    for train_idx, test_idx in kf.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2_scores.append(r2)
        rmse_scores.append(rmse)
    return np.mean(r2_scores), np.std(r2_scores), np.mean(rmse_scores), np.std(rmse_scores)

# å®šä¹‰å¿«é€Ÿè¯„ä¼°å‡½æ•°ï¼ˆç”¨äºSFSã€SBSã€GAï¼‰
def quick_evaluate_features(X, y, feature_indices=None):
    """å¿«é€Ÿè¯„ä¼°ç‰¹å¾å­é›†çš„æ€§èƒ½"""
    if feature_indices is not None:
        if len(feature_indices) == 0:
            return -float('inf')
        X_subset = X[:, feature_indices]
    else:
        X_subset = X
    
    try:
        # ä½¿ç”¨Ridgeå›å½’è¿›è¡Œå¿«é€Ÿè¯„ä¼°
        model = Ridge(alpha=1.0, random_state=CONFIG['random_state'])
        r2_mean, _, _, _ = manual_cross_val(model, X_subset, y, n_splits=3)  # ä½¿ç”¨3æŠ˜CVåŠ é€Ÿ
        return r2_mean
    except:
        return -float('inf')

# ============== åŸºäºå®é™…æ•°æ®åº“çš„ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹å®ç° ==============

def physics_guided_feature_engineering_exact(X_reduced, remaining_features, y):
    """
    åŸºäºæ‚¨å®é™…æ•°æ®åº“ç‰¹å¾çš„ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹
    ä½¿ç”¨ç¡®åˆ‡çš„åˆ—åè¿›è¡Œè®¡ç®—
    
    Args:
        X_reduced: é€’å½’æ¶ˆé™¤åçš„ç‰¹å¾çŸ©é˜µ
        remaining_features: å‰©ä½™çš„ç‰¹å¾å
        y: ç›®æ ‡å˜é‡
    
    Returns:
        X_enhanced: å¢å¼ºåçš„ç‰¹å¾çŸ©é˜µ
        enhanced_features: å¢å¼ºåçš„ç‰¹å¾ååˆ—è¡¨
    """
    
    print("=== åŸºäºå®é™…æ•°æ®åº“çš„ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹ ===")
    print(f"  åŸºç¡€ç‰¹å¾æ•°: {len(remaining_features)}")
    print(f"  æ•°æ®åº“ç‰¹å¾éªŒè¯: æ£€æŸ¥å¿…è¦ç‰¹å¾æ˜¯å¦å­˜åœ¨")
    
    # å°†numpyæ•°ç»„è½¬æ¢ä¸ºDataFrame
    feature_df = pd.DataFrame(X_reduced, columns=remaining_features)
    
    # å­˜å‚¨æ–°ç‰¹å¾
    new_features = {}
    
    # ============== ç‰¹å¾å­˜åœ¨æ€§æ£€æŸ¥å‡½æ•° ==============
    def check_feature_exists(feature_name, feature_list):
        """æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨"""
        return feature_name in feature_list
    
    def get_feature_safely(df, feature_name, default_value=0):
        """å®‰å…¨è·å–ç‰¹å¾å€¼"""
        if feature_name in df.columns:
            return df[feature_name]
        else:
            print(f"    âš ï¸ ç‰¹å¾ '{feature_name}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            return pd.Series([default_value] * len(df), index=df.index)
    
    # ============== ç¬¬ä¸€ç±»ï¼šæ ¸å¿ƒéŸ§æ€§åˆ¤æ®ç‰¹å¾ ==============
    print("\n  === æ ¸å¿ƒéŸ§æ€§åˆ¤æ®ç‰¹å¾ ===")
    
    # 1. Pughæ¨¡é‡æ¯” (B/G) - åŸºäºVECè®¡ç®—
    if check_feature_exists('VEC', remaining_features):
        new_features['Pugh_modulus_ratio'] = (feature_df['VEC'] * 9.4 - 4.0) / (feature_df['VEC'] * 4.2 + 25)
        print("  âœ… Pughæ¨¡é‡æ¯” (B/G) - åŸºäºVECè®¡ç®—")
    else:
        print("  âŒ ç¼ºå°‘VECï¼Œæ— æ³•è®¡ç®—Pughæ¨¡é‡æ¯”")
    
    # 2. æ³Šæ¾æ¯”é¢„æµ‹ - åŸºäºVEC
    if check_feature_exists('VEC', remaining_features):
        new_features['Poisson_ratio_predicted'] = 0.33 - 0.015 * feature_df['VEC']
        print("  âœ… é¢„æµ‹æ³Šæ¾æ¯” - åŸºäºVECè®¡ç®—")
    
    # 3. Cauchyå‹åŠ›æŒ‡æ ‡ - åŸºäºVEC
    if check_feature_exists('VEC', remaining_features):
        new_features['Cauchy_pressure_indicator'] = -5.2 * feature_df['VEC'] + 45
        print("  âœ… Cauchyå‹åŠ›æŒ‡æ ‡ - åŸºäºVECè®¡ç®—")
    
    # 4. Siè¿‡é¥±å’Œåº¦
    if check_feature_exists('Si', remaining_features):
        new_features['Si_supersaturation'] = np.maximum(0, feature_df['Si'] - 0.08)
        print("  âœ… Siè¿‡é¥±å’Œåº¦ - åŸºäºSiå«é‡è®¡ç®—")
    else:
        print("  âŒ ç¼ºå°‘Siå«é‡ï¼Œæ— æ³•è®¡ç®—Siè¿‡é¥±å’Œåº¦")
    
    # 5. Nb-SiåŸå­å°ºå¯¸å¤±é…
    if check_feature_exists('Nb', remaining_features) and check_feature_exists('Si', remaining_features):
        # åŸºäºNb(1.43Ã…)å’ŒSi(1.11Ã…)åŸå­åŠå¾„
        r_Nb, r_Si = 1.43, 1.11
        new_features['atomic_size_mismatch_NbSi'] = (feature_df['Nb'] * feature_df['Si'] * 
                                                    ((r_Nb - r_Si) / r_Nb)**2)
        print("  âœ… Nb-SiåŸå­å°ºå¯¸å¤±é… - åŸºäºNbã€Siå«é‡è®¡ç®—")
    else:
        print("  âŒ ç¼ºå°‘Nbæˆ–Siå«é‡ï¼Œæ— æ³•è®¡ç®—åŸå­å°ºå¯¸å¤±é…")
    
    # ============== ç¬¬äºŒç±»ï¼šç”µå­ç»“æ„ç‰¹å¾ ==============
    print("\n  === ç”µå­ç»“æ„ç‰¹å¾ ===")
    
    # 6. VEC-Î©è€¦åˆ
    if (check_feature_exists('VEC', remaining_features) and 
        check_feature_exists('Î©', remaining_features)):
        new_features['VEC_Omega_coupling'] = feature_df['VEC'] * feature_df['Î©']
        print("  âœ… VEC-Î©è€¦åˆ")
    
    # 7. VECç¨³å®šåŒ–æ•ˆåº”
    if (check_feature_exists('VEC', remaining_features) and 
        check_feature_exists('Î”Hmix', remaining_features)):
        new_features['VEC_stabilization'] = feature_df['VEC'] / (1 + np.abs(feature_df['Î”Hmix']))
        print("  âœ… VECç¨³å®šåŒ–æ•ˆåº”")
    
    # 8. VECåç¦»åº¦
    if check_feature_exists('VEC', remaining_features):
        new_features['VEC_deviation_from_ideal'] = np.abs(feature_df['VEC'] - 4.5)
        new_features['VEC_squared_deviation'] = (feature_df['VEC'] - 4.5)**2
        print("  âœ… VECåç¦»åº¦ç‰¹å¾")
    
    # 9. ç”µå­å±€åŸŸåŒ–å‚æ•°
    if (check_feature_exists('mean_E2 electronegativity (Pauling)', remaining_features) and 
        check_feature_exists('var_E2 electronegativity (Pauling)', remaining_features)):
        mean_chi = feature_df['mean_E2 electronegativity (Pauling)']
        var_chi = feature_df['var_E2 electronegativity (Pauling)']
        new_features['electron_localization'] = mean_chi / (var_chi + 1e-6)
        print("  âœ… ç”µå­å±€åŸŸåŒ–å‚æ•°")
    
    # ============== ç¬¬ä¸‰ç±»ï¼šçƒ­åŠ›å­¦ç¨³å®šæ€§ç‰¹å¾ ==============
    print("\n  === çƒ­åŠ›å­¦ç¨³å®šæ€§ç‰¹å¾ ===")
    
    R = 8.314  # æ°”ä½“å¸¸æ•°
    
    # 10. çƒ­åŠ›å­¦é©±åŠ¨åŠ›
    if (check_feature_exists('Î”G', remaining_features) and 
        check_feature_exists('Tm', remaining_features)):
        new_features['thermodynamic_driving_force'] = -feature_df['Î”G'] / (R * feature_df['Tm'])
        print("  âœ… çƒ­åŠ›å­¦é©±åŠ¨åŠ›")
    
    # 11. ç†µç¨³å®šåŒ–æ•ˆåº”
    if check_feature_exists('Î”Smix', remaining_features):
        new_features['entropy_stabilization'] = feature_df['Î”Smix'] / R
        print("  âœ… ç†µç¨³å®šåŒ–æ•ˆåº”")
    
    # 12. ç„“ç†µç«äº‰
    if (check_feature_exists('Î”Hmix', remaining_features) and 
        check_feature_exists('Î”Smix', remaining_features) and 
        check_feature_exists('Tm', remaining_features)):
        new_features['enthalpy_entropy_competition'] = (np.abs(feature_df['Î”Hmix']) / 
                                                       (feature_df['Tm'] * feature_df['Î”Smix'] + 1e-6))
        print("  âœ… ç„“ç†µç«äº‰æ•ˆåº”")
    
    # 13. ç›¸ç¨³å®šæ€§æŒ‡æ•°
    if (check_feature_exists('Î©', remaining_features) and 
        check_feature_exists('Î”Hmix', remaining_features) and 
        check_feature_exists('Tm', remaining_features)):
        new_features['phase_stability_index'] = (feature_df['Î©'] * 
                                                np.exp(-np.abs(feature_df['Î”Hmix'])/(R * feature_df['Tm'])))
        print("  âœ… ç›¸ç¨³å®šæ€§æŒ‡æ•°")
    
    # 14. çƒ­ç¨³å®šæ€§å› å­
    if (check_feature_exists('Tm', remaining_features) and 
        check_feature_exists('Î”Tm', remaining_features)):
        new_features['thermal_stability_factor'] = feature_df['Tm'] / (np.abs(feature_df['Î”Tm']) + 1)
        print("  âœ… çƒ­ç¨³å®šæ€§å› å­")
    
    # ============== ç¬¬å››ç±»ï¼šNb-Siä½“ç³»ä¸“ç”¨ç‰¹å¾ ==============
    print("\n  === Nb-Siä½“ç³»ä¸“ç”¨ç‰¹å¾ ===")
    
    # 15. Nb/Siæ¯”å€¼
    if (check_feature_exists('Nb', remaining_features) and 
        check_feature_exists('Si', remaining_features)):
        new_features['Nb_Si_ratio'] = feature_df['Nb'] / (feature_df['Si'] + 1e-6)
        print("  âœ… Nb/Siæ¯”å€¼")
    
    # 16. å…±æ™¶è·ç¦»ç‰¹å¾
    if check_feature_exists('Si', remaining_features):
        new_features['eutectic_distance'] = np.abs(feature_df['Si'] - 0.16)
        new_features['hypereutectic_indicator'] = np.maximum(0, feature_df['Si'] - 0.16)
        print("  âœ… å…±æ™¶è·ç¦»å’Œè¿‡å…±æ™¶æŒ‡ç¤ºå™¨")
    
    # 17. ç¡…åŒ–ç‰©å½¢æˆå€¾å‘
    if (check_feature_exists('Si', remaining_features) and 
        check_feature_exists('mean_C5 enthalpy atomization', remaining_features)):
        new_features['silicide_forming_tendency'] = (feature_df['Si'] * 
                                                    feature_df['mean_C5 enthalpy atomization'])
        print("  âœ… ç¡…åŒ–ç‰©å½¢æˆå€¾å‘")
    
    # 18. ç¡…åŒ–ç‰©ä½“ç§¯åˆ†æ•°é¢„æµ‹
    if check_feature_exists('Si', remaining_features):
        new_features['silicide_volume_fraction'] = feature_df['Si'] * (1 - np.exp(-feature_df['Si'] / 0.1))
        print("  âœ… ç¡…åŒ–ç‰©ä½“ç§¯åˆ†æ•°é¢„æµ‹")
    
    # 19. å›ºæº¶åº¦é™åˆ¶å› å­
    if (check_feature_exists('Si', remaining_features) and 
        check_feature_exists('Tm', remaining_features)):
        new_features['solubility_limit_factor'] = feature_df['Si'] / (np.exp(-1000 / feature_df['Tm']) + 1e-6)
        print("  âœ… å›ºæº¶åº¦é™åˆ¶å› å­")
    
    # ============== ç¬¬äº”ç±»ï¼šéŸ§æ€§å’ŒåŠ›å­¦æ€§èƒ½ç‰¹å¾ ==============
    print("\n  === éŸ§æ€§å’ŒåŠ›å­¦æ€§èƒ½ç‰¹å¾ ===")
    
    # 20. éŸ§æ€§æŒ‡ç¤ºå™¨
    if (check_feature_exists('mean_C12 modulus Young', remaining_features) and 
        check_feature_exists('mean_å¯†åº¦', remaining_features) and 
        check_feature_exists('var_E2 electronegativity (Pauling)', remaining_features)):
        E = feature_df['mean_C12 modulus Young']
        rho = feature_df['mean_å¯†åº¦']
        var_chi = feature_df['var_E2 electronegativity (Pauling)']
        new_features['toughness_indicator'] = np.sqrt(E * rho) / (var_chi + 1e-6)
        print("  âœ… éŸ§æ€§æŒ‡ç¤ºå™¨")
    
    # 21. è£‚çº¹é˜»åŠ›å› å­
    if (check_feature_exists('mean_C12 modulus Young', remaining_features) and 
        check_feature_exists('mean_å¯†åº¦', remaining_features) and 
        check_feature_exists('var_S13 radii atomic (coordination number 12) (pm)', remaining_features)):
        E = feature_df['mean_C12 modulus Young']
        rho = feature_df['mean_å¯†åº¦']
        var_r = feature_df['var_S13 radii atomic (coordination number 12) (pm)']
        new_features['crack_resistance_factor'] = E / (rho * var_r + 1e-6)
        print("  âœ… è£‚çº¹é˜»åŠ›å› å­")
    
    # 22. å¼¹æ€§å„å‘å¼‚æ€§
    if (check_feature_exists('mean_C12 modulus Young', remaining_features) and 
        'Poisson_ratio_predicted' in new_features):
        E = feature_df['mean_C12 modulus Young']
        nu = new_features['Poisson_ratio_predicted']
        estimated_shear_modulus = E / (2 * (1 + nu))
        new_features['elastic_anisotropy'] = E / (estimated_shear_modulus + 1e-6)
        print("  âœ… å¼¹æ€§å„å‘å¼‚æ€§å‚æ•°")
    
    # 23. å¡‘æ€§å˜å½¢èƒ½åŠ›
    if ('Pugh_modulus_ratio' in new_features and 
        'Poisson_ratio_predicted' in new_features):
        new_features['plastic_deformation_capacity'] = (new_features['Pugh_modulus_ratio'] * 
                                                       new_features['Poisson_ratio_predicted'])
        print("  âœ… å¡‘æ€§å˜å½¢èƒ½åŠ›æŒ‡æ ‡")
    
    # 24. å¾®è§‚åº”åŠ›é›†ä¸­å› å­
    if (check_feature_exists('Î´', remaining_features) and 
        check_feature_exists('var_S13 radii atomic (coordination number 12) (pm)', remaining_features)):
        new_features['stress_concentration_factor'] = (feature_df['Î´'] * 
                                                      feature_df['var_S13 radii atomic (coordination number 12) (pm)'])
        print("  âœ… å¾®è§‚åº”åŠ›é›†ä¸­å› å­")
    
    # ============== ç¬¬å…­ç±»ï¼šæ‰©æ•£å’ŒåŠ¨åŠ›å­¦ç‰¹å¾ ==============
    print("\n  === æ‰©æ•£å’ŒåŠ¨åŠ›å­¦ç‰¹å¾ ===")
    
    # 25. æ‰©æ•£é˜»åŠ›
    if (check_feature_exists('mean_C1 temperature melting', remaining_features) and 
        check_feature_exists('mean_S13 radii atomic (coordination number 12) (pm)', remaining_features)):
        Tm = feature_df['mean_C1 temperature melting']
        r_atom = feature_df['mean_S13 radii atomic (coordination number 12) (pm)']
        new_features['diffusion_resistance'] = Tm / (r_atom**2 + 1e-6)
        print("  âœ… æ‰©æ•£é˜»åŠ›")
    
    # 26. æ‰©æ•£æ¿€æ´»èƒ½ä¼°ç®—
    if (check_feature_exists('mean_C1 temperature melting', remaining_features) and 
        check_feature_exists('Î”Hmix', remaining_features)):
        Tm = feature_df['mean_C1 temperature melting']
        delta_H = feature_df['Î”Hmix']
        new_features['diffusion_activation_energy'] = 0.3 * Tm * R + np.abs(delta_H)
        print("  âœ… æ‰©æ•£æ¿€æ´»èƒ½ä¼°ç®—")
    
    # ============== ç¬¬ä¸ƒç±»ï¼šåˆé‡‘å…ƒç´ ååŒæ•ˆåº” ==============
    print("\n  === åˆé‡‘å…ƒç´ ååŒæ•ˆåº” ===")
    
    # 27. éš¾ç†”å…ƒç´ å¼ºåŒ–æ•ˆåº”
    refractory_elements = ['Ti', 'Hf', 'Zr', 'Mo', 'V', 'Ta']
    available_refractory = [elem for elem in refractory_elements if elem in remaining_features]
    
    if len(available_refractory) >= 2:
        refractory_sum = sum(feature_df[elem] for elem in available_refractory)
        new_features['refractory_elements_sum'] = refractory_sum
        
        if (check_feature_exists('mean_C1 temperature melting', remaining_features) and 
            check_feature_exists('mean_å¯†åº¦', remaining_features)):
            Tm = feature_df['mean_C1 temperature melting']
            rho = feature_df['mean_å¯†åº¦']
            new_features['refractory_strengthening'] = refractory_sum * Tm / rho
            print(f"  âœ… éš¾ç†”å…ƒç´ å¼ºåŒ–æ•ˆåº” (åŒ…å«{len(available_refractory)}ä¸ªå…ƒç´ : {available_refractory})")
    
    # 28. Ti-AlååŒæ•ˆåº”
    if (check_feature_exists('Ti', remaining_features) and 
        check_feature_exists('Al', remaining_features)):
        new_features['Ti_Al_synergy'] = feature_df['Ti'] * feature_df['Al'] * 2.0
        print("  âœ… Ti-AlååŒæ•ˆåº”")
    
    # 29. CræŠ—æ°§åŒ–æ•ˆåº”
    if (check_feature_exists('Cr', remaining_features) and 
        check_feature_exists('mean_E2 electronegativity (Pauling)', remaining_features) and 
        check_feature_exists('mean_S13 radii atomic (coordination number 12) (pm)', remaining_features)):
        Cr = feature_df['Cr']
        chi = feature_df['mean_E2 electronegativity (Pauling)']
        r_atom = feature_df['mean_S13 radii atomic (coordination number 12) (pm)']
        new_features['Cr_oxidation_resistance'] = Cr * chi / r_atom
        print("  âœ… CræŠ—æ°§åŒ–æ•ˆåº”")
    
    # ============== ç¬¬å…«ç±»ï¼šç•Œé¢å’Œå¾®è§‚ç»“æ„ç‰¹å¾ ==============
    print("\n  === ç•Œé¢å’Œå¾®è§‚ç»“æ„ç‰¹å¾ ===")
    
    # 30. ç•Œé¢å…±æ ¼åº¦
    if check_feature_exists('var_S13 radii atomic (coordination number 12) (pm)', remaining_features):
        var_r = feature_df['var_S13 radii atomic (coordination number 12) (pm)']
        new_features['interface_coherency'] = 1 / (1 + var_r)
        print("  âœ… ç•Œé¢å…±æ ¼åº¦")
    
    # 31. æ™¶æ ¼å¤±é…æ•ˆåº”
    if (check_feature_exists('var_S10 Lattice Constants a', remaining_features) and 
        check_feature_exists('var_S11 Lattice Constants b', remaining_features)):
        var_a = feature_df['var_S10 Lattice Constants a']
        var_b = feature_df['var_S11 Lattice Constants b']
        new_features['lattice_mismatch_effect'] = var_a * var_b
        print("  âœ… æ™¶æ ¼å¤±é…æ•ˆåº”")
    
    # 32. æ™¶æ ¼ç•¸å˜èƒ½
    if (check_feature_exists('Î´', remaining_features) and 
        check_feature_exists('mean_C12 modulus Young', remaining_features)):
        delta = feature_df['Î´']
        E = feature_df['mean_C12 modulus Young']
        new_features['lattice_distortion_energy'] = delta**2 * E
        print("  âœ… æ™¶æ ¼ç•¸å˜èƒ½")
    
    # ============== ç¬¬ä¹ç±»ï¼šç›¸åˆ†ç¦»å’Œæå‡ºç‰¹å¾ ==============
    print("\n  === ç›¸åˆ†ç¦»å’Œæå‡ºç‰¹å¾ ===")
    
    # 33. ç›¸åˆ†ç¦»å€¾å‘
    if (check_feature_exists('Î”Hmix', remaining_features) and 
        check_feature_exists('Î”Smix', remaining_features)):
        new_features['phase_separation_tendency'] = feature_df['Î”Hmix'] / (feature_df['Î”Smix'] + 1e-6)
        print("  âœ… ç›¸åˆ†ç¦»å€¾å‘")
    
    # 34. æå‡ºç¡¬åŒ–æ½œåŠ›
    if (check_feature_exists('Si', remaining_features) and 
        check_feature_exists('mean_C12 modulus Young', remaining_features)):
        Si = feature_df['Si']
        E = feature_df['mean_C12 modulus Young']
        new_features['precipitation_hardening_potential'] = Si * E * 0.1
        print("  âœ… æå‡ºç¡¬åŒ–æ½œåŠ›")
    
    # ============== è´¨é‡æ£€æŸ¥å’Œç»„åˆç‰¹å¾ ==============
    print(f"\n  === ç‰¹å¾è´¨é‡æ£€æŸ¥ ===")
    print(f"  ç”Ÿæˆç‰©ç†ç‰¹å¾æ€»æ•°: {len(new_features)}ä¸ª")
    
    if new_features:
        # è´¨é‡æ£€æŸ¥
        valid_features = {}
        correlation_scores = {}
        
        for feature_name, feature_values in new_features.items():
            # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if (not np.any(np.isnan(feature_values)) and 
                not np.any(np.isinf(feature_values)) and 
                np.var(feature_values) >= CONFIG['variance_threshold']):
                
                # è®¡ç®—ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§
                try:
                    correlation = abs(np.corrcoef(feature_values, y)[0, 1])
                    if correlation >= CONFIG['correlation_threshold']:
                        valid_features[feature_name] = feature_values
                        correlation_scores[feature_name] = correlation
                except:
                    # å¦‚æœç›¸å…³æ€§è®¡ç®—å¤±è´¥ï¼Œä»ç„¶ä¿ç•™ç‰¹å¾
                    valid_features[feature_name] = feature_values
                    correlation_scores[feature_name] = CONFIG['correlation_threshold']
        
        print(f"  é€šè¿‡è´¨é‡æ£€æŸ¥çš„ç‰¹å¾: {len(valid_features)}ä¸ª")
        
        # æŒ‰ç›¸å…³æ€§æ’åºæ˜¾ç¤º
        if correlation_scores:
            sorted_features = sorted(correlation_scores.items(), key=lambda x: x[1], reverse=True)
            print(f"  ç›¸å…³æ€§æœ€é«˜çš„å‰10ä¸ªç‰¹å¾:")
            for i, (name, corr) in enumerate(sorted_features[:10]):
                print(f"    {i+1:2d}. {name}: r={corr:.4f}")
        
        if valid_features:
            # ç»„åˆåŸå§‹ç‰¹å¾å’Œæ–°ç‰¹å¾
            valid_features_df = pd.DataFrame(valid_features, index=feature_df.index)
            enhanced_df = pd.concat([feature_df, valid_features_df], axis=1)
            
            # ç‰¹å¾ç±»åˆ«ç»Ÿè®¡
            print(f"\n  === ç‰¹å¾ç±»åˆ«ç»Ÿè®¡ ===")
            categories = {
                'æ ¸å¿ƒéŸ§æ€§åˆ¤æ®': ['Pugh_modulus_ratio', 'Poisson_ratio_predicted', 'Cauchy_pressure_indicator', 
                            'Si_supersaturation', 'atomic_size_mismatch_NbSi'],
                'Nb-Siä¸“ç”¨': ['Nb_Si_ratio', 'eutectic_distance', 'hypereutectic_indicator', 
                            'silicide_forming_tendency', 'silicide_volume_fraction'],
                'ç”µå­ç»“æ„': ['VEC_Omega_coupling', 'VEC_stabilization', 'VEC_deviation_from_ideal', 
                           'electron_localization'],
                'çƒ­åŠ›å­¦': ['thermodynamic_driving_force', 'phase_stability_index', 'entropy_stabilization',
                         'thermal_stability_factor'],
                'åŠ›å­¦æ€§èƒ½': ['toughness_indicator', 'crack_resistance_factor', 'elastic_anisotropy',
                          'plastic_deformation_capacity'],
                'å¾®è§‚ç»“æ„': ['interface_coherency', 'lattice_distortion_energy', 'precipitation_hardening_potential']
            }
            
            for category, feature_list in categories.items():
                count = sum(1 for f in feature_list if f in valid_features)
                if count > 0:
                    print(f"  {category}: {count}ä¸ªç‰¹å¾")
            
            print(f"\n  âœ… æˆåŠŸç”Ÿæˆ {len(valid_features)} ä¸ªç‰©ç†æ„ä¹‰æ˜ç¡®çš„ç‰¹å¾")
            print(f"  âœ… æ‰€æœ‰ç‰¹å¾å‡åŸºäºæ‚¨æ•°æ®åº“ä¸­çš„ç°æœ‰ç‰¹å¾è®¡ç®—")
            
            return enhanced_df.values, enhanced_df.columns.tolist()
        else:
            print("  âš ï¸ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„ç‰©ç†ç‰¹å¾")
            return X_reduced, remaining_features
    else:
        print("  âš ï¸ æ— æ³•ç”Ÿæˆç‰©ç†ç‰¹å¾ï¼Œå¯èƒ½ç¼ºå°‘å…³é”®åŸºç¡€ç‰¹å¾")
        return X_reduced, remaining_features

# ============== 1. è¯»å–æ•°æ® ==============
print("=== å¼€å§‹æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ===")
df = pd.read_excel(CONFIG['data_file'])
print(f"âœ“ æˆåŠŸåŠ è½½æ•°æ®: {df.shape}")

# åŠ¨æ€è·å–ç‰¹å¾åˆ—å - æ’é™¤ç›®æ ‡åˆ—å’ŒæŒ‡å®šçš„æ’é™¤åˆ—
target_col = CONFIG['target_col']
exclude_cols = CONFIG['exclude_cols']

# è·å–æ‰€æœ‰æ•°å€¼å‹ç‰¹å¾åˆ—
all_columns = df.columns.tolist()
features_name = []

for col in all_columns:
    # æ’é™¤ç›®æ ‡åˆ—å’ŒæŒ‡å®šæ’é™¤åˆ—
    if col != target_col and col not in exclude_cols:
        # åªä¿ç•™æ•°å€¼å‹åˆ—
        if pd.api.types.is_numeric_dtype(df[col]):
            features_name.append(col)

print(f"âœ“ åŠ¨æ€è¯†åˆ«åˆ° {len(features_name)} ä¸ªæ•°å€¼ç‰¹å¾")
print(f"âœ“ ç›®æ ‡åˆ—: {target_col}")
print(f"âœ“ å°†åœ¨æœ€ç»ˆæ¨¡å‹ä¿å­˜æ—¶è¿‡æ»¤çš„å…ƒç´ ç‰¹å¾: {CONFIG['forbidden_features_for_final_model']}")

# æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨
if target_col not in df.columns:
    raise ValueError(f"ç›®æ ‡åˆ— '{target_col}' ä¸å­˜åœ¨äºæ•°æ®ä¸­")

X_all = df[features_name]
y_all = df[[target_col]]  # DataFrame

# ============== å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ– ==============
print("\n=== æ•°æ®æ ‡å‡†åŒ–ä¸æ¸…ç† ===")
scaler = StandardScaler()
X_all_scaled = scaler.fit_transform(X_all)
y_all_np = y_all[target_col].values.ravel()

# ============== ç§»é™¤ NaN ==============
# æ£€æŸ¥å¹¶ç§»é™¤åŒ…å«NaNçš„è¡Œ
nan_mask_x = ~np.isnan(X_all_scaled).any(axis=1)
nan_mask_y = ~np.isnan(y_all_np)
valid_mask = nan_mask_x & nan_mask_y

X_all_scaled = X_all_scaled[valid_mask]
y_all_np = y_all_np[valid_mask]

# ä¿å­˜åŸå§‹æ•°æ®ç”¨äºæ ‘æ¨¡å‹
X_all_original = X_all.values[valid_mask]

print(f"âœ“ ç§»é™¤NaNåæ•°æ®å½¢çŠ¶: X{X_all_scaled.shape}, y{y_all_np.shape}")
print(f"âœ“ äº¤å‰éªŒè¯è®¾ç½®: ä»…åœ¨ç‰¹å¾ç­›é€‰é˜¶æ®µä½¿ç”¨{CONFIG['cv_folds']}æŠ˜äº¤å‰éªŒè¯")

# æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
if CONFIG['enable_data_consistency_check']:
    print(f"\n=== æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥ ===")
    calculate_data_checksum(X_all_scaled, "æ ‡å‡†åŒ–ç‰¹å¾çŸ©é˜µ")
    calculate_data_checksum(y_all_np, "ç›®æ ‡å˜é‡")

# ä¿å­˜åˆå§‹ç‰¹å¾åç§°åˆ—è¡¨
initial_features_name = features_name.copy()

# ============== ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹é˜¶æ®µ ==============
if CONFIG['physics_guided_enhancement']:
    print("\n=== ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹é˜¶æ®µ ===")
    X_all_scaled, features_name = physics_guided_feature_engineering_exact(
        X_all_scaled, features_name, y_all_np
    )
    
    # åŒæ—¶æ›´æ–°åŸå§‹æ•°æ®çŸ©é˜µï¼ˆå¯¹äºç‰©ç†ç‰¹å¾ï¼Œä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®ï¼‰
    if X_all_scaled.shape[1] > X_all_original.shape[1]:
        # æœ‰æ–°çš„ç‰©ç†ç‰¹å¾è¢«æ·»åŠ 
        n_original_features = X_all_original.shape[1]
        n_new_features = X_all_scaled.shape[1] - n_original_features
        
        # å¯¹äºæ ‘æ¨¡å‹ï¼Œç‰©ç†ç‰¹å¾ä¹Ÿä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®ï¼ˆå› ä¸ºç‰©ç†ç‰¹å¾æ˜¯ä»æ ‡å‡†åŒ–æ•°æ®ç”Ÿæˆçš„ï¼‰
        physics_features = X_all_scaled[:, n_original_features:]
        X_all_original = np.column_stack([X_all_original, physics_features])
    
    print(f"âœ“ ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹å®Œæˆ")
    
    # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
    if CONFIG['enable_data_consistency_check']:
        calculate_data_checksum(X_all_scaled, "ç‰©ç†ç‰¹å¾å·¥ç¨‹åçš„æ ‡å‡†åŒ–ç‰¹å¾çŸ©é˜µ")
else:
    print("\n=== è·³è¿‡ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹ ===")

# ============== ç¬¬ä¸€é˜¶æ®µï¼šç›¸å…³æ€§ç­›é€‰å†—ä½™ç‰¹å¾ ==============
print("\n=== ç¬¬ä¸€é˜¶æ®µ: ç›¸å…³æ€§ç­›é€‰ ===")
corr_matrix = np.corrcoef(X_all_scaled, rowvar=False)
remove_indices = []

for i in range(corr_matrix.shape[0]):
    for j in range(i + 1, corr_matrix.shape[1]):
        if abs(corr_matrix[i, j]) > 0.95:
            corr_i = abs(pearsonr(X_all_scaled[:, i], y_all_np)[0])
            corr_j = abs(pearsonr(X_all_scaled[:, j], y_all_np)[0])
            if corr_i < corr_j:
                remove_indices.append(i)
            else:
                remove_indices.append(j)

remove_indices = list(set(remove_indices))

# æ‰¾å‡ºè¦ç§»é™¤çš„ç‰¹å¾åç§°
removed_features = [features_name[i] for i in remove_indices]
print(f"ç§»é™¤çš„é«˜ç›¸å…³ç‰¹å¾({len(removed_features)}ä¸ª): {', '.join(removed_features[:10])}{'...' if len(removed_features) > 10 else ''}")

# ç§»é™¤å†—ä½™ç‰¹å¾
X_all_scaled = np.delete(X_all_scaled, remove_indices, axis=1)
X_all_original = np.delete(X_all_original, remove_indices, axis=1)
features_name = [name for i, name in enumerate(features_name) if i not in remove_indices]

print(f"âœ“ ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œå‰©ä½™ {len(features_name)} ä¸ªç‰¹å¾")

# æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
if CONFIG['enable_data_consistency_check']:
    calculate_data_checksum(X_all_scaled, "ç›¸å…³æ€§ç­›é€‰åçš„ç‰¹å¾çŸ©é˜µ")

# ============== ç¬¬äºŒé˜¶æ®µï¼šæ›´ç¨³å¥çš„é€’å½’æ¶ˆé™¤ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰ ==============
print("\n=== ç¬¬äºŒé˜¶æ®µ: é€’å½’æ¶ˆé™¤ (ä½¿ç”¨ç¨³å¥æ–¹æ³•ï¼Œå¹¶è¡Œè®¡ç®—) ===")

def evaluate_feature_subset(i, X, y, features_name):
    """è¯„ä¼°ç§»é™¤ç¬¬iä¸ªç‰¹å¾åçš„æ€§èƒ½"""
    X_subset = np.delete(X, i, axis=1)
    
    # æ£€æŸ¥å­é›†æ˜¯å¦æœ‰æ•ˆ
    if X_subset.shape[1] == 0:
        return i, float('inf')
    
    try:
        error_scores = []
        
        # 1. Ridgeå›å½’ï¼ˆå¤„ç†å¤šé‡å…±çº¿æ€§ï¼‰
        ridge = Ridge(alpha=1.0, random_state=CONFIG['random_state'])
        ridge_scores = cross_val_score(ridge, X_subset, y, 
                                     scoring='neg_mean_squared_error', 
                                     cv=CONFIG['cv_folds'], 
                                     n_jobs=1)  # å­è¿›ç¨‹ä¸­ä¸å†å¹¶è¡Œ
        if not np.any(np.isnan(ridge_scores)):
            error_scores.append(-np.mean(ridge_scores))
        
        # 2. Huberå›å½’ï¼ˆå¯¹å¼‚å¸¸å€¼ç¨³å¥ï¼‰
        huber = HuberRegressor(max_iter=200)
        huber_scores = cross_val_score(huber, X_subset, y, 
                                     scoring='neg_mean_squared_error', 
                                     cv=CONFIG['cv_folds'],
                                     n_jobs=1)
        if not np.any(np.isnan(huber_scores)):
            error_scores.append(-np.mean(huber_scores))
        
        # 3. éšæœºæ£®æ—ï¼ˆéçº¿æ€§å…³ç³»ï¼‰
        rf = RandomForestRegressor(n_estimators=50, random_state=CONFIG['random_state'], n_jobs=1)
        rf_scores = cross_val_score(rf, X_subset, y, 
                                  scoring='neg_mean_squared_error', 
                                  cv=CONFIG['cv_folds'],
                                  n_jobs=1)
        if not np.any(np.isnan(rf_scores)):
            error_scores.append(-np.mean(rf_scores))
        
        # å¦‚æœè‡³å°‘æœ‰ä¸€ä¸ªæ–¹æ³•æˆåŠŸï¼Œä½¿ç”¨å¹³å‡è¯¯å·®
        if error_scores:
            error = np.mean(error_scores)
        else:
            error = float('inf')
        
    except Exception as e:
        error = float('inf')
    
    return i, error

def robust_recursive_elimination(X, X_orig, y, features_name):
    """ä½¿ç”¨æ›´ç¨³å¥çš„æ–¹æ³•è¿›è¡Œé€’å½’æ¶ˆé™¤ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰"""
    n = X.shape[1]
    last_error = float('inf')
    round_num = 1
    consecutive_no_improvement = 0
    
    print(f"  å¼€å§‹é€’å½’æ¶ˆé™¤ï¼Œå½“å‰ç‰¹å¾æ•°: {X.shape[1]}, ç›®æ ‡ç‰¹å¾æ•°: â‰¥{CONFIG['max_k']}")
    print(f"  ä½¿ç”¨ {CONFIG['n_jobs']} ä¸ªè¿›ç¨‹å¹¶è¡Œè®¡ç®—")
    
    while X.shape[1] > CONFIG['max_k']:
        print(f"\n  ç¬¬ {round_num} è½®è¯„ä¼° (å‰©ä½™ç‰¹å¾: {X.shape[1]})")
        
        # å¹¶è¡Œè¯„ä¼°æ‰€æœ‰ç‰¹å¾
        results = Parallel(n_jobs=CONFIG['n_jobs'], verbose=1)(
            delayed(evaluate_feature_subset)(i, X, y, features_name) 
            for i in range(X.shape[1])
        )
        
        # æå–ç»“æœ
        errors = [float('inf')] * X.shape[1]
        for i, error in results:
            errors[i] = error
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„è¯¯å·®å€¼
        valid_errors = [e for e in errors if e != float('inf')]
        if not valid_errors:
            print("  æ— æ³•è®¡ç®—æœ‰æ•ˆè¯¯å·®ï¼Œåœæ­¢é€’å½’æ¶ˆé™¤")
            break
        
        # æ‰¾å‡ºç§»é™¤åè¯¯å·®æœ€å°çš„ç‰¹å¾
        min_error_index = np.argmin(errors)
        min_error = errors[min_error_index]
        
        # å¦‚æœæœ€å°è¯¯å·®æ˜¯infï¼Œè¯´æ˜æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾å¯ä»¥ç§»é™¤
        if min_error == float('inf'):
            print("  æ‰€æœ‰ç‰¹å¾ç§»é™¤åéƒ½äº§ç”Ÿæ— æ•ˆç»“æœï¼Œåœæ­¢é€’å½’æ¶ˆé™¤")
            break
        
        print(f"  ç¬¬ {round_num} è½®: ç§»é™¤ '{features_name[min_error_index]}', è¯¯å·®: {min_error:.4f}")
        
        # æ£€æŸ¥æ˜¯å¦æ”¹è¿›
        if min_error >= last_error:
            consecutive_no_improvement += 1
            if consecutive_no_improvement >= 3:
                print("  è¿ç»­3è½®æ²¡æœ‰æ”¹è¿›ï¼Œåœæ­¢é€’å½’æ¶ˆé™¤")
                break
        else:
            consecutive_no_improvement = 0
        
        last_error = min_error
        X = np.delete(X, min_error_index, axis=1)
        X_orig = np.delete(X_orig, min_error_index, axis=1)
        features_name = [name for j, name in enumerate(features_name) if j != min_error_index]
        round_num += 1
        
        # å®‰å…¨æ£€æŸ¥
        if X.shape[1] <= CONFIG['max_k']:
            print(f"  è¾¾åˆ°æœ€å¤§ç‰¹å¾æ•°é™åˆ¶ (K={CONFIG['max_k']})ï¼Œåœæ­¢é€’å½’æ¶ˆé™¤")
            break
    
    return X, X_orig, features_name

# æ‰§è¡Œç¨³å¥çš„é€’å½’æ¶ˆé™¤
X_all_scaled, X_all_original, features_name = robust_recursive_elimination(
    X_all_scaled, X_all_original, y_all_np, features_name
)
print(f"âœ“ ç¬¬äºŒé˜¶æ®µå®Œæˆï¼Œå‰©ä½™ {len(features_name)} ä¸ªç‰¹å¾")

# æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
if CONFIG['enable_data_consistency_check']:
    calculate_data_checksum(X_all_scaled, "é€’å½’æ¶ˆé™¤åçš„ç‰¹å¾çŸ©é˜µ")

# ============== 2. å®šä¹‰ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼ˆä¼ ç»Ÿ+é«˜çº§æ–¹æ³•ï¼‰ ==============

# ä¼ ç»Ÿæ–¹æ³•
def feature_selection_rfe(X, y, k):
    selector = RFE(estimator=Ridge(alpha=1.0), n_features_to_select=k)
    X_sel = selector.fit_transform(X, y)
    sel_indices = np.where(selector.support_)[0]
    return X[:, sel_indices], 'RFE', sel_indices

def feature_selection_mutual_info(X, y, k):
    selector = SelectKBest(mutual_info_regression, k=k)
    X_sel = selector.fit_transform(X, y)
    sel_indices = np.where(selector.get_support())[0]
    return X[:, sel_indices], 'Mutual Information', sel_indices

def feature_selection_f_regression(X, y, k):
    selector = SelectKBest(f_regression, k=k)
    X_sel = selector.fit_transform(X, y)
    sel_indices = np.where(selector.get_support())[0]
    return X[:, sel_indices], 'F-Regression', sel_indices

def feature_selection_pcc(X, y, k):
    pcc = [pearsonr(X[:, i], y)[0] for i in range(X.shape[1])]
    pcc = np.abs(pcc)  # Using absolute values of the correlation coefficients
    indices = np.argsort(pcc)[-k:]
    return X[:, indices], 'PCC', indices

def feature_selection_lasso(X, y, k):
    lasso = Lasso(alpha=0.01)
    lasso.fit(X, y)
    importance = np.abs(lasso.coef_)
    indices = np.argsort(importance)[-k:]
    return X[:, indices], 'Lasso', indices

def feature_selection_rf(X, y, k):
    rf = RandomForestRegressor(n_estimators=100, n_jobs=CONFIG['n_jobs'])
    rf.fit(X, y)
    importance = rf.feature_importances_
    indices = np.argsort(importance)[-k:]
    return X[:, indices], 'RandomForest', indices

def feature_selection_anova(X, y, k):
    f_values, p_values = f_classif(X, y)
    indices = np.argsort(f_values)[-k:]
    return X[:, indices], 'ANOVA_F', indices

def feature_selection_selectkbest(X, y, k):
    selector = SelectKBest(f_classif, k=k)
    X_sel = selector.fit_transform(X, y)
    sel_indices = np.where(selector.get_support())[0]
    return X[:, sel_indices], 'SelectKBest', sel_indices

# ============== æ–°å¢é«˜çº§æ–¹æ³• ==============

def feature_selection_sfs(X, y, k):
    """
    Sequential Forward Selection (SFS) é¡ºåºå‰å‘é€‰æ‹©
    ä»ç©ºç‰¹å¾é›†å¼€å§‹ï¼Œæ¯æ¬¡æ·»åŠ ä¸€ä¸ªèƒ½æœ€å¤§åŒ–RÂ²çš„ç‰¹å¾
    """
    n_features = X.shape[1]
    selected_indices = []
    available_indices = list(range(n_features))
    
    print(f"    å¼€å§‹SFSï¼Œç›®æ ‡ç‰¹å¾æ•°: {k}, å¯ç”¨ç‰¹å¾æ•°: {len(available_indices)}")
    
    for step in range(k):
        best_score = -float('inf')
        best_idx = None
        
        for idx in available_indices:
            # ä¸´æ—¶ç‰¹å¾é›†
            temp_indices = selected_indices + [idx]
            
            # è¯„ä¼°æ€§èƒ½
            score = quick_evaluate_features(X, y, temp_indices)
            if score > best_score:
                best_score = score
                best_idx = idx
        
        if best_idx is not None:
            selected_indices.append(best_idx)
            available_indices.remove(best_idx)
            print(f"    SFSæ­¥éª¤{step+1}: é€‰æ‹©ç‰¹å¾ '{features_name[best_idx]}', RÂ²={best_score:.4f}")
        else:
            print(f"    SFSæ­¥éª¤{step+1}: æ— å¯ç”¨ç‰¹å¾")
            break
    
    if len(selected_indices) == k:
        return X[:, selected_indices], 'SFS', selected_indices
    else:
        print(f"    SFSæœªèƒ½é€‰æ‹©è¶³å¤Ÿç‰¹å¾: {len(selected_indices)}/{k}")
        return None, 'SFS', None

def feature_selection_sbs(X, y, k):
    """
    Sequential Backward Selection (SBS) é¡ºåºåå‘é€‰æ‹©
    ä»å…¨ç‰¹å¾é›†å¼€å§‹ï¼Œæ¯æ¬¡ç§»é™¤ä¸€ä¸ªå¯¹æ€§èƒ½å½±å“æœ€å°çš„ç‰¹å¾
    """
    n_features = X.shape[1]
    selected_indices = list(range(n_features))
    
    print(f"    å¼€å§‹SBSï¼Œç›®æ ‡ç‰¹å¾æ•°: {k}, åˆå§‹ç‰¹å¾æ•°: {len(selected_indices)}")
    
    while len(selected_indices) > k:
        best_score = -float('inf')
        worst_idx = None
        
        for idx in selected_indices:
            # ä¸´æ—¶ç§»é™¤è¯¥ç‰¹å¾
            temp_indices = [i for i in selected_indices if i != idx]
            
            # è¯„ä¼°æ€§èƒ½
            score = quick_evaluate_features(X, y, temp_indices)
            if score > best_score:
                best_score = score
                worst_idx = idx
        
        if worst_idx is not None:
            selected_indices.remove(worst_idx)
            step = len(selected_indices)
            print(f"    SBSæ­¥éª¤{n_features-step}: ç§»é™¤ç‰¹å¾ '{features_name[worst_idx]}', RÂ²={best_score:.4f}")
        else:
            print("    SBS: æ— æ³•ç»§ç»­ç§»é™¤ç‰¹å¾")
            break
    
    if len(selected_indices) == k:
        return X[:, selected_indices], 'SBS', selected_indices
    else:
        print(f"    SBSæœªèƒ½è¾¾åˆ°ç›®æ ‡ç‰¹å¾æ•°: {len(selected_indices)}/{k}")
        return None, 'SBS', None

def feature_selection_ga(X, y, k):
    """
    Genetic Algorithm (GA) é—ä¼ ç®—æ³•ç‰¹å¾é€‰æ‹©
    ä½¿ç”¨é—ä¼ ç®—æ³•æœç´¢æœ€ä¼˜çš„kä¸ªç‰¹å¾ç»„åˆ
    """
    n_features = X.shape[1]
    
    print(f"    å¼€å§‹GAï¼Œç›®æ ‡ç‰¹å¾æ•°: {k}, å¯ç”¨ç‰¹å¾æ•°: {n_features}")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(CONFIG['random_state'])
    np.random.seed(CONFIG['random_state'])
    
    # åˆå§‹åŒ–ç§ç¾¤
    population = []
    for _ in range(CONFIG['ga_population']):
        # éšæœºé€‰æ‹©kä¸ªç‰¹å¾
        individual = random.sample(range(n_features), k)
        population.append(sorted(individual))
    
    def evaluate_individual(individual):
        """è¯„ä¼°ä¸ªä½“é€‚åº”åº¦"""
        return quick_evaluate_features(X, y, individual)
    
    def crossover(parent1, parent2):
        """äº¤å‰æ“ä½œ"""
        # é€‰æ‹©ä¸¤ä¸ªçˆ¶ä»£çš„ç‰¹å¾ç»„åˆï¼Œç„¶åéšæœºé€‰æ‹©kä¸ª
        combined = list(set(parent1 + parent2))
        if len(combined) >= k:
            child = random.sample(combined, k)
        else:
            # å¦‚æœç»„åˆç‰¹å¾ä¸è¶³kä¸ªï¼Œè¡¥å……éšæœºç‰¹å¾
            child = combined.copy()
            remaining = [i for i in range(n_features) if i not in child]
            if len(remaining) > 0:
                child.extend(random.sample(remaining, min(k - len(child), len(remaining))))
        return sorted(child[:k])
    
    def mutate(individual):
        """å˜å¼‚æ“ä½œ"""
        if random.random() < CONFIG['ga_mutation_rate']:
            # éšæœºæ›¿æ¢ä¸€ä¸ªç‰¹å¾
            idx_to_replace = random.randint(0, len(individual) - 1)
            remaining = [i for i in range(n_features) if i not in individual]
            if remaining:
                individual[idx_to_replace] = random.choice(remaining)
        return sorted(individual)
    
    # è¿›åŒ–è¿‡ç¨‹
    best_individual = None
    best_fitness = -float('inf')
    
    for generation in range(CONFIG['ga_generations']):
        # è¯„ä¼°é€‚åº”åº¦
        fitness_scores = []
        for individual in population:
            fitness = evaluate_individual(individual)
            fitness_scores.append(fitness)
            
            if fitness > best_fitness:
                best_fitness = fitness
                best_individual = individual.copy()
        
        if generation % 5 == 0:
            print(f"    GAç¬¬{generation}ä»£: æœ€ä½³é€‚åº”åº¦={best_fitness:.4f}")
        
        # é€‰æ‹©æ“ä½œï¼ˆé”¦æ ‡èµ›é€‰æ‹©ï¼‰
        new_population = []
        
        # ä¿ç•™æœ€ä¼˜ä¸ªä½“
        new_population.append(best_individual.copy())
        
        while len(new_population) < CONFIG['ga_population']:
            # é”¦æ ‡èµ›é€‰æ‹©
            tournament_size = 3
            tournament_indices = random.sample(range(len(population)), tournament_size)
            tournament_fitness = [fitness_scores[i] for i in tournament_indices]
            winner_idx = tournament_indices[np.argmax(tournament_fitness)]
            parent1 = population[winner_idx]
            
            tournament_indices = random.sample(range(len(population)), tournament_size)
            tournament_fitness = [fitness_scores[i] for i in tournament_indices]
            winner_idx = tournament_indices[np.argmax(tournament_fitness)]
            parent2 = population[winner_idx]
            
            # äº¤å‰å’Œå˜å¼‚
            if random.random() < CONFIG['ga_crossover_rate']:
                child = crossover(parent1, parent2)
            else:
                child = parent1.copy()
            
            child = mutate(child)
            new_population.append(child)
        
        population = new_population
    
    print(f"    GAå®Œæˆ: æœ€ä½³é€‚åº”åº¦={best_fitness:.4f}")
    
    if best_individual is not None and len(best_individual) == k:
        selected_features = [features_name[i] for i in best_individual]
        print(f"    GAé€‰æ‹©çš„ç‰¹å¾: {', '.join(selected_features)}")
        return X[:, best_individual], 'GA', best_individual
    else:
        print(f"    GAæœªèƒ½æ‰¾åˆ°æœ‰æ•ˆè§£")
        return None, 'GA', None

# ============== 3. ç‰¹å¾ç­›é€‰ - å¤„ç†K=4,5,6,7ï¼ˆä¼ ç»Ÿ+é«˜çº§æ–¹æ³•ï¼‰ ==============
print(f"\n=== å¼€å§‹ç‰¹å¾ç­›é€‰ (å¤„ç†K=4,5,6,7, ä½¿ç”¨ä¼ ç»Ÿ+é«˜çº§ç‰¹å¾é€‰æ‹©æ–¹æ³•+ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹) ===")
print(f"ä¼ ç»Ÿæ–¹æ³•: 8ç§ (RFE, äº’ä¿¡æ¯, Få›å½’, PCC, Lasso, éšæœºæ£®æ—, ANOVA, SelectKBest)")
print(f"é«˜çº§æ–¹æ³•: 3ç§ (SFS, SBS, GA)")
print(f"ç‰©ç†å¼•å¯¼: åŸºäºå®é™…æ•°æ®åº“ç‰¹å¾çš„ç‰©ç†æ„ä¹‰ç‰¹å¾å·¥ç¨‹")
print(f"ç­›é€‰ç­–ç•¥: å„æ–¹æ³•ç›´æ¥é€‰æ‹©æœ€ä¼˜ç‰¹å¾ï¼Œæ— éœ€é¢å¤–è¯„ä¼°")
print(f"æ³¨æ„: å…ƒç´ ç‰¹å¾å°†å‚ä¸ç‰¹å¾é€‰æ‹©ï¼Œä½†åŒ…å«æŒ‡å®šå…ƒç´ ç‰¹å¾çš„æ¨¡å‹åœ¨æœ€ç»ˆä¿å­˜æ—¶å°†è¢«å‰”é™¤")

good_feature_sets = []

# å¤„ç†K=4,5,6,7
for k in CONFIG['target_k_range']:
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç‰¹å¾
    if k > len(features_name):
        print(f"\n--- ç‰¹å¾æ•° K = {k} è¶…è¿‡å¯ç”¨ç‰¹å¾æ•° {len(features_name)}ï¼Œè·³è¿‡ ---")
        continue
        
    print(f"\n--- ç‰¹å¾æ•° K = {k} ---")
    
    # ä½¿ç”¨ä¼ ç»Ÿ+é«˜çº§ç‰¹å¾é€‰æ‹©æ–¹æ³•
    feature_selectors = [
        # ä¼ ç»Ÿæ–¹æ³•
        feature_selection_rfe, 
        feature_selection_mutual_info, 
        feature_selection_f_regression,
        feature_selection_pcc, 
        feature_selection_lasso, 
        feature_selection_rf,
        feature_selection_anova, 
        feature_selection_selectkbest,
        # é«˜çº§æ–¹æ³•
        feature_selection_sfs,
        feature_selection_sbs,
        feature_selection_ga
    ]

    for selector in feature_selectors:
        try:
            result = selector(X_all_scaled, y_all_np, k)
            if result[0] is None:
                print(f"{selector.__name__.replace('feature_selection_', '').upper():>25} | è·³è¿‡ï¼ˆç‰¹å¾é€‰æ‹©å¤±è´¥ï¼‰")
                continue
            X_sel, method_label, sel_idx = result
            X_sel_orig = X_all_original[:, sel_idx]
            
            # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
            if CONFIG['enable_data_consistency_check']:
                calculate_data_checksum(X_sel, f"{method_label}_k={k} é€‰æ‹©çš„ç‰¹å¾çŸ©é˜µ")
            
            # ç›´æ¥åŠ å…¥ç‰¹å¾é›†ï¼Œä¸è¿›è¡Œé¢å¤–è¯„ä¼°
            method_display = method_label
            if method_label in ['SFS', 'SBS', 'GA']:
                method_display = f"â˜… {method_label}"  # æ ‡è®°é«˜çº§æ–¹æ³•
            
            print(f"{method_display:>25} | æˆåŠŸé€‰æ‹© {k} ä¸ªç‰¹å¾")
            good_feature_sets.append((X_sel, X_sel_orig, 0.0, method_label, k, sel_idx))
            print(f"    âœ“ ç‰¹å¾é›†å·²åŠ å…¥")
                
        except Exception as e:
            method_name = selector.__name__.replace('feature_selection_', '').upper()
            print(f"{method_name:>25} | é”™è¯¯: {str(e)[:50]}")

print(f"\n=== ç‰¹å¾ç­›é€‰å®Œæˆ ===")
print(f"æ‰¾åˆ° {len(good_feature_sets)} ä¸ªç‰¹å¾é›† (K=4,5,6,7, ä¼ ç»Ÿ+é«˜çº§æ–¹æ³•+ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹)")
print(f"æ³¨æ„: ç‰¹å¾è´¨é‡å°†é€šè¿‡åç»­çš„14ç§æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ¥è¯„ä¼°")
print(f"æ³¨æ„: æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨é»˜è®¤å‚æ•°")

# ============== 4. è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œæ— è´å¶æ–¯ä¼˜åŒ–ï¼‰ ==============
print(f"\n=== å¼€å§‹æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° (ä½¿ç”¨é»˜è®¤å‚æ•°+ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹+æ ‡å‡†éªŒè¯å’Œå¤šç§è¯„ä¼°æŒ‡æ ‡: RÂ², RMSE, MAE, MSE, RMSLE, æ— è´å¶æ–¯ä¼˜åŒ–) ===")
print(f"ğŸ“Š æ ‡å‡†éªŒè¯: 80%-20%åˆ†å‰²éªŒè¯ï¼Œè¯„ä¼°å¤šç§æŒ‡æ ‡")
print(f"âš¡ æ‰€æœ‰æ¨¡å‹ä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œæ— è¶…å‚æ•°ä¼˜åŒ–")
print(f"ğŸ” è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ï¼Œå°†è¯¦ç»†ç›‘æ§æ•°æ®ä¼ é€’å’Œæ¨¡å‹ç‹¬ç«‹æ€§")

# å®šä¹‰æ¨¡å‹ç±»å­—å…¸ï¼ˆå»é™¤PCAã€LightGBMå’ŒSVRï¼‰
models_classes = {
    "LinearRegression": LinearRegression,
    "Ridge": Ridge,
    "Lasso": Lasso,
    "ElasticNet": ElasticNet,
    "BayesianRidge": BayesianRidge,
    "HuberRegressor": HuberRegressor,
    "RandomForest": RandomForestRegressor,
    "GradientBoosting": GradientBoostingRegressor,
    "KNeighbors": KNeighborsRegressor,
    "DecisionTree": DecisionTreeRegressor,
    "XGBoost": XGBRegressor,
    "CatBoost": CatBoostRegressor,
    "HistGradientBoosting": HistGradientBoostingRegressor,
    "AdaBoost": AdaBoostRegressor,
    "Bagging": BaggingRegressor,
    "Voting": VotingRegressor
}

# é»˜è®¤æ¨¡å‹å‚æ•°
def get_default_model(model_name, model_class):
    """è·å–å¸¦æœ‰åˆé€‚é»˜è®¤å‚æ•°çš„æ¨¡å‹å®ä¾‹"""
    default_params = {}
    
    if 'random_state' in model_class().get_params():
        default_params['random_state'] = CONFIG['random_state']
    
    if 'n_jobs' in model_class().get_params() and model_name not in ['CatBoost']:
        default_params['n_jobs'] = CONFIG['n_jobs']
    elif model_name == 'CatBoost':
        default_params['verbose'] = 0
        default_params['random_state'] = CONFIG['random_state']
        if CONFIG['n_jobs'] > 0:
            default_params['thread_count'] = CONFIG['n_jobs']
        else:
            default_params['thread_count'] = -1
    
    # ç‰¹æ®Šæ¨¡å‹çš„é¢å¤–å‚æ•°
    if model_name == 'Voting':
        # ä¸ºVotingRegressorå®šä¹‰å­ä¼°è®¡å™¨
        default_params = {
            'estimators': [
                ('rf', RandomForestRegressor(n_estimators=100, random_state=CONFIG['random_state'], n_jobs=CONFIG['n_jobs'])),
                ('knn', KNeighborsRegressor(n_jobs=CONFIG['n_jobs']))
            ],
            'n_jobs': CONFIG['n_jobs']
        }
    
    return model_class(**default_params)

# åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ ‘æ¨¡å‹çš„åŸå§‹æ•°æ®
tree_based_models = ['RandomForest', 'GradientBoosting', 'XGBoost', 
                    'CatBoost', 'DecisionTree', 'HistGradientBoosting', 'AdaBoost', 'Bagging']

results_dict = {}
all_qualified_models = []  # å­˜å‚¨æ‰€æœ‰æ»¡è¶³RÂ²æ¡ä»¶çš„æ¨¡å‹

# ç”Ÿæˆæ—¶é—´æˆ³ç”¨äºæ–‡ä»¶å‘½å
timestamp = int(time.time())

for idx, (X_sel, X_sel_orig, r2_val, method_label, k, sel_idx) in enumerate(good_feature_sets):
    print(f"\n[{idx+1}/{len(good_feature_sets)}] è¯„ä¼°ç‰¹å¾é›†: {method_label}_k={k}")
    
    # æ‰“å°é€‰æ‹©çš„ç‰¹å¾åç§°
    if sel_idx is not None:
        selected_features = [features_name[i] for i in sel_idx]
        print(f"  é€‰æ‹©çš„ç‰¹å¾: {', '.join(selected_features)}")
    else:
        print(f"  é€‰æ‹©çš„ç‰¹å¾: ç‰¹å¾è½¬æ¢æ–¹æ³• (æ— åŸå§‹ç‰¹å¾å)")
    
    # ç‰¹åˆ«æ ‡æ³¨é«˜çº§æ–¹æ³•å’Œé‡ç‚¹æ–¹æ³•çš„ç»“æœ
    if method_label in ['SFS', 'SBS', 'GA']:
        print(f"  â˜… {method_label}é«˜çº§æ–¹æ³•é€‰æ‹©çš„ç‰¹å¾é›†")
    if method_label in FOCUS_METHODS:
        print(f"  ğŸ¯ {method_label}é‡ç‚¹æ–¹æ³•")
    
    # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
    if CONFIG['enable_data_consistency_check']:
        calculate_data_checksum(X_sel, f"{method_label}_k={k} æ ‡å‡†åŒ–æ•°æ®")
        calculate_data_checksum(X_sel_orig, f"{method_label}_k={k} åŸå§‹æ•°æ®")
    
    feature_set_key = f"{method_label}_k={k}"
    
    for m_name, model_class in models_classes.items():
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ•°æ®
            if m_name in tree_based_models:
                # æ ‘æ¨¡å‹ä½¿ç”¨åŸå§‹æ•°æ®
                X_train_sel, X_test_sel, y_train, y_test = train_test_split(
                    X_sel_orig, y_all_np, test_size=0.2, random_state=CONFIG['random_state']
                )
                X_for_eval = X_sel_orig
                data_type = "åŸå§‹æ•°æ®"
            else:
                # å…¶ä»–æ¨¡å‹ä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®
                X_train_sel, X_test_sel, y_train, y_test = train_test_split(
                    X_sel, y_all_np, test_size=0.2, random_state=CONFIG['random_state']
                )
                X_for_eval = X_sel
                data_type = "æ ‡å‡†åŒ–æ•°æ®"
            
            # è·å–è®­ç»ƒå’Œæµ‹è¯•ç´¢å¼•
            _, _, _, _, train_indices, test_indices = train_test_split(
                X_for_eval, y_all_np, np.arange(len(y_all_np)), 
                test_size=0.2, random_state=CONFIG['random_state']
            )
            
            # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
            if CONFIG['enable_data_consistency_check']:
                calculate_data_checksum(X_train_sel, f"{method_label}_{m_name} è®­ç»ƒæ•°æ®({data_type})")
                calculate_data_checksum(X_test_sel, f"{method_label}_{m_name} æµ‹è¯•æ•°æ®({data_type})")
            
            # ä½¿ç”¨é»˜è®¤å‚æ•°åˆ›å»ºæ¨¡å‹ - ç¡®ä¿æ¯æ¬¡éƒ½æ˜¯æ–°å®ä¾‹
            model = get_default_model(m_name, model_class)
            
            # æ¨¡å‹ç‹¬ç«‹æ€§æ£€æŸ¥
            if CONFIG['debug_mode'] and idx > 0:
                previous_model = get_default_model(m_name, model_class)
                verify_model_independence(model, previous_model, m_name)
            
            # ä¸ºé‡ç‚¹æ¨¡å‹å’Œé‡ç‚¹æ–¹æ³•ç»„åˆæä¾›æ ‡è®°
            is_focus_combination = (m_name in FOCUS_MODELS and method_label in FOCUS_METHODS)
            if is_focus_combination:
                print(f"    ğŸ¯ é‡ç‚¹ç»„åˆ: {method_label} + {m_name}")
            
            if CONFIG['debug_mode']:
                print(f"    ğŸ” ä½¿ç”¨{data_type}è®­ç»ƒæ¨¡å‹: {m_name}")
                print(f"    ğŸ” æ¨¡å‹å®ä¾‹ID: {id(model)}")
            else:
                print(f"    ä½¿ç”¨é»˜è®¤å‚æ•°: {m_name}")
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_train_sel, y_train)
            pred_train = model.predict(X_train_sel)
            pred_test = model.predict(X_test_sel)

            # ç«‹å³è®¡ç®—å’ŒéªŒè¯æŒ‡æ ‡
            r2_train = r2_score(y_train, pred_train)
            rmse_train = np.sqrt(mean_squared_error(y_train, pred_train))
            mae_train = mean_absolute_error(y_train, pred_train)
            mse_train = mean_squared_error(y_train, pred_train)
            rmsle_train = calculate_rmsle(y_train, pred_train)
            
            r2_test = r2_score(y_test, pred_test)
            rmse_test = np.sqrt(mean_squared_error(y_test, pred_test))
            mae_test = mean_absolute_error(y_test, pred_test)
            mse_test = mean_squared_error(y_test, pred_test)
            rmsle_test = calculate_rmsle(y_test, pred_test)

            # è°ƒè¯•æ¨¡å¼ä¸‹æ‰“å°è¯¦ç»†ä¿¡æ¯
            if CONFIG['debug_mode']:
                print(f"    ğŸ” ç«‹å³è®¡ç®—çš„æŒ‡æ ‡: RÂ²={r2_train:.4f}/{r2_test:.4f}, MAE={mae_train:.4f}/{mae_test:.4f}")
                print(f"    ğŸ” é¢„æµ‹å€¼èŒƒå›´: è®­ç»ƒé›†[{pred_train.min():.2f}, {pred_train.max():.2f}], æµ‹è¯•é›†[{pred_test.min():.2f}, {pred_test.max():.2f}]")

            # ä¿å­˜ç»“æœåˆ°å­—å…¸
            model_key = f"{method_label}_k={k}_{m_name}"
            results_dict[model_key] = {
                "Train RÂ²": r2_train,
                "Test RÂ²": r2_test,
                "Train RMSE": rmse_train,
                "Test RMSE": rmse_test,
                "Train MAE": mae_train,
                "Test MAE": mae_test,
                "Train MSE": mse_train,
                "Test MSE": mse_test,
                "Train RMSLE": rmsle_train,
                "Test RMSLE": rmsle_test,
                "Selected Indices": sel_idx,
                "K": k,
                "Optimized": False,  # æ— ä¼˜åŒ–
                "Focus Combination": is_focus_combination,
                "Data Type": data_type,  # è®°å½•ä½¿ç”¨çš„æ•°æ®ç±»å‹
                "Model Instance ID": id(model)  # è®°å½•æ¨¡å‹å®ä¾‹ID
            }
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç›®æ ‡æ¡ä»¶ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼š0.905 â‰¤ RÂ² < 0.99ï¼‰
            if (r2_train >= CONFIG['min_r2_threshold'] and r2_train < CONFIG['max_r2_threshold'] and 
                r2_test >= CONFIG['min_r2_threshold'] and r2_test < CONFIG['max_r2_threshold']):
                
                # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶
                if CONFIG['save_detailed_predictions']:
                    saved_file = save_detailed_predictions(
                        model_key, y_train, pred_train, y_test, pred_test,
                        train_indices, test_indices, timestamp
                    )
                
                # å…‹éš†æ¨¡å‹ä»¥é¿å…çŠ¶æ€æ··ä¹±
                from sklearn.base import clone
                model_copy = clone(model)
                model_copy.fit(X_for_eval, y_all_np)  # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ
                
                all_qualified_models.append({
                    'model_key': model_key,
                    'method': method_label,
                    'model': m_name,
                    'k': k,
                    'train_r2': r2_train,
                    'test_r2': r2_test,
                    'train_mae': mae_train,
                    'test_mae': mae_test,
                    'train_mse': mse_train,
                    'test_mse': mse_test,
                    'train_rmsle': rmsle_train,
                    'test_rmsle': rmsle_test,
                    'features': [features_name[i] for i in sel_idx] if sel_idx is not None else 'Transformed',
                    'model_instance': model_copy,  # ä¿å­˜é‡æ–°è®­ç»ƒçš„æ¨¡å‹
                    'X_data': X_for_eval,            # ä¿å­˜å¯¹åº”çš„æ•°æ®
                    'X_sel': X_sel,                # ä¿å­˜æ ‡å‡†åŒ–æ•°æ®
                    'X_sel_orig': X_sel_orig,      # ä¿å­˜åŸå§‹æ•°æ®
                    'y_data': y_all_np,            # ä¿å­˜ç›®æ ‡æ•°æ®
                    'sel_idx': sel_idx,
                    'is_tree_based': m_name in tree_based_models,  # æ ‡è®°æ˜¯å¦æ˜¯æ ‘æ¨¡å‹
                    'is_advanced': method_label in ['SFS', 'SBS', 'GA'],  # æ ‡è®°æ˜¯å¦æ˜¯é«˜çº§æ–¹æ³•
                    'optimized': False,    # æ ‡è®°æœªè¿›è¡Œä¼˜åŒ–
                    'is_focus_model': m_name in FOCUS_MODELS,  # æ ‡è®°æ˜¯å¦æ˜¯é‡ç‚¹æ¨¡å‹
                    'is_focus_method': method_label in FOCUS_METHODS,  # æ ‡è®°æ˜¯å¦æ˜¯é‡ç‚¹æ–¹æ³•
                    'is_focus_combination': is_focus_combination,  # æ ‡è®°æ˜¯å¦æ˜¯é‡ç‚¹ç»„åˆ
                    'data_type': data_type,  # è®°å½•ä½¿ç”¨çš„æ•°æ®ç±»å‹
                    'original_model_id': id(model),  # è®°å½•åŸå§‹æ¨¡å‹ID
                    'predictions_file': saved_file if CONFIG['save_detailed_predictions'] else None,  # è®°å½•é¢„æµ‹æ–‡ä»¶è·¯å¾„
                })
                
                advanced_mark = "â˜…" if method_label in ['SFS', 'SBS', 'GA'] else ""
                focus_mark = "ğŸ¯" if is_focus_combination else ""
                
                if method_label in ['SFS', 'SBS', 'GA']:
                    print(f"    {advanced_mark}â˜…â˜… {method_label}+{m_name}{focus_mark}: "
                          f"RÂ²={r2_train:.3f}/{r2_test:.3f}, MAE={mae_train:.3f}/{mae_test:.3f}, "
                          f"RMSLE={rmsle_train:.3f}/{rmsle_test:.3f} [æ»¡è¶³ç›®æ ‡!]")
                else:
                    print(f"    â˜…â˜…â˜… {m_name}{focus_mark}: "
                          f"RÂ²={r2_train:.3f}/{r2_test:.3f}, MAE={mae_train:.3f}/{mae_test:.3f}, "
                          f"RMSLE={rmsle_train:.3f}/{rmsle_test:.3f} [æ»¡è¶³ç›®æ ‡!]")
                    
            else:
                focus_mark = "ğŸ¯" if is_focus_combination else ""
                if method_label in ['SFS', 'SBS', 'GA']:
                    print(f"    Â· {method_label}+{m_name}{focus_mark}: RÂ²={r2_train:.3f}/{r2_test:.3f}, MAE={mae_train:.3f}/{mae_test:.3f}, RMSLE={rmsle_train:.3f}/{rmsle_test:.3f}")
                else:
                    print(f"    Â· {m_name}{focus_mark}: RÂ²={r2_train:.3f}/{r2_test:.3f}, MAE={mae_train:.3f}/{mae_test:.3f}, RMSLE={rmsle_train:.3f}/{rmsle_test:.3f}")
            
        except Exception as e:
            print(f"    {m_name} è®­ç»ƒå¤±è´¥: {str(e)[:50]}")
            if CONFIG['debug_mode']:
                import traceback
                print(f"    ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

# ============== 5. æœ€ç»ˆæ¨¡å‹è¿‡æ»¤é˜¶æ®µ ==============
print(f"\n=== æœ€ç»ˆæ¨¡å‹è¿‡æ»¤é˜¶æ®µ: å‰”é™¤åŒ…å«æŒ‡å®šå…ƒç´ ç‰¹å¾çš„æ¨¡å‹ ===")
print(f"æ»¡è¶³RÂ²æ¡ä»¶(0.905â‰¤RÂ²<0.99)çš„æ¨¡å‹æ€»æ•°: {len(all_qualified_models)}")
print(f"éœ€è¦å‰”é™¤çš„å…ƒç´ ç‰¹å¾: {CONFIG['forbidden_features_for_final_model']}")

# è¿‡æ»¤åŒ…å«ç¦æ­¢å…ƒç´ ç‰¹å¾çš„æ¨¡å‹
best_models = []
filtered_models = []

for model_info in all_qualified_models:
    features = model_info['features']
    if isinstance(features, list):
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¦æ­¢çš„å…ƒç´ ç‰¹å¾
        forbidden_check = [f for f in features if f in CONFIG['forbidden_features_for_final_model']]
        if forbidden_check:
            filtered_models.append({
                'model_key': model_info['model_key'],
                'forbidden_features': forbidden_check,
                'all_features': features,
                'predictions_file': model_info.get('predictions_file')
            })
            print(f"âŒ å‰”é™¤æ¨¡å‹ {model_info['model_key']}: åŒ…å«ç¦æ­¢å…ƒç´ ç‰¹å¾ {forbidden_check}")
            # å¦‚æœæœ‰ä¿å­˜é¢„æµ‹æ–‡ä»¶ï¼Œä¹Ÿè®°å½•ä¸‹æ¥
            if model_info.get('predictions_file'):
                print(f"    ğŸ“ é¢„æµ‹æ–‡ä»¶: {os.path.basename(model_info['predictions_file'])}")
        else:
            best_models.append(model_info)
            print(f"âœ… ä¿ç•™æ¨¡å‹ {model_info['model_key']}: ä¸å«ç¦æ­¢å…ƒç´ ç‰¹å¾")
            # å¦‚æœæœ‰ä¿å­˜é¢„æµ‹æ–‡ä»¶ï¼Œä¹Ÿè®°å½•ä¸‹æ¥
            if model_info.get('predictions_file'):
                print(f"    ğŸ“ é¢„æµ‹æ–‡ä»¶: {os.path.basename(model_info['predictions_file'])}")
    else:
        # è½¬æ¢ç‰¹å¾æ–¹æ³•ï¼Œæ— æ³•æ£€æŸ¥å…·ä½“ç‰¹å¾åï¼Œä¿ç•™
        best_models.append(model_info)
        print(f"âœ… ä¿ç•™æ¨¡å‹ {model_info['model_key']}: ç‰¹å¾è½¬æ¢æ–¹æ³•")

print(f"\nâœ“ æœ€ç»ˆæ¨¡å‹è¿‡æ»¤å®Œæˆ:")
print(f"  æ»¡è¶³RÂ²æ¡ä»¶çš„æ¨¡å‹: {len(all_qualified_models)}ä¸ª")
print(f"  å‰”é™¤åŒ…å«å…ƒç´ ç‰¹å¾çš„æ¨¡å‹: {len(filtered_models)}ä¸ª")
print(f"  æœ€ç»ˆä¿ç•™çš„æ¨¡å‹: {len(best_models)}ä¸ª")

# ============== 6. æ˜¾ç¤ºç»“æœ ==============
print(f"\n=== æ¨¡å‹è¯„ä¼°ç»“æœ (ä½¿ç”¨ä¼ ç»Ÿ+é«˜çº§ç‰¹å¾é€‰æ‹©æ–¹æ³•+ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹+æ ‡å‡†éªŒè¯+é»˜è®¤å‚æ•°, æœ€ç»ˆå‰”é™¤åŒ…å«æŒ‡å®šå…ƒç´ ç‰¹å¾çš„æ¨¡å‹, æ— è´å¶æ–¯ä¼˜åŒ–) ===")

# åˆ›å»ºç‰¹å¾åç§°æ˜ å°„
feature_name_mapping = create_feature_name_mapping()

# ä¿å­˜æ‰€æœ‰ç»“æœåˆ°CSV
import csv
csv_file = os.path.join(CONFIG['output_dir'], 'all_model_results_physics_guided_no_bayesian_k4567_fixed.csv')
with open(csv_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Model Key", "K", "Selected Feature Indices", "Selected Feature Names", "Selected Feature Codes",
                    "Train RÂ²", "Test RÂ²", "Train RMSE", "Test RMSE", "Train MAE", "Test MAE", 
                    "Train MSE", "Test MSE", "Train RMSLE", "Test RMSLE", "Method Type", "Focus Combination",
                    "Data Type", "Model Instance ID"])
    for model_key, metrics in results_dict.items():
        sel_indices = metrics['Selected Indices']
        if sel_indices is not None:
            sel_indices_str = ', '.join(map(str, sel_indices))
            sel_names = [features_name[i] for i in sel_indices]
            sel_names_str = ', '.join(sel_names)
            # åº”ç”¨ç‰¹å¾åç§°æ˜ å°„
            sel_codes = apply_feature_name_mapping(sel_names, feature_name_mapping)
            sel_codes_str = ', '.join(sel_codes)
        else:
            sel_indices_str = "N/A (Transformed)"
            sel_names_str = "N/A (Transformed)"
            sel_codes_str = "N/A (Transformed)"
        
        # åˆ¤æ–­æ–¹æ³•ç±»å‹
        method_type = "ä¼ ç»Ÿæ–¹æ³•"
        if any(method in model_key for method in ['SFS_', 'SBS_', 'GA_']):
            method_type = "é«˜çº§æ–¹æ³•"
        
        writer.writerow([
            model_key, metrics['K'], sel_indices_str, sel_names_str, sel_codes_str,
            f"{metrics['Train RÂ²']:.6f}", f"{metrics['Test RÂ²']:.6f}", 
            f"{metrics['Train RMSE']:.6f}", f"{metrics['Test RMSE']:.6f}",
            f"{metrics['Train MAE']:.6f}", f"{metrics['Test MAE']:.6f}",
            f"{metrics['Train MSE']:.6f}", f"{metrics['Test MSE']:.6f}",
            f"{metrics['Train RMSLE']:.6f}", f"{metrics['Test RMSLE']:.6f}",
            method_type, "æ˜¯" if metrics['Focus Combination'] else "å¦",
            metrics.get('Data Type', 'Unknown'), metrics.get('Model Instance ID', 'Unknown')
        ])

print(f"âœ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {csv_file}")

# ä¿å­˜è¢«è¿‡æ»¤çš„æ¨¡å‹ä¿¡æ¯
filtered_csv = os.path.join(CONFIG['output_dir'], 'filtered_models_with_element_features_k4567_fixed.csv')
with open(filtered_csv, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Model Key", "Forbidden Features Found", "All Features", "All Feature Codes", "Predictions File"])
    for model_info in filtered_models:
        # åº”ç”¨ç‰¹å¾åç§°æ˜ å°„
        feature_codes = apply_feature_name_mapping(model_info['all_features'], feature_name_mapping)
        writer.writerow([
            model_info['model_key'],
            ', '.join(model_info['forbidden_features']),
            ', '.join(model_info['all_features']),
            ', '.join(feature_codes),
            os.path.basename(model_info.get('predictions_file', '')) if model_info.get('predictions_file') else ''
        ])

print(f"âœ“ è¢«è¿‡æ»¤çš„æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜åˆ°: {filtered_csv}")

# ä¿å­˜é¢„æµ‹æ–‡ä»¶ç´¢å¼•
predictions_index_file = os.path.join(CONFIG['output_dir'], 'predictions_files_index_k4567.csv')
with open(predictions_index_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Model Key", "Predictions File", "Status", "Features", "Feature Codes", "Test RÂ²", "Test MAE"])
    
    for model_info in all_qualified_models:
        status = "ä¿ç•™" if model_info in best_models else "å‰”é™¤"
        features_str = ', '.join(model_info['features']) if isinstance(model_info['features'], list) else model_info['features']
        
        # åº”ç”¨ç‰¹å¾åç§°æ˜ å°„
        if isinstance(model_info['features'], list):
            feature_codes = apply_feature_name_mapping(model_info['features'], feature_name_mapping)
            feature_codes_str = ', '.join(feature_codes)
        else:
            feature_codes_str = model_info['features']
        
        writer.writerow([
            model_info['model_key'],
            os.path.basename(model_info.get('predictions_file', '')) if model_info.get('predictions_file') else '',
            status,
            features_str,
            feature_codes_str,
            f"{model_info['test_r2']:.4f}",
            f"{model_info['test_mae']:.4f}"
        ])

print(f"âœ“ é¢„æµ‹æ–‡ä»¶ç´¢å¼•å·²ä¿å­˜åˆ°: {predictions_index_file}")

# ============== 7. é‡ç‚¹å±•ç¤ºæœ€ç»ˆä¿ç•™çš„æ¨¡å‹ ==============
if best_models:
    print(f"\n=== æœ€ç»ˆä¿ç•™çš„æ¨¡å‹ (K=4,5,6,7, 0.905â‰¤Train RÂ²<0.99, 0.905â‰¤Test RÂ²<0.99, ä¸å«æŒ‡å®šå…ƒç´ ç‰¹å¾, åŒ…å«ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹+æ ‡å‡†éªŒè¯+é»˜è®¤å‚æ•°, æ— è´å¶æ–¯ä¼˜åŒ–) ===")
    print(f"å…±ä¿ç•™ {len(best_models)} ä¸ªæ¨¡å‹ç»„åˆ")
    
    # ç»Ÿè®¡é«˜çº§æ–¹æ³•çš„è¡¨ç°
    advanced_models = [m for m in best_models if m['is_advanced']]
    focus_models = [m for m in best_models if m['is_focus_model']]
    focus_methods = [m for m in best_models if m['is_focus_method']]
    focus_combinations = [m for m in best_models if m['is_focus_combination']]
    
    print(f"å…¶ä¸­ {len(advanced_models)} ä¸ªæ¥è‡ªé«˜çº§æ–¹æ³• (SFS, SBS, GA)")
    print(f"å…¶ä¸­ {len(focus_models)} ä¸ªä½¿ç”¨äº†é‡ç‚¹æ¨¡å‹ {FOCUS_MODELS}")
    print(f"å…¶ä¸­ {len(focus_methods)} ä¸ªä½¿ç”¨äº†é‡ç‚¹æ–¹æ³• {FOCUS_METHODS}")
    print(f"å…¶ä¸­ {len(focus_combinations)} ä¸ªæ˜¯é‡ç‚¹ç»„åˆ (é‡ç‚¹æ¨¡å‹+é‡ç‚¹æ–¹æ³•)")
    print(f"æ³¨æ„: æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œæ— è¶…å‚æ•°ä¼˜åŒ–")
    print(f"æ³¨æ„: å½“å‰æµ‹è¯•14ç§æœºå™¨å­¦ä¹ æ¨¡å‹")
    print(f"æ³¨æ„: å·²å‰”é™¤åŒ…å«æŒ‡å®šå…ƒç´ ç‰¹å¾çš„æ¨¡å‹: {CONFIG['forbidden_features_for_final_model']}")
    print(f"ğŸ“ è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {predictions_dir}")
    
    # ä¿å­˜æœ€ç»ˆä¿ç•™çš„æ¨¡å‹
    best_csv = os.path.join(CONFIG['output_dir'], 'best_models_physics_guided_no_bayesian_k4567_fixed.csv')
    with open(best_csv, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["æ’å", "ç‰¹å¾é€‰æ‹©æ–¹æ³•", "æ¨¡å‹", "K", "Train RÂ²", "Test RÂ²", 
                        "Train MAE", "Test MAE", "Train RMSLE", "Test RMSLE", 
                        "é€‰æ‹©çš„ç‰¹å¾", "ç‰¹å¾ç®€ç§°ç¼–ç ", "æ–¹æ³•ç±»å‹", "é‡ç‚¹ç»„åˆ", "æ•°æ®ç±»å‹", "é¢„æµ‹æ–‡ä»¶"])
        
        # æŒ‰æµ‹è¯•é›†RÂ²æ’åº
        best_models.sort(key=lambda x: x['test_r2'], reverse=True)
        
        for idx, model_info in enumerate(best_models):
            features_str = ', '.join(model_info['features']) if isinstance(model_info['features'], list) else model_info['features']
            
            # åº”ç”¨ç‰¹å¾åç§°æ˜ å°„
            if isinstance(model_info['features'], list):
                feature_codes = apply_feature_name_mapping(model_info['features'], feature_name_mapping)
                feature_codes_str = ', '.join(feature_codes)
            else:
                feature_codes_str = model_info['features']
                
            method_type = "é«˜çº§æ–¹æ³•" if model_info['is_advanced'] else "ä¼ ç»Ÿæ–¹æ³•"
            focus_combination_str = "æ˜¯" if model_info['is_focus_combination'] else "å¦"
            predictions_file = os.path.basename(model_info.get('predictions_file', '')) if model_info.get('predictions_file') else ''
            
            writer.writerow([
                idx + 1,
                model_info['method'],
                model_info['model'],
                model_info['k'],
                f"{model_info['train_r2']:.4f}",
                f"{model_info['test_r2']:.4f}",
                f"{model_info['train_mae']:.4f}",
                f"{model_info['test_mae']:.4f}",
                f"{model_info['train_rmsle']:.4f}",
                f"{model_info['test_rmsle']:.4f}",
                features_str,
                feature_codes_str,
                method_type,
                focus_combination_str,
                model_info.get('data_type', 'Unknown'),
                predictions_file
            ])
    
    print(f"\nâœ“ æœ€ç»ˆä¿ç•™çš„æ¨¡å‹å·²ä¿å­˜åˆ°: {best_csv}")
    
    # ============== æ‰“å°æ‰€æœ‰æœ€ç»ˆä¿ç•™çš„æ¨¡å‹ ==============
    print(f"\n=== æ‰€æœ‰æœ€ç»ˆä¿ç•™çš„æ¨¡å‹è¯¦ç»†ä¿¡æ¯ ===")
    for idx, model_info in enumerate(best_models):
        features_str = ', '.join(model_info['features']) if isinstance(model_info['features'], list) else model_info['features']
        
        # åº”ç”¨ç‰¹å¾åç§°æ˜ å°„
        if isinstance(model_info['features'], list):
            feature_codes = apply_feature_name_mapping(model_info['features'], feature_name_mapping)
            feature_codes_str = ', '.join(feature_codes)
        else:
            feature_codes_str = model_info['features']
        
        advanced_mark = "â˜…" if model_info['is_advanced'] else " "
        focus_mark = "ğŸ¯" if model_info['is_focus_combination'] else " "
        
        print(f"\nç¬¬{idx+1}å{advanced_mark}{focus_mark}:")
        print(f"  æ–¹æ³•: {model_info['method']}, æ¨¡å‹: {model_info['model']}, K={model_info['k']}")
        print(f"  Train RÂ²: {model_info['train_r2']:.4f}, Test RÂ²: {model_info['test_r2']:.4f}")
        print(f"  Train MAE: {model_info['train_mae']:.4f}, Test MAE: {model_info['test_mae']:.4f}")
        print(f"  Train RMSLE: {model_info['train_rmsle']:.4f}, Test RMSLE: {model_info['test_rmsle']:.4f}")
        print(f"  ç‰¹å¾: {features_str}")
        print(f"  ç‰¹å¾ç¼–ç : {feature_codes_str}")
        print(f"  æ•°æ®ç±»å‹: {model_info.get('data_type', 'Unknown')}")
        
        # æ˜¾ç¤ºé¢„æµ‹æ–‡ä»¶ä¿¡æ¯
        if model_info.get('predictions_file'):
            filename = os.path.basename(model_info['predictions_file'])
            print(f"  ğŸ“ é¢„æµ‹æ–‡ä»¶: {filename}")
        
        # ç‰¹æ®Šæ ‡è®°
        marks = []
        if model_info['is_advanced']:
            marks.append("â˜… é«˜çº§æ–¹æ³•")
        if model_info['is_focus_combination']:
            marks.append("ğŸ¯ é‡ç‚¹ç»„åˆ")
        if model_info['is_focus_model']:
            marks.append("ğŸ¯ é‡ç‚¹æ¨¡å‹")
        if model_info['is_focus_method']:
            marks.append("ğŸ¯ é‡ç‚¹æ–¹æ³•")
        marks.append("âš¡ é»˜è®¤å‚æ•°")
        marks.append("ğŸš« æ— æŒ‡å®šå…ƒç´ ")
        marks.append("ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜")
        
        if marks:
            print(f"  æ ‡è®°: {' | '.join(marks)}")
    
    # ============== ä¿å­˜æ¨¡å‹å’Œæ•°æ® ==============
    print(f"\n=== ä¿å­˜æ¨¡å‹æ•°æ® ===")
    
    # ä¿å­˜æ¨¡å‹åˆ—è¡¨
    models_file = os.path.join(CONFIG['output_dir'], 'best_models_physics_guided_no_bayesian_k4567_fixed.pkl')
    with open(models_file, 'wb') as f:
        pickle.dump(best_models, f)
    print(f"âœ“ æ¨¡å‹åˆ—è¡¨å·²ä¿å­˜åˆ°: {models_file}")
    
    # ä¿å­˜ç‰¹å¾åç§°åˆ—è¡¨
    features_file = os.path.join(CONFIG['output_dir'], 'features_name_physics_guided_no_bayesian_k4567_fixed.pkl')
    with open(features_file, 'wb') as f:
        pickle.dump(features_name, f)
    print(f"âœ“ ç‰¹å¾åç§°å·²ä¿å­˜åˆ°: {features_file}")
    
    # ä¿å­˜ç‰¹å¾åç§°æ˜ å°„
    mapping_file = os.path.join(CONFIG['output_dir'], 'feature_name_mapping_k4567.pkl')
    with open(mapping_file, 'wb') as f:
        pickle.dump(feature_name_mapping, f)
    print(f"âœ“ ç‰¹å¾åç§°æ˜ å°„å·²ä¿å­˜åˆ°: {mapping_file}")
    
    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    scaler_file = os.path.join(CONFIG['output_dir'], 'scaler_physics_guided_no_bayesian_k4567_fixed.pkl')
    joblib.dump(scaler, scaler_file)
    print(f"âœ“ æ ‡å‡†åŒ–å™¨å·²ä¿å­˜åˆ°: {scaler_file}")
    
    # ä¿å­˜å…¶ä»–å¿…è¦çš„æ•°æ®
    data_info = {
        'X_all_scaled': X_all_scaled,
        'X_all_original': X_all_original,
        'y_all_np': y_all_np,
        'initial_features_name': initial_features_name,
        'tree_based_models': tree_based_models,
        'CONFIG': CONFIG,
        'FOCUS_MODELS': FOCUS_MODELS,
        'FOCUS_METHODS': FOCUS_METHODS,
        'forbidden_features_for_final_model': CONFIG['forbidden_features_for_final_model'],
        'advanced_methods_included': True,  # æ ‡è®°åŒ…å«é«˜çº§æ–¹æ³•
        'enhanced_metrics_included': True,   # æ ‡è®°åŒ…å«å¢å¼ºæŒ‡æ ‡
        'bayesian_optimization_included': False,  # æ ‡è®°æœªåŒ…å«è´å¶æ–¯ä¼˜åŒ–
        'physics_guided_enhancement_included': True,  # æ ‡è®°åŒ…å«ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹
        'default_parameters_only': True,           # æ ‡è®°ä»…ä½¿ç”¨é»˜è®¤å‚æ•°
        'element_features_filtered_at_end': True,  # æ ‡è®°åœ¨æœ€ç»ˆé˜¶æ®µè¿‡æ»¤å…ƒç´ ç‰¹å¾
        'total_qualified_models': len(all_qualified_models),  # æ€»çš„æ»¡è¶³æ¡ä»¶æ¨¡å‹æ•°
        'filtered_models_count': len(filtered_models),  # è¢«è¿‡æ»¤çš„æ¨¡å‹æ•°
        'final_models_count': len(best_models),  # æœ€ç»ˆä¿ç•™çš„æ¨¡å‹æ•°
        'detailed_predictions_saved': CONFIG['save_detailed_predictions'],  # æ ‡è®°æ˜¯å¦ä¿å­˜è¯¦ç»†é¢„æµ‹
        'predictions_directory': predictions_dir,  # é¢„æµ‹æ–‡ä»¶ç›®å½•
        'data_consistency_check_enabled': CONFIG['enable_data_consistency_check'],  # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
        'debug_mode_enabled': CONFIG['debug_mode'],  # è°ƒè¯•æ¨¡å¼
        'timestamp': timestamp,  # æ—¶é—´æˆ³
        'target_k_range': CONFIG['target_k_range'],  # Kå€¼èŒƒå›´
        'feature_name_mapping': feature_name_mapping,  # ç‰¹å¾åç§°æ˜ å°„
        'physics_guided_config': {
            'exact_feature_names': CONFIG['exact_feature_names'],
            'auto_feature_detection': CONFIG['auto_feature_detection'],
            'quality_check_enabled': CONFIG['quality_check_enabled'],
            'correlation_threshold': CONFIG['correlation_threshold'],
            'variance_threshold': CONFIG['variance_threshold']
        }
    }
    data_file = os.path.join(CONFIG['output_dir'], 'data_info_physics_guided_no_bayesian_k4567_fixed.pkl')
    with open(data_file, 'wb') as f:
        pickle.dump(data_info, f)
    print(f"âœ“ æ•°æ®ä¿¡æ¯å·²ä¿å­˜åˆ°: {data_file}")
    
else:
    print(f"\nâš ï¸ æ²¡æœ‰æœ€ç»ˆä¿ç•™çš„æ¨¡å‹ (ç»è¿‡å…ƒç´ ç‰¹å¾è¿‡æ»¤å)")
    print("å¯èƒ½çš„åŸå› :")
    print("1. æ»¡è¶³RÂ²æ¡ä»¶çš„æ¨¡å‹éƒ½åŒ…å«äº†æŒ‡å®šçš„å…ƒç´ ç‰¹å¾")
    print("2. RÂ²é˜ˆå€¼è®¾ç½®è¿‡ä¸¥æ ¼")
    print("3. æŒ‡å®šçš„ç¦æ­¢å…ƒç´ ç‰¹å¾è¿‡å¤š")
    print(f"   æ»¡è¶³RÂ²æ¡ä»¶çš„æ¨¡å‹æ€»æ•°: {len(all_qualified_models)}")
    print(f"   è¢«è¿‡æ»¤çš„æ¨¡å‹æ•°: {len(filtered_models)}")
    print(f"   ç¦æ­¢çš„å…ƒç´ ç‰¹å¾: {CONFIG['forbidden_features_for_final_model']}")

# ============== 8. ç»Ÿè®¡åˆ†æ ==============
print(f"\n=== ç»Ÿè®¡åˆ†æ (æœ€ç»ˆæ¨¡å‹è¿‡æ»¤ç­–ç•¥: åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåå‰”é™¤åŒ…å«æŒ‡å®šå…ƒç´ ç‰¹å¾çš„æ¨¡å‹) ===")

print(f"\næ¨¡å‹è¿‡æ»¤ç»Ÿè®¡:")
print(f"  æ»¡è¶³RÂ²æ¡ä»¶(0.905â‰¤RÂ²<0.99)çš„æ¨¡å‹æ€»æ•°: {len(all_qualified_models)}")
print(f"  åŒ…å«æŒ‡å®šå…ƒç´ ç‰¹å¾è¢«å‰”é™¤çš„æ¨¡å‹æ•°: {len(filtered_models)}")
print(f"  æœ€ç»ˆä¿ç•™çš„æ¨¡å‹æ•°: {len(best_models)}")
print(f"  æ¨¡å‹ä¿ç•™ç‡: {len(best_models)/len(all_qualified_models)*100:.1f}%" if all_qualified_models else "0%")

if filtered_models:
    print(f"\nè¢«å‰”é™¤æ¨¡å‹çš„å…ƒç´ ç‰¹å¾ç»Ÿè®¡:")
    element_count = {}
    for model_info in filtered_models:
        for element in model_info['forbidden_features']:
            element_count[element] = element_count.get(element, 0) + 1
    
    sorted_elements = sorted(element_count.items(), key=lambda x: x[1], reverse=True)
    for element, count in sorted_elements:
        print(f"  {element}: {count}ä¸ªæ¨¡å‹åŒ…å«")

if best_models:
    print(f"\næœ€ç»ˆä¿ç•™æ¨¡å‹çš„æ€§èƒ½ç»Ÿè®¡:")
    final_train_r2 = [m['train_r2'] for m in best_models]
    final_test_r2 = [m['test_r2'] for m in best_models]
    final_train_mae = [m['train_mae'] for m in best_models]
    final_test_mae = [m['test_mae'] for m in best_models]
    
    print(f"  è®­ç»ƒé›†RÂ² - å¹³å‡: {np.mean(final_train_r2):.4f}, æœ€å¤§: {np.max(final_train_r2):.4f}, æœ€å°: {np.min(final_train_r2):.4f}")
    print(f"  æµ‹è¯•é›†RÂ² - å¹³å‡: {np.mean(final_test_r2):.4f}, æœ€å¤§: {np.max(final_test_r2):.4f}, æœ€å°: {np.min(final_test_r2):.4f}")
    print(f"  è®­ç»ƒé›†MAE - å¹³å‡: {np.mean(final_train_mae):.4f}, æœ€å¤§: {np.max(final_train_mae):.4f}, æœ€å°: {np.min(final_train_mae):.4f}")
    print(f"  æµ‹è¯•é›†MAE - å¹³å‡: {np.mean(final_test_mae):.4f}, æœ€å¤§: {np.max(final_test_mae):.4f}, æœ€å°: {np.min(final_test_mae):.4f}")
    
    # æŒ‰Kå€¼ç»Ÿè®¡
    k_stats = {}
    for model in best_models:
        k = model['k']
        if k not in k_stats:
            k_stats[k] = []
        k_stats[k].append(model['test_r2'])
    
    print(f"\næŒ‰ç‰¹å¾æ•°Kç»Ÿè®¡:")
    for k in sorted(k_stats.keys()):
        r2_values = k_stats[k]
        print(f"  K={k}: {len(r2_values)}ä¸ªæ¨¡å‹, å¹³å‡RÂ²={np.mean(r2_values):.4f}, æœ€ä½³RÂ²={np.max(r2_values):.4f}")

# ============== 9. æ‰¾å‡ºæœ€ä¼˜æ¨¡å‹ ==============
if best_models:
    best_model_info = max(best_models, key=lambda x: x['test_r2'])
    
    print(f"\n=== å…¨å±€æœ€ä¼˜ç»“æœ (ç»è¿‡å…ƒç´ ç‰¹å¾è¿‡æ»¤å) ===")
    print(f"æœ€ä¼˜æ¨¡å‹: {best_model_info['model_key']}")
    print(f"æµ‹è¯•é›†RÂ²: {best_model_info['test_r2']:.4f}")
    print(f"æµ‹è¯•é›†MAE: {best_model_info['test_mae']:.4f}")
    print(f"æµ‹è¯•é›†RMSLE: {best_model_info['test_rmsle']:.4f}")
    print(f"ç‰¹å¾é€‰æ‹©æ–¹æ³•: {best_model_info['method']}")
    print(f"æœºå™¨å­¦ä¹ æ¨¡å‹: {best_model_info['model']}")
    
    features_for_display = best_model_info['features'] if isinstance(best_model_info['features'], list) else [best_model_info['features']]
    print(f"é€‰æ‹©çš„ç‰¹å¾: {', '.join(features_for_display)}")
    
    # æ˜¾ç¤ºç‰¹å¾ç¼–ç 
    if isinstance(best_model_info['features'], list):
        feature_codes = apply_feature_name_mapping(best_model_info['features'], feature_name_mapping)
        print(f"ç‰¹å¾ç¼–ç : {', '.join(feature_codes)}")
    
    print(f"ä½¿ç”¨çš„æ•°æ®ç±»å‹: {best_model_info.get('data_type', 'Unknown')}")
    
    if best_model_info.get('predictions_file'):
        print(f"ğŸ“ è¯¦ç»†é¢„æµ‹ç»“æœæ–‡ä»¶: {os.path.basename(best_model_info['predictions_file'])}")
    
    marks = []
    if best_model_info['is_advanced']:
        marks.append("â˜… é«˜çº§æ–¹æ³•")
    if best_model_info['is_focus_combination']:
        marks.append("ğŸ¯ é‡ç‚¹ç»„åˆ")
    marks.append("âš¡ é»˜è®¤å‚æ•°")
    marks.append("ğŸš« æ— æŒ‡å®šå…ƒç´ ")
    marks.append("ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜")
    
    if marks:
        print(f"ç‰¹æ®Šæ ‡è®°: {' | '.join(marks)}")
    
    # ä¿å­˜æœ€ä¼˜æ¨¡å‹ä¿¡æ¯
    best_info = {
        'best_model': best_model_info['model_key'],
        'best_r2': best_model_info['test_r2'],
        'best_mae': best_model_info['test_mae'],
        'best_rmsle': best_model_info['test_rmsle'],
        'best_method': best_model_info['method'],
        'best_ml_model': best_model_info['model'],
        'best_features': best_model_info['features'],
        'best_feature_codes': apply_feature_name_mapping(best_model_info['features'], feature_name_mapping) if isinstance(best_model_info['features'], list) else best_model_info['features'],
        'best_is_advanced': best_model_info['is_advanced'],
        'best_is_focus_combination': best_model_info['is_focus_combination'],
        'best_data_type': best_model_info.get('data_type', 'Unknown'),
        'best_predictions_file': best_model_info.get('predictions_file'),
        'total_feature_sets': len(good_feature_sets),
        'total_models_tested': len(results_dict),
        'total_qualified_models': len(all_qualified_models),
        'filtered_models': len(filtered_models),
        'final_models': len(best_models),
        'forbidden_features': CONFIG['forbidden_features_for_final_model'],
        'physics_guided_enhancement': CONFIG['physics_guided_enhancement'],
        'bayesian_optimization': False,
        'default_parameters_only': True,
        'element_features_filtered_at_end': True,
        'detailed_predictions_saved': CONFIG['save_detailed_predictions'],
        'data_consistency_check_enabled': CONFIG['enable_data_consistency_check'],
        'debug_mode_enabled': CONFIG['debug_mode'],
        'min_r2_threshold': CONFIG['min_r2_threshold'],
        'max_r2_threshold': CONFIG['max_r2_threshold'],
        'target_k_range': CONFIG['target_k_range'],
        'timestamp': timestamp
    }
    
    best_file = os.path.join(CONFIG['output_dir'], 'best_model_info_physics_guided_no_bayesian_k4567_fixed.txt')
    with open(best_file, 'w', encoding='utf-8') as f:
        f.write("=== æ¨¡å‹è®­ç»ƒæ€»ç»“ (ä¿®å¤ç‰ˆ: ä¼ ç»Ÿ+é«˜çº§ç‰¹å¾é€‰æ‹©æ–¹æ³•+ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹+æ ‡å‡†éªŒè¯+é»˜è®¤å‚æ•°, K=4,5,6,7, æœ€ç»ˆå‰”é™¤åŒ…å«æŒ‡å®šå…ƒç´ ç‰¹å¾çš„æ¨¡å‹, æ— è´å¶æ–¯ä¼˜åŒ–) ===\n")
        f.write(f"æœ€ä¼˜æ¨¡å‹: {best_info['best_model']}\n")
        f.write(f"æœ€ä¼˜æµ‹è¯•é›†RÂ²: {best_info['best_r2']:.4f}\n")
        f.write(f"æœ€ä¼˜æµ‹è¯•é›†MAE: {best_info['best_mae']:.4f}\n")
        f.write(f"æœ€ä¼˜æµ‹è¯•é›†RMSLE: {best_info['best_rmsle']:.4f}\n")
        f.write(f"æœ€ä¼˜ç‰¹å¾é€‰æ‹©æ–¹æ³•: {best_info['best_method']}\n")
        f.write(f"æœ€ä¼˜æœºå™¨å­¦ä¹ æ¨¡å‹: {best_info['best_ml_model']}\n")
        
        features_for_txt = best_info['best_features'] if isinstance(best_info['best_features'], list) else [best_info['best_features']]
        f.write(f"æœ€ä¼˜æ¨¡å‹ç‰¹å¾: {', '.join(features_for_txt)}\n")
        
        codes_for_txt = best_info['best_feature_codes'] if isinstance(best_info['best_feature_codes'], list) else [best_info['best_feature_codes']]
        f.write(f"æœ€ä¼˜æ¨¡å‹ç‰¹å¾ç¼–ç : {', '.join(codes_for_txt)}\n")
        
        f.write(f"æœ€ä¼˜æ¨¡å‹ä½¿ç”¨çš„æ•°æ®ç±»å‹: {best_info['best_data_type']}\n")
        f.write(f"æœ€ä¼˜æ¨¡å‹é¢„æµ‹æ–‡ä»¶: {os.path.basename(best_info['best_predictions_file']) if best_info['best_predictions_file'] else 'None'}\n")
        f.write(f"æœ€ä¼˜æ¨¡å‹æ˜¯å¦æ¥è‡ªé«˜çº§æ–¹æ³•: {'æ˜¯' if best_info['best_is_advanced'] else 'å¦'}\n")
        f.write(f"æœ€ä¼˜æ¨¡å‹æ˜¯å¦æ˜¯é‡ç‚¹ç»„åˆ: {'æ˜¯' if best_info['best_is_focus_combination'] else 'å¦'}\n")
        f.write(f"ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–: {'æ˜¯' if best_info['bayesian_optimization'] else 'å¦'}\n")
        f.write(f"ä»…ä½¿ç”¨é»˜è®¤å‚æ•°: {'æ˜¯' if best_info['default_parameters_only'] else 'å¦'}\n")
        f.write(f"æœ€ç»ˆé˜¶æ®µè¿‡æ»¤å…ƒç´ ç‰¹å¾: {'æ˜¯' if best_info['element_features_filtered_at_end'] else 'å¦'}\n")
        f.write(f"ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ: {'æ˜¯' if best_info['detailed_predictions_saved'] else 'å¦'}\n")
        f.write(f"æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥: {'å¯ç”¨' if best_info['data_consistency_check_enabled'] else 'ç¦ç”¨'}\n")
        f.write(f"è°ƒè¯•æ¨¡å¼: {'å¯ç”¨' if best_info['debug_mode_enabled'] else 'ç¦ç”¨'}\n")
        f.write(f"ç‰¹å¾æ•°èŒƒå›´: K={best_info['target_k_range']}\n")
        f.write(f"ç¨‹åºè¿è¡Œæ—¶é—´æˆ³: {best_info['timestamp']}\n")
        f.write(f"\n=== æ¨¡å‹ç»Ÿè®¡ ===\n")
        f.write(f"è¯„ä¼°çš„ç‰¹å¾é›†æ€»æ•°: {best_info['total_feature_sets']}\n")
        f.write(f"è®­ç»ƒçš„æ¨¡å‹æ€»æ•°: {best_info['total_models_tested']}\n")
        f.write(f"æ»¡è¶³RÂ²æ¡ä»¶(0.905â‰¤RÂ²<0.99)çš„æ¨¡å‹æ€»æ•°: {best_info['total_qualified_models']}\n")
        f.write(f"åŒ…å«æŒ‡å®šå…ƒç´ ç‰¹å¾è¢«å‰”é™¤çš„æ¨¡å‹æ•°: {best_info['filtered_models']}\n")
        f.write(f"æœ€ç»ˆä¿ç•™çš„æ¨¡å‹æ•°: {best_info['final_models']}\n")
        f.write(f"æ¨¡å‹ä¿ç•™ç‡: {best_info['final_models']/best_info['total_qualified_models']*100:.1f}%\n")
        f.write(f"RÂ²é˜ˆå€¼èŒƒå›´: {best_info['min_r2_threshold']:.3f} â‰¤ RÂ² < {best_info['max_r2_threshold']:.2f} (é¿å…è¿‡æ‹Ÿåˆ)\n")
        f.write(f"æœ€ç»ˆå‰”é™¤çš„æŒ‡å®šå…ƒç´ ç‰¹å¾: {best_info['forbidden_features']}\n")
        f.write(f"ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹: {'å¯ç”¨' if best_info['physics_guided_enhancement'] else 'ç¦ç”¨'}\n")
        f.write(f"\n=== ä¿®å¤å†…å®¹ ===\n")
        f.write(f"1. ä¿®æ”¹target_k_rangeä¸º[4,5,6,7]ï¼Œæ‰©å±•ç‰¹å¾æ•°èŒƒå›´\n")
        f.write(f"2. æ·»åŠ ç‰¹å¾åç§°ç®€ç§°ç¼–ç æ˜ å°„ï¼ŒåŸºäºPDFæ–‡æ¡£çš„139ä¸ªç‰¹å¾ç¼–ç ç³»ç»Ÿ\n")
        f.write(f"3. ä¿®å¤CSVæ–‡ä»¶ä¿å­˜æ—¶çš„ç¼–ç é”™è¯¯é—®é¢˜\n")
        f.write(f"4. æ‰€æœ‰è¾“å‡ºæ–‡ä»¶åæ·»åŠ k4567æ ‡è¯†ï¼ŒåŒºåˆ†ä¸åŒKå€¼èŒƒå›´çš„ç»“æœ\n")
        f.write(f"5. åœ¨ç»“æœæ˜¾ç¤ºä¸­åŒæ—¶æ˜¾ç¤ºåŸç‰¹å¾åå’Œç®€ç§°ç¼–ç \n")
        f.write(f"6. ä¿å­˜ç‰¹å¾åç§°æ˜ å°„å­—å…¸ï¼Œä¾¿äºåç»­ä½¿ç”¨\n")
        f.write(f"\n=== ç‰¹å¾ç¼–ç ç³»ç»Ÿ ===\n")
        f.write(f"æˆåˆ†ç‰¹å¾(CC): 12ä¸ªå…ƒç´ æˆåˆ†ç‰¹å¾ï¼ŒCC1-CC12\n")
        f.write(f"åŸå­ç”µå­(AE): 10ä¸ªåŸå­ç”µå­æ€§è´¨ç‰¹å¾ï¼ŒAE1m-AE5v\n")
        f.write(f"åŸå­åŠå¾„(AR): åŸå­åŠå¾„ç›¸å…³ç‰¹å¾\n")
        f.write(f"åŸå­ç£æ€§(AM): ç£åŒ–ç‡ç›¸å…³ç‰¹å¾\n")
        f.write(f"åŸå­æ ¸(AN): åŸå­è´¨é‡ç›¸å…³ç‰¹å¾\n")
        f.write(f"åŸå­é‡é‡(AW): åŸå­é‡é‡ç›¸å…³ç‰¹å¾\n")
        f.write(f"çƒ­åŠ›å­¦(TH): 8ä¸ªçƒ­åŠ›å­¦ç‰¹å¾\n")
        f.write(f"æœºæ¢°æ€§èƒ½(PM): 6ä¸ªæœºæ¢°æ€§èƒ½ç‰¹å¾\n")
        f.write(f"çƒ­å­¦æ€§èƒ½(PT): 18ä¸ªçƒ­å­¦æ€§èƒ½ç‰¹å¾\n")
        f.write(f"ç”µå­¦æ€§èƒ½(PE): 4ä¸ªç”µå­¦æ€§èƒ½ç‰¹å¾\n")
        f.write(f"å¯†åº¦æ€§èƒ½(PD): 4ä¸ªå¯†åº¦æ€§èƒ½ç‰¹å¾\n")
        f.write(f"æ™¶æ ¼ç»“æ„(SL): 6ä¸ªæ™¶æ ¼ç»“æ„ç‰¹å¾\n")
        f.write(f"ç©ºé—´ç¾¤(SS): 2ä¸ªç©ºé—´ç¾¤ç‰¹å¾\n")
        f.write(f"ç‰©ç†å·¥ç¨‹ç‰¹å¾(36ä¸ª): JTéŸ§æ€§åˆ¤æ®5ä¸ª, JEç”µå­ç»“æ„4ä¸ª, JHçƒ­åŠ›å­¦4ä¸ª, JN-NbSiä¸“ç”¨5ä¸ª, JMåŠ›å­¦5ä¸ª, JDæ‰©æ•£2ä¸ª, JAåˆé‡‘3ä¸ª, JSç»“æ„5ä¸ª, JXå…¶ä»–3ä¸ª\n")
        f.write(f"\n=== æŠ€æœ¯é…ç½® ===\n")
        f.write(f"ç‰¹å¾æ•°: K=4,5,6,7 (ä»åŸæ¥çš„K=4,5æ‰©å±•)\n")
        f.write(f"ä¼ ç»Ÿç‰¹å¾é€‰æ‹©æ–¹æ³•: 8ç§ (RFE, äº’ä¿¡æ¯, Få›å½’, PCC, Lasso, éšæœºæ£®æ—, ANOVA, SelectKBest)\n")
        f.write(f"é«˜çº§ç‰¹å¾é€‰æ‹©æ–¹æ³•: 3ç§ (SFS, SBS, GA)\n")
        f.write(f"SFSé…ç½®: Ridgeå›å½’è¯„ä¼°ï¼Œæœ€å¤§{CONFIG['sfs_max_iter']}æ¬¡è¿­ä»£\n")
        f.write(f"SBSé…ç½®: Ridgeå›å½’è¯„ä¼°ï¼Œæœ€å¤§{CONFIG['sbs_max_iter']}æ¬¡è¿­ä»£\n")
        f.write(f"GAé…ç½®: ç§ç¾¤{CONFIG['ga_population']}ï¼Œä»£æ•°{CONFIG['ga_generations']}ï¼Œå˜å¼‚ç‡{CONFIG['ga_mutation_rate']}\n")
        f.write(f"ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹é…ç½®: åŸºäºå®é™…æ•°æ®åº“ç‰¹å¾çš„34ä¸ªç‰©ç†æ„ä¹‰ç‰¹å¾\n")
        f.write(f"æœºå™¨å­¦ä¹ æ¨¡å‹: 16ç§\n")
        f.write(f"æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œæ— è¶…å‚æ•°ä¼˜åŒ–\n")
        f.write(f"è¯„ä¼°æŒ‡æ ‡: RÂ², RMSE, MAE, MSE, RMSLE (è®­ç»ƒé›†å’Œæµ‹è¯•é›†)\n")
        f.write(f"éªŒè¯ç­–ç•¥: æ ‡å‡†80%-20%åˆ†å‰²éªŒè¯\n")
        f.write(f"é‡ç‚¹æ¨¡å‹: {FOCUS_MODELS}\n")
        f.write(f"é‡ç‚¹æ–¹æ³•: {FOCUS_METHODS}\n")
        f.write(f"ç‰¹å¾ç­›é€‰ç­–ç•¥: ä¼ ç»Ÿç»Ÿè®¡æ–¹æ³•+é«˜çº§æœç´¢ç®—æ³•+ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹\n")
        f.write(f"è¯„ä¼°ç­–ç•¥: é«˜çº§æ–¹æ³•å†…éƒ¨ä½¿ç”¨Ridgeå›å½’3æŠ˜äº¤å‰éªŒè¯\n")
        f.write(f"è¶…å‚æ•°ä¼˜åŒ–ç­–ç•¥: æ— è¶…å‚æ•°ä¼˜åŒ–ï¼Œæ‰€æœ‰æ¨¡å‹ä½¿ç”¨é»˜è®¤å‚æ•°\n")
        f.write(f"å…ƒç´ ç‰¹å¾è¿‡æ»¤ç­–ç•¥: åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåå‰”é™¤åŒ…å«æŒ‡å®šå…ƒç´ ç‰¹å¾çš„æ¨¡å‹\n")
        f.write(f"å…ƒç´ ç‰¹å¾é™åˆ¶: å‰”é™¤åŒ…å«{len(CONFIG['forbidden_features_for_final_model'])}ä¸ªå…ƒç´ ç‰¹å¾çš„æ¨¡å‹: {CONFIG['forbidden_features_for_final_model']}\n")
        f.write(f"é€’å½’æ¶ˆé™¤é…ç½®: max_k={CONFIG['max_k']}ï¼Œä½¿ç”¨Ridgeã€Huberã€éšæœºæ£®æ—ç»„åˆè¯„ä¼°\n")
        f.write(f"éªŒè¯ç­–ç•¥: æ ‡å‡†80%-20%è®­ç»ƒæµ‹è¯•åˆ†å‰²\n")
        f.write(f"å¹¶è¡Œè®¾ç½®: ä½¿ç”¨ {CONFIG['n_jobs']} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œè®¡ç®—\n")
        f.write(f"æ–¹æ³•åˆ›æ–°: ç»“åˆä¼ ç»Ÿç»Ÿè®¡æ–¹æ³•ã€ç°ä»£æœç´¢ç®—æ³•ã€ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹ï¼Œå…¨é¢æ¢ç´¢ç‰¹å¾ç©ºé—´\n")
        f.write(f"å¢å¼ºåŠŸèƒ½: æ–°å¢MAEã€MSEã€RMSLEç­‰è¯„ä¼°æŒ‡æ ‡ï¼Œæä¾›æ›´å…¨é¢çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°\n")
        f.write(f"ç‰©ç†å¼•å¯¼ç‰¹å¾å·¥ç¨‹: åŸºäºæ‚¨å®é™…æ•°æ®åº“ç‰¹å¾çš„34ä¸ªç‰©ç†æ„ä¹‰æ˜ç¡®çš„ç‰¹å¾ï¼Œæ¶µç›–éŸ§æ€§åˆ¤æ®ã€ç”µå­ç»“æ„ã€çƒ­åŠ›å­¦ç­‰9å¤§ç±»åˆ«\n")
        f.write(f"æ€§èƒ½ä¼˜åŒ–: ç§»é™¤è´å¶æ–¯ä¼˜åŒ–ï¼Œå¤§å¹…å‡å°‘è®¡ç®—æ—¶é—´ï¼Œé€‚åˆå¿«é€ŸåŸå‹å¼€å‘å’ŒåŸºå‡†æµ‹è¯•\n")
        f.write(f"è´¨é‡æ§åˆ¶: æœ€ç»ˆé˜¶æ®µçš„å…ƒç´ ç‰¹å¾è¿‡æ»¤ï¼Œç¡®ä¿ä¿å­˜çš„æ¨¡å‹ä¸ä¾èµ–ç‰¹å®šçš„å…ƒç´ æˆåˆ†\n")
        f.write(f"æ•°æ®å®Œæ•´æ€§: æ¯ä¸ªæ»¡è¶³æ¡ä»¶çš„æ¨¡å‹éƒ½æœ‰å¯¹åº”çš„è¯¦ç»†é¢„æµ‹ç»“æœæ–‡ä»¶ï¼Œç¡®ä¿ç»“æœå¯è¿½æº¯\n")
        f.write(f"ç¼–ç ç³»ç»Ÿ: åŸºäºPDFæ–‡æ¡£çš„139ä¸ªç‰¹å¾å®Œæ•´ç¼–ç ç³»ç»Ÿï¼Œæä¾›ç®€æ´çš„ç‰¹å¾æ ‡è¯†\n")
    
    print(f"âœ“ æœ€ä¼˜æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜åˆ°: {best_file}")

print(f"\n=== ç¨‹åºæ‰§è¡Œå®Œæˆ (ä¿®å¤ç‰ˆ K=4,5,6,7) ===")
print(f"è¾“å‡ºç›®å½•: {CONFIG['output_dir']}")
print(f"ç‰¹å¾æ•°èŒƒå›´: K={CONFIG['target_k_range']}")
print(f"æœ€ç»ˆå‰”é™¤åŒ…å«ä»¥ä¸‹å…ƒç´ ç‰¹å¾çš„æ¨¡å‹: {CONFIG['forbidden_features_for_final_model']}")
print(f"RÂ²é˜ˆå€¼èŒƒå›´: {CONFIG['min_r2_threshold']:.2f} â‰¤ RÂ² < {CONFIG['max_r2_threshold']:.2f} (é¿å…è¿‡æ‹Ÿåˆ)")
print(f"ç¨‹åºè¿è¡Œæ—¶é—´æˆ³: {timestamp}")
print(f"\nè¯·æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶:")
print(f"1. all_model_results_physics_guided_no_bayesian_k4567_fixed.csv - æ‰€æœ‰æ¨¡å‹ç»“æœ(å«ç‰¹å¾ç¼–ç )")
print(f"2. best_models_physics_guided_no_bayesian_k4567_fixed.csv - æœ€ç»ˆä¿ç•™çš„æ¨¡å‹(å«ç‰¹å¾ç¼–ç )")
print(f"3. filtered_models_with_element_features_k4567_fixed.csv - è¢«å‰”é™¤çš„æ¨¡å‹(å«ç‰¹å¾ç¼–ç )")
print(f"4. predictions_files_index_k4567.csv - é¢„æµ‹æ–‡ä»¶ç´¢å¼•(å«ç‰¹å¾ç¼–ç )")
print(f"5. best_model_info_physics_guided_no_bayesian_k4567_fixed.txt - æœ€ä¼˜æ¨¡å‹æ€»ç»“")
print(f"6. best_models_physics_guided_no_bayesian_k4567_fixed.pkl - ä¿å­˜çš„æ¨¡å‹å®ä¾‹")
print(f"7. features_name_physics_guided_no_bayesian_k4567_fixed.pkl - ç‰¹å¾åç§°")
print(f"8. feature_name_mapping_k4567.pkl - ç‰¹å¾åç§°æ˜ å°„å­—å…¸")
print(f"9. scaler_physics_guided_no_bayesian_k4567_fixed.pkl - æ ‡å‡†åŒ–å™¨")
print(f"10. data_info_physics_guided_no_bayesian_k4567_fixed.pkl - æ•°æ®ä¿¡æ¯")
print(f"11. detailed_predictions/ - è¯¦ç»†é¢„æµ‹ç»“æœç›®å½•")
print(f"\nâœ… ä¸»è¦ä¿®æ”¹å†…å®¹:")
print(f"ğŸ”§ Kå€¼èŒƒå›´: ä»[4,5]æ‰©å±•åˆ°[4,5,6,7]ï¼Œå¢åŠ ç‰¹å¾é€‰æ‹©çš„çµæ´»æ€§")
print(f"ğŸ”§ ç‰¹å¾ç¼–ç : åŸºäºPDFæ–‡æ¡£çš„139ä¸ªç‰¹å¾ç¼–ç ç³»ç»Ÿï¼Œæä¾›ç®€æ´æ ‡è¯†")
print(f"ğŸ”§ ç¼–ç é”™è¯¯ä¿®å¤: è§£å†³CSVæ–‡ä»¶ä¿å­˜æ—¶çš„å­—ç¬¦ç¼–ç é—®é¢˜")
print(f"ğŸ”§ æ–‡ä»¶å‘½å: æ‰€æœ‰è¾“å‡ºæ–‡ä»¶æ·»åŠ k4567æ ‡è¯†ï¼Œä¾¿äºè¯†åˆ«")
print(f"ğŸ”§ æ˜¾ç¤ºä¼˜åŒ–: ç»“æœä¸­åŒæ—¶æ˜¾ç¤ºå®Œæ•´ç‰¹å¾åå’Œç®€ç§°ç¼–ç ")
print(f"ğŸ”§ æ˜ å°„ä¿å­˜: å•ç‹¬ä¿å­˜ç‰¹å¾åç§°æ˜ å°„å­—å…¸ï¼Œä¾¿äºåç»­ä½¿ç”¨")
print(f"\nç°åœ¨æ‚¨å¯ä»¥:")
print(f"â€¢ æµ‹è¯•K=4,5,6,7å››ç§ä¸åŒçš„ç‰¹å¾æ•°ç»„åˆ")
print(f"â€¢ ä½¿ç”¨ç®€æ´çš„ç‰¹å¾ç¼–ç (å¦‚CC1, AE2m, JT1ç­‰)å¿«é€Ÿè¯†åˆ«ç‰¹å¾")
print(f"â€¢ æ ¹æ®ç¼–ç ç³»ç»Ÿå¿«é€Ÿäº†è§£ç‰¹å¾çš„ç‰©ç†æ„ä¹‰å’Œç±»åˆ«")
print(f"â€¢ é¿å…é•¿ç‰¹å¾åé€ æˆçš„æ˜¾ç¤ºå’Œåˆ†æå›°éš¾")
print(f"â€¢ ä½¿ç”¨æ ‡å‡†åŒ–çš„ç‰¹å¾å‘½åä½“ç³»è¿›è¡Œåç»­ç ”ç©¶")
