{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce7ce7d-1f49-472d-a3cd-dd8d630f421b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GBDTæ¨¡å‹PDPåˆ†æç¨‹åºï¼ˆç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†ç‰ˆï¼‰\n",
      "================================================================================\n",
      "è¾“å‡ºç›®å½•: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\GBDTæ¨¡å‹PDPåˆ†æç»“æœ\n",
      "åˆ†æç±»å‹: GBDTæ¨¡å‹ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç† + å®Œæ•´PDPå’Œäº¤äº’åˆ†æ\n",
      "matplotlibåç«¯: Agg\n",
      "äº¤äº’æ¨¡å¼: å…³é—­\n",
      "\n",
      "ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†é…ç½®:\n",
      "â€¢ ç½‘æ ¼åˆ†è¾¨ç‡: 200ç‚¹ (Individual PDP)\n",
      "â€¢ 2Däº¤äº’åˆ†è¾¨ç‡: 50Ã—50 (æ‰€æœ‰ç‰¹å¾å¯¹)\n",
      "â€¢ 3Däº¤äº’åˆ†è¾¨ç‡: 30Â³ (é‡è¦ç‰¹å¾ç»„åˆ)\n",
      "â€¢ æœ€å¤§3Däº¤äº’æ•°: 10 (è®¡ç®—æ•ˆç‡è€ƒè™‘)\n",
      "â€¢ Bootstrapæ¬¡æ•°: å°æ•°æ®é›†2000, ä¸­ç­‰1000, å¤§å‹500\n",
      "â€¢ ç½®ä¿¡æ°´å¹³: 95.0%\n",
      "â€¢ ç›´æ–¹å›¾ç®±æ•°: 50 (ç‰¹å¾åˆ†å¸ƒ)\n",
      "\n",
      "ç»Ÿä¸€å¼‚å¸¸å€¼æ£€æµ‹é…ç½®:\n",
      "â€¢ å¼‚å¸¸å€¼æ£€æµ‹: å¯ç”¨\n",
      "â€¢ ç»Ÿä¸€æ¸…æ´—: å¯ç”¨\n",
      "â€¢ è‡ªåŠ¨æ¸…æ´—: æ˜¯\n",
      "â€¢ å¤šç‰¹å¾è”åˆæ£€æµ‹: å¯ç”¨\n",
      "â€¢ å¼‚å¸¸å€¼æ¯”ä¾‹å‡è®¾: 5.0%\n",
      "â€¢ æœ€å°‘æŠ•ç¥¨æ•°: 2\n",
      "â€¢ Z-Scoreé˜ˆå€¼: 3.0\n",
      "â€¢ IQRå€æ•°: 1.5\n",
      "â€¢ é‡ç‚¹æ£€æµ‹ç‰¹å¾: Î©\n",
      "â€¢ ä¿å­˜å¼‚å¸¸å€¼æŠ¥å‘Š: æ˜¯\n",
      "â€¢ å¯¹æ¯”åˆ†æ: æ˜¯\n",
      "\n",
      "============================================================\n",
      "\n",
      "[1/1] åˆ†æGBDTæ¨¡å‹: PCC_k=5_GradientBoosting\n",
      "  ç‰¹å¾: mean_C4 enthalpy melting, solubility_limit_factor, mean_S10 Lattice Constants a, Î›, Î©\n",
      "=== åŠ è½½GBDTæ¨¡å‹å’Œæ•°æ® ===\n",
      "âœ“ æˆåŠŸåŠ è½½GBDTæ¨¡å‹: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\æœ€ä¼˜æ¨¡å‹GBDTä¸“ç”¨è®­ç»ƒ\\optimal_GBDT_model_1756353666.pkl\n",
      "âœ“ æˆåŠŸåŠ è½½æ¨¡å‹ä¿¡æ¯: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\æœ€ä¼˜æ¨¡å‹GBDTä¸“ç”¨è®­ç»ƒ\\model_info_1756353666.pkl\n",
      "âœ“ æˆåŠŸåŠ è½½åŸå§‹æ•°æ®: (221, 6)\n",
      "âœ“ æ•°æ®æ¸…ç†å®Œæˆ: (221, 5)\n",
      "âœ“ ç‰¹å¾: ['mean_C4 enthalpy melting', 'solubility_limit_factor', 'mean_S10 Lattice Constants a', 'Î›', 'Î©']\n",
      "âœ“ ç›®æ ‡å˜é‡: KQ\n",
      "âœ“ æˆåŠŸæ„é€ æ¨¡å‹ä¿¡æ¯ï¼Œå‡†å¤‡è¿›è¡ŒPDPåˆ†æ\n",
      "  æ€§èƒ½: RÂ²=0.922975, MAE=2.218439\n",
      "  æ•°æ®é›†å¤§å°: 221\n",
      "  ç½‘æ ¼åˆ†è¾¨ç‡: 200ç‚¹ (é«˜ç²¾åº¦å…‰æ»‘æ›²çº¿)\n",
      "  è‡ªé€‚åº”é…ç½®: Bootstrap 2000æ¬¡\n",
      "  ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†: å¯ç”¨\n",
      "  è¾“å‡ºç›®å½•: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\GBDTæ¨¡å‹PDPåˆ†æç»“æœ\\PCC_k5_GradientBoosting_Optimal\n",
      "\n",
      "å¼€å§‹ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†å’ŒPDPåˆ†æ...\n",
      "\n",
      "1ï¸âƒ£ ç»Ÿä¸€å¼‚å¸¸å€¼æ£€æµ‹å’Œæ•°æ®æ¸…æ´—\n",
      "\n",
      "ğŸ” å¼€å§‹å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆç»Ÿä¸€æ¸…æ´—æ¨¡å¼ï¼‰...\n",
      "  æ•°æ®é›†å¤§å°: 221 æ ·æœ¬ Ã— 5 ç‰¹å¾\n",
      "  æ­¥éª¤ 1/3: é€ç‰¹å¾å¼‚å¸¸å€¼æ£€æµ‹...\n",
      "    â­ï¸ è·³è¿‡ç‰¹å¾: mean_C4 enthalpy melting (ä¸åœ¨é‡ç‚¹æ£€æµ‹åˆ—è¡¨ä¸­)\n",
      "    â­ï¸ è·³è¿‡ç‰¹å¾: solubility_limit_factor (ä¸åœ¨é‡ç‚¹æ£€æµ‹åˆ—è¡¨ä¸­)\n",
      "    â­ï¸ è·³è¿‡ç‰¹å¾: mean_S10 Lattice Constants a (ä¸åœ¨é‡ç‚¹æ£€æµ‹åˆ—è¡¨ä¸­)\n",
      "    â­ï¸ è·³è¿‡ç‰¹å¾: Î› (ä¸åœ¨é‡ç‚¹æ£€æµ‹åˆ—è¡¨ä¸­)\n",
      "    ğŸ¯ æ£€æµ‹ç‰¹å¾: Î©\n",
      "      z_score: 4 ä¸ªå¼‚å¸¸å€¼ (1.81%)\n",
      "      iqr: 23 ä¸ªå¼‚å¸¸å€¼ (10.41%)\n",
      "      percentile_1_99: 6 ä¸ªå¼‚å¸¸å€¼ (2.71%)\n",
      "      percentile_0.5_99.5: 3 ä¸ªå¼‚å¸¸å€¼ (1.36%)\n",
      "      isolation_forest: 11 ä¸ªå¼‚å¸¸å€¼ (4.98%)\n",
      "      ğŸ” PDPè·³è·ƒæ£€æµ‹: Î©\n",
      "        æœ€å¤§æ¢¯åº¦ä½ç½®: 6.6327, æ¢¯åº¦å€¼: 13.8375\n",
      "        æ ‡è®° 1 ä¸ªè·³è·ƒé™„è¿‘çš„æ ·æœ¬\n",
      "  æ­¥éª¤ 2/3: å¤šç‰¹å¾è”åˆå¼‚å¸¸å€¼æ£€æµ‹...\n",
      "      isolation_forest_multi: 11 ä¸ªå¼‚å¸¸å€¼ (4.98%)\n",
      "      mahalanobis: 11 ä¸ªå¼‚å¸¸å€¼ (4.98%)\n",
      "  æ­¥éª¤ 3/3: åˆ›å»ºå…¨å±€å¼‚å¸¸å€¼æ©ç ...\n",
      "    âœ… å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹å®Œæˆ:\n",
      "      - æ£€æµ‹æ–¹æ³•æ€»æ•°: 8\n",
      "      - å¼‚å¸¸å€¼æ ·æœ¬æ•°: 15/221 (6.79%)\n",
      "      - æŠ•ç¥¨é˜ˆå€¼: 2\n",
      "ğŸ§¹ æ•°æ®æ¸…æ´—å®Œæˆ:\n",
      "  åŸå§‹æ ·æœ¬æ•°: 221\n",
      "  ç§»é™¤å¼‚å¸¸å€¼: 15\n",
      "  æ¸…æ´—åæ ·æœ¬æ•°: 206\n",
      "  æ•°æ®ä¿ç•™ç‡: 93.21%\n",
      "âœ… å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹æŠ¥å‘Šå·²ä¿å­˜: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\GBDTæ¨¡å‹PDPåˆ†æç»“æœ\\PCC_k5_GradientBoosting_Optimal\\outlier_detection\\Global_Outlier_Report.xlsx\n",
      "    ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†å®Œæˆï¼šç§»é™¤ 15 ä¸ªå¼‚å¸¸æ ·æœ¬\n",
      "    ç”Ÿæˆæ¸…æ´—å‰åæ•°æ®å¯¹æ¯”åˆ†æ...\n",
      "      ç”Ÿæˆæ¸…æ´—å‰åPDPå¯¹æ¯”åˆ†æ...\n",
      "        âœ“ PDPæ¸…æ´—å‰åå¯¹æ¯”åˆ†æå·²ä¿å­˜\n",
      "      ç”Ÿæˆæ¸…æ´—å‰åPDPå¯¹æ¯”åˆ†æ...\n",
      "        âœ“ PDPæ¸…æ´—å‰åå¯¹æ¯”åˆ†æå·²ä¿å­˜\n",
      "      ç”Ÿæˆæ¸…æ´—å‰åPDPå¯¹æ¯”åˆ†æ...\n",
      "        âœ“ PDPæ¸…æ´—å‰åå¯¹æ¯”åˆ†æå·²ä¿å­˜\n",
      "      ç”Ÿæˆæ¸…æ´—å‰åPDPå¯¹æ¯”åˆ†æ...\n",
      "        âœ“ PDPæ¸…æ´—å‰åå¯¹æ¯”åˆ†æå·²ä¿å­˜\n",
      "      ç”Ÿæˆæ¸…æ´—å‰åPDPå¯¹æ¯”åˆ†æ...\n",
      "        âœ“ PDPæ¸…æ´—å‰åå¯¹æ¯”åˆ†æå·²ä¿å­˜\n",
      "\n",
      "2ï¸âƒ£ ç»Ÿä¸€æ¸…æ´—æ•°æ®PDPåˆ†æï¼ˆIndividual + ç½®ä¿¡åŒºé—´ï¼‰\n",
      "  ç”Ÿæˆç»Ÿä¸€æ¸…æ´—PDPåˆ†æï¼ˆIndividual + ç½®ä¿¡åŒºé—´ï¼‰...\n",
      "    æ•°æ®é›†å¤§å°: 206 (å·²æ¸…æ´—)\n",
      "    ç½‘æ ¼åˆ†è¾¨ç‡: 200ç‚¹ (å…‰æ»‘æ›²çº¿)\n",
      "    Bootstrapæ¬¡æ•°: 2000 (å­¦æœ¯æ ‡å‡†)\n",
      "    å¤„ç†ç‰¹å¾: mean_C4 enthalpy melting\n",
      "      å¼€å§‹ 2000 æ¬¡Bootstrapé‡‡æ ·...\n",
      "        Bootstrapè¿›åº¦: 200/2000\n",
      "        Bootstrapè¿›åº¦: 400/2000\n",
      "        Bootstrapè¿›åº¦: 600/2000\n",
      "        Bootstrapè¿›åº¦: 800/2000\n",
      "        Bootstrapè¿›åº¦: 1000/2000\n",
      "        Bootstrapè¿›åº¦: 1200/2000\n",
      "        Bootstrapè¿›åº¦: 1400/2000\n",
      "        Bootstrapè¿›åº¦: 1600/2000\n",
      "        Bootstrapè¿›åº¦: 1800/2000\n",
      "        Bootstrapè¿›åº¦: 2000/2000\n",
      "      æˆåŠŸBootstrap: 2000/2000 (100.0%)\n",
      "      âœ“ ç‰¹å¾ mean_C4 enthalpy melting ç»Ÿä¸€æ¸…æ´—PDPåˆ†æå’Œå®Œæ•´æ•°æ®å·²ä¿å­˜\n",
      "    å¤„ç†ç‰¹å¾: solubility_limit_factor\n",
      "      å¼€å§‹ 2000 æ¬¡Bootstrapé‡‡æ ·...\n",
      "        Bootstrapè¿›åº¦: 200/2000\n",
      "        Bootstrapè¿›åº¦: 400/2000\n",
      "        Bootstrapè¿›åº¦: 600/2000\n",
      "        Bootstrapè¿›åº¦: 800/2000\n",
      "        Bootstrapè¿›åº¦: 1000/2000\n",
      "        Bootstrapè¿›åº¦: 1200/2000\n",
      "        Bootstrapè¿›åº¦: 1400/2000\n",
      "        Bootstrapè¿›åº¦: 1600/2000\n",
      "        Bootstrapè¿›åº¦: 1800/2000\n",
      "        Bootstrapè¿›åº¦: 2000/2000\n",
      "      æˆåŠŸBootstrap: 2000/2000 (100.0%)\n",
      "      âœ“ ç‰¹å¾ solubility_limit_factor ç»Ÿä¸€æ¸…æ´—PDPåˆ†æå’Œå®Œæ•´æ•°æ®å·²ä¿å­˜\n",
      "    å¤„ç†ç‰¹å¾: mean_S10 Lattice Constants a\n",
      "      å¼€å§‹ 2000 æ¬¡Bootstrapé‡‡æ ·...\n",
      "        Bootstrapè¿›åº¦: 200/2000\n",
      "        Bootstrapè¿›åº¦: 400/2000\n",
      "        Bootstrapè¿›åº¦: 600/2000\n",
      "        Bootstrapè¿›åº¦: 800/2000\n",
      "        Bootstrapè¿›åº¦: 1000/2000\n",
      "        Bootstrapè¿›åº¦: 1200/2000\n",
      "        Bootstrapè¿›åº¦: 1400/2000\n",
      "        Bootstrapè¿›åº¦: 1600/2000\n",
      "        Bootstrapè¿›åº¦: 1800/2000\n",
      "        Bootstrapè¿›åº¦: 2000/2000\n",
      "      æˆåŠŸBootstrap: 2000/2000 (100.0%)\n",
      "      âœ“ ç‰¹å¾ mean_S10 Lattice Constants a ç»Ÿä¸€æ¸…æ´—PDPåˆ†æå’Œå®Œæ•´æ•°æ®å·²ä¿å­˜\n",
      "    å¤„ç†ç‰¹å¾: Î›\n",
      "      å¼€å§‹ 2000 æ¬¡Bootstrapé‡‡æ ·...\n",
      "        Bootstrapè¿›åº¦: 200/2000\n",
      "        Bootstrapè¿›åº¦: 400/2000\n",
      "        Bootstrapè¿›åº¦: 600/2000\n",
      "        Bootstrapè¿›åº¦: 800/2000\n",
      "        Bootstrapè¿›åº¦: 1000/2000\n",
      "        Bootstrapè¿›åº¦: 1200/2000\n",
      "        Bootstrapè¿›åº¦: 1400/2000\n",
      "        Bootstrapè¿›åº¦: 1600/2000\n",
      "        Bootstrapè¿›åº¦: 1800/2000\n",
      "        Bootstrapè¿›åº¦: 2000/2000\n",
      "      æˆåŠŸBootstrap: 2000/2000 (100.0%)\n",
      "      âœ“ ç‰¹å¾ Î› ç»Ÿä¸€æ¸…æ´—PDPåˆ†æå’Œå®Œæ•´æ•°æ®å·²ä¿å­˜\n",
      "    å¤„ç†ç‰¹å¾: Î©\n",
      "      å¼€å§‹ 2000 æ¬¡Bootstrapé‡‡æ ·...\n",
      "        Bootstrapè¿›åº¦: 200/2000\n",
      "        Bootstrapè¿›åº¦: 400/2000\n",
      "        Bootstrapè¿›åº¦: 600/2000\n",
      "        Bootstrapè¿›åº¦: 800/2000\n",
      "        Bootstrapè¿›åº¦: 1000/2000\n",
      "        Bootstrapè¿›åº¦: 1200/2000\n",
      "        Bootstrapè¿›åº¦: 1400/2000\n",
      "        Bootstrapè¿›åº¦: 1600/2000\n",
      "        Bootstrapè¿›åº¦: 1800/2000\n",
      "        Bootstrapè¿›åº¦: 2000/2000\n",
      "      æˆåŠŸBootstrap: 2000/2000 (100.0%)\n",
      "      âœ“ ç‰¹å¾ Î© ç»Ÿä¸€æ¸…æ´—PDPåˆ†æå’Œå®Œæ•´æ•°æ®å·²ä¿å­˜\n",
      "    âœ“ æˆåŠŸå¤„ç† 5/5 ä¸ªç‰¹å¾\n",
      "    âœ“ ç»Ÿä¸€æ¸…æ´—PDPç»¼åˆå¯¹æ¯”å›¾å’Œæ•°æ®å·²ä¿å­˜\n",
      "\n",
      "3ï¸âƒ£ ç»Ÿä¸€æ¸…æ´—æ•°æ®2Dç‰¹å¾äº¤äº’PDPåˆ†æï¼ˆæ‰€æœ‰ç‰¹å¾å¯¹ï¼‰\n",
      "  ç”Ÿæˆå®Œæ•´çš„2Dç‰¹å¾äº¤äº’PDPåˆ†æï¼ˆä½¿ç”¨ç»Ÿä¸€æ¸…æ´—æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾å¯¹ï¼‰...\n",
      "    æ€»å…±éœ€è¦åˆ†æ 10 ä¸ªç‰¹å¾äº¤äº’å¯¹\n",
      "    2Dç½‘æ ¼åˆ†è¾¨ç‡: 50Ã—50\n",
      "    ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—åæ•°æ®: 206 æ ·æœ¬\n",
      "    [1/10] å¤„ç†äº¤äº’: mean_C4 enthalpy melting Ã— solubility_limit_factor\n",
      "      âœ“ 2Däº¤äº’ mean_C4 enthalpy melting Ã— solubility_limit_factor åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [2/10] å¤„ç†äº¤äº’: mean_C4 enthalpy melting Ã— mean_S10 Lattice Constants a\n",
      "      âœ“ 2Däº¤äº’ mean_C4 enthalpy melting Ã— mean_S10 Lattice Constants a åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [3/10] å¤„ç†äº¤äº’: mean_C4 enthalpy melting Ã— Î›\n",
      "      âœ“ 2Däº¤äº’ mean_C4 enthalpy melting Ã— Î› åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [4/10] å¤„ç†äº¤äº’: mean_C4 enthalpy melting Ã— Î©\n",
      "      âœ“ 2Däº¤äº’ mean_C4 enthalpy melting Ã— Î© åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [5/10] å¤„ç†äº¤äº’: solubility_limit_factor Ã— mean_S10 Lattice Constants a\n",
      "      âœ“ 2Däº¤äº’ solubility_limit_factor Ã— mean_S10 Lattice Constants a åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [6/10] å¤„ç†äº¤äº’: solubility_limit_factor Ã— Î›\n",
      "      âœ“ 2Däº¤äº’ solubility_limit_factor Ã— Î› åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [7/10] å¤„ç†äº¤äº’: solubility_limit_factor Ã— Î©\n",
      "      âœ“ 2Däº¤äº’ solubility_limit_factor Ã— Î© åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [8/10] å¤„ç†äº¤äº’: mean_S10 Lattice Constants a Ã— Î›\n",
      "      âœ“ 2Däº¤äº’ mean_S10 Lattice Constants a Ã— Î› åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [9/10] å¤„ç†äº¤äº’: mean_S10 Lattice Constants a Ã— Î©\n",
      "      âœ“ 2Däº¤äº’ mean_S10 Lattice Constants a Ã— Î© åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [10/10] å¤„ç†äº¤äº’: Î› Ã— Î©\n",
      "      âœ“ 2Däº¤äº’ Î› Ã— Î© åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    ç”Ÿæˆäº¤äº’å¼ºåº¦æ’åºæŠ¥å‘Š...\n",
      "      âœ“ äº¤äº’å¼ºåº¦æ’åºæŠ¥å‘Šå·²ä¿å­˜\n",
      "    âœ“ å®Œæ•´2Däº¤äº’åˆ†æå®Œæˆï¼šæˆåŠŸå¤„ç† 10/10 ä¸ªäº¤äº’å¯¹\n",
      "\n",
      "4ï¸âƒ£ ç»Ÿä¸€æ¸…æ´—æ•°æ®3Dç‰¹å¾äº¤äº’PDPåˆ†æï¼ˆé‡è¦ç‰¹å¾ç»„åˆï¼‰\n",
      "  ç”Ÿæˆé€‰å®šçš„3Dç‰¹å¾äº¤äº’PDPåˆ†æï¼ˆä½¿ç”¨ç»Ÿä¸€æ¸…æ´—æ•°æ®ï¼‰...\n",
      "    é€‰æ‹©äº†å‰ 5 ä¸ªæœ€é‡è¦ç‰¹å¾è¿›è¡Œ3Dåˆ†æ\n",
      "    æ€»å…±éœ€è¦åˆ†æ 10 ä¸ª3Dç‰¹å¾äº¤äº’\n",
      "    3Dç½‘æ ¼åˆ†è¾¨ç‡: 30Â³\n",
      "    ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—åæ•°æ®: 206 æ ·æœ¬\n",
      "    [1/10] å¤„ç†3Däº¤äº’: solubility_limit_factor Ã— Î› Ã— mean_S10 Lattice Constants a\n",
      "      è®¡ç®— 27000 ä¸ª3Dç½‘æ ¼ç‚¹...\n",
      "        3Dè®¡ç®—è¿›åº¦: 16.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 33.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 50.0%\n",
      "        3Dè®¡ç®—è¿›åº¦: 66.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 83.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 100.0%\n",
      "      âœ“ 3Däº¤äº’ solubility_limit_factor Ã— Î› Ã— mean_S10 Lattice Constants a åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [2/10] å¤„ç†3Däº¤äº’: solubility_limit_factor Ã— Î› Ã— mean_C4 enthalpy melting\n",
      "      è®¡ç®— 27000 ä¸ª3Dç½‘æ ¼ç‚¹...\n",
      "        3Dè®¡ç®—è¿›åº¦: 16.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 33.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 50.0%\n",
      "        3Dè®¡ç®—è¿›åº¦: 66.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 83.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 100.0%\n",
      "      âœ“ 3Däº¤äº’ solubility_limit_factor Ã— Î› Ã— mean_C4 enthalpy melting åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [3/10] å¤„ç†3Däº¤äº’: solubility_limit_factor Ã— Î› Ã— Î©\n",
      "      è®¡ç®— 27000 ä¸ª3Dç½‘æ ¼ç‚¹...\n",
      "        3Dè®¡ç®—è¿›åº¦: 16.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 33.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 50.0%\n",
      "        3Dè®¡ç®—è¿›åº¦: 66.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 83.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 100.0%\n",
      "      âœ“ 3Däº¤äº’ solubility_limit_factor Ã— Î› Ã— Î© åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [4/10] å¤„ç†3Däº¤äº’: solubility_limit_factor Ã— mean_S10 Lattice Constants a Ã— mean_C4 enthalpy melting\n",
      "      è®¡ç®— 27000 ä¸ª3Dç½‘æ ¼ç‚¹...\n",
      "        3Dè®¡ç®—è¿›åº¦: 16.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 33.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 50.0%\n",
      "        3Dè®¡ç®—è¿›åº¦: 66.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 83.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 100.0%\n",
      "      âœ“ 3Däº¤äº’ solubility_limit_factor Ã— mean_S10 Lattice Constants a Ã— mean_C4 enthalpy melting åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [5/10] å¤„ç†3Däº¤äº’: solubility_limit_factor Ã— mean_S10 Lattice Constants a Ã— Î©\n",
      "      è®¡ç®— 27000 ä¸ª3Dç½‘æ ¼ç‚¹...\n",
      "        3Dè®¡ç®—è¿›åº¦: 16.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 33.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 50.0%\n",
      "        3Dè®¡ç®—è¿›åº¦: 66.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 83.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 100.0%\n",
      "      âœ“ 3Däº¤äº’ solubility_limit_factor Ã— mean_S10 Lattice Constants a Ã— Î© åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [6/10] å¤„ç†3Däº¤äº’: solubility_limit_factor Ã— mean_C4 enthalpy melting Ã— Î©\n",
      "      è®¡ç®— 27000 ä¸ª3Dç½‘æ ¼ç‚¹...\n",
      "        3Dè®¡ç®—è¿›åº¦: 16.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 33.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 50.0%\n",
      "        3Dè®¡ç®—è¿›åº¦: 66.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 83.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 100.0%\n",
      "      âœ“ 3Däº¤äº’ solubility_limit_factor Ã— mean_C4 enthalpy melting Ã— Î© åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [7/10] å¤„ç†3Däº¤äº’: Î› Ã— mean_S10 Lattice Constants a Ã— mean_C4 enthalpy melting\n",
      "      è®¡ç®— 27000 ä¸ª3Dç½‘æ ¼ç‚¹...\n",
      "        3Dè®¡ç®—è¿›åº¦: 16.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 33.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 50.0%\n",
      "        3Dè®¡ç®—è¿›åº¦: 66.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 83.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 100.0%\n",
      "      âœ“ 3Däº¤äº’ Î› Ã— mean_S10 Lattice Constants a Ã— mean_C4 enthalpy melting åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [8/10] å¤„ç†3Däº¤äº’: Î› Ã— mean_S10 Lattice Constants a Ã— Î©\n",
      "      è®¡ç®— 27000 ä¸ª3Dç½‘æ ¼ç‚¹...\n",
      "        3Dè®¡ç®—è¿›åº¦: 16.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 33.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 50.0%\n",
      "        3Dè®¡ç®—è¿›åº¦: 66.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 83.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 100.0%\n",
      "      âœ“ 3Däº¤äº’ Î› Ã— mean_S10 Lattice Constants a Ã— Î© åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [9/10] å¤„ç†3Däº¤äº’: Î› Ã— mean_C4 enthalpy melting Ã— Î©\n",
      "      è®¡ç®— 27000 ä¸ª3Dç½‘æ ¼ç‚¹...\n",
      "        3Dè®¡ç®—è¿›åº¦: 16.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 33.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 50.0%\n",
      "        3Dè®¡ç®—è¿›åº¦: 66.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 83.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 100.0%\n",
      "      âœ“ 3Däº¤äº’ Î› Ã— mean_C4 enthalpy melting Ã— Î© åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    [10/10] å¤„ç†3Däº¤äº’: mean_S10 Lattice Constants a Ã— mean_C4 enthalpy melting Ã— Î©\n",
      "      è®¡ç®— 27000 ä¸ª3Dç½‘æ ¼ç‚¹...\n",
      "        3Dè®¡ç®—è¿›åº¦: 16.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 33.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 50.0%\n",
      "        3Dè®¡ç®—è¿›åº¦: 66.7%\n",
      "        3Dè®¡ç®—è¿›åº¦: 83.3%\n",
      "        3Dè®¡ç®—è¿›åº¦: 100.0%\n",
      "      âœ“ 3Däº¤äº’ mean_S10 Lattice Constants a Ã— mean_C4 enthalpy melting Ã— Î© åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\n",
      "    ç”Ÿæˆ3Däº¤äº’å¼ºåº¦æ’åºæŠ¥å‘Š...\n",
      "      âœ“ 3Däº¤äº’å¼ºåº¦æ’åºæŠ¥å‘Šå·²ä¿å­˜\n",
      "    âœ“ 3Däº¤äº’åˆ†æå®Œæˆï¼šæˆåŠŸå¤„ç† 10/10 ä¸ª3Däº¤äº’\n",
      "\n",
      "GBDTæ¨¡å‹ PCC_k5_GradientBoosting_Optimal ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†åˆ†æå®Œæˆï¼\n",
      "\n",
      "åˆ†æç»“æœç»Ÿè®¡:\n",
      "â€¢ åŸå§‹æ ·æœ¬æ•°: 221\n",
      "â€¢ å¼‚å¸¸å€¼æ£€æµ‹: å·²æ‰§è¡Œ\n",
      "â€¢ å¼‚å¸¸å€¼æ¸…æ´—: ç§»é™¤ 15 ä¸ªæ ·æœ¬\n",
      "â€¢ æ¸…æ´—åæ ·æœ¬æ•°: 206\n",
      "â€¢ æ•°æ®ä¿ç•™ç‡: 93.21%\n",
      "â€¢ åˆ†æç‰¹å¾æ•°: 5\n",
      "â€¢ ç½‘æ ¼åˆ†è¾¨ç‡: 200ç‚¹ (é«˜ç²¾åº¦)\n",
      "â€¢ 2Däº¤äº’åˆ†æ: 10 å¯¹ (ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—æ•°æ®)\n",
      "â€¢ 3Däº¤äº’åˆ†æ: 10 ç»„ (ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—æ•°æ®)\n",
      "â€¢ æ•°æ®ä¸€è‡´æ€§: æ‰€æœ‰åˆ†æä½¿ç”¨ç›¸åŒçš„æ¸…æ´—åæ•°æ®\n",
      "â€¢ æ¸…æ´—å‰åå¯¹æ¯”: å·²ç”Ÿæˆ\n",
      "â€¢ å…¨å±€å¼‚å¸¸å€¼æŠ¥å‘Š: å·²ç”Ÿæˆ\n",
      "\n",
      "================================================================================\n",
      "GBDTæ¨¡å‹PDPåˆ†æå®Œæˆï¼\n",
      "================================================================================\n",
      "è¾“å‡ºç›®å½•: D:\\åšä¸€ä¸‹\\manuscript5-å˜Siç»„\\GBDTæ¨¡å‹PDPåˆ†æç»“æœ\n",
      "\n",
      "è¯¦ç»†ç»“æœä¿å­˜åœ¨ä»¥ä¸‹ç›®å½•ä¸­:\n",
      "  â€¢ combined_pdp/ (ç»Ÿä¸€æ¸…æ´—PDPåˆ†æ)\n",
      "  â€¢ interaction_2d_pdp/ (ç»Ÿä¸€æ¸…æ´—2Däº¤äº’åˆ†æ)\n",
      "  â€¢ interaction_3d_pdp/ (ç»Ÿä¸€æ¸…æ´—3Däº¤äº’åˆ†æ)\n",
      "  â€¢ outlier_detection/ (å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹æŠ¥å‘Š)\n",
      "  â€¢ cleaned_analysis/ (æ¸…æ´—å‰åå¯¹æ¯”)\n",
      "\n",
      "ç”Ÿæˆçš„ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†åˆ†æç±»å‹:\n",
      "  â€¢ ç»Ÿä¸€æ¸…æ´—Individual PDPå›¾ï¼š200ç½‘æ ¼ç‚¹ + Excelæ•°æ®\n",
      "  â€¢ ç»Ÿä¸€æ¸…æ´—2Däº¤äº’å›¾ï¼š50Ã—50 æ‰€æœ‰ç‰¹å¾å¯¹ + Excelæ•°æ®\n",
      "  â€¢ ç»Ÿä¸€æ¸…æ´—3Däº¤äº’å›¾ï¼š30Â³ é‡è¦ç‰¹å¾ç»„åˆ + åˆ‡ç‰‡å¯è§†åŒ– + Excelæ•°æ®\n",
      "  â€¢ å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹æŠ¥å‘Šï¼šå¤šç§æ£€æµ‹æ–¹æ³• + å¤šç‰¹å¾è”åˆ + æŠ•ç¥¨æœºåˆ¶ + Excelæ•°æ®\n",
      "  â€¢ æ¸…æ´—å‰åå¯¹æ¯”ï¼šPDPæ›²çº¿+åˆ†å¸ƒ+æ–œç‡+ç»Ÿè®¡å¯¹æ¯” + Excelæ•°æ®\n",
      "  â€¢ äº¤äº’å¼ºåº¦æ’åºï¼š2Då’Œ3Däº¤äº’çš„å¼ºåº¦åˆ†æå’Œæ’åºï¼ˆåŸºäºç»Ÿä¸€æ¸…æ´—æ•°æ®ï¼‰\n",
      "\n",
      "ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†æ ¸å¿ƒæ”¹è¿›:\n",
      "  â€¢ å…¨å±€ç»Ÿä¸€å¼‚å¸¸å€¼æ£€æµ‹ (ä¸€æ¬¡æ£€æµ‹ï¼Œå…¨å±€åº”ç”¨)\n",
      "  â€¢ å¤šç‰¹å¾è”åˆå¼‚å¸¸å€¼æ£€æµ‹ (é©¬æ°è·ç¦»+å¤šç»´IsolationForest)\n",
      "  â€¢ æ™ºèƒ½æŠ•ç¥¨æœºåˆ¶ (å¤šç§æ£€æµ‹æ–¹æ³•ç»¼åˆå†³ç­–)\n",
      "  â€¢ æ•°æ®ä¸€è‡´æ€§ä¿è¯ (æ‰€æœ‰PDPå’Œäº¤äº’åˆ†æä½¿ç”¨ç›¸åŒçš„æ¸…æ´æ•°æ®)\n",
      "  â€¢ é’ˆå¯¹æ€§Î©ç‰¹å¾å¤„ç† (é‡ç‚¹æ£€æµ‹åˆ—è¡¨å¯é…ç½®)\n",
      "  â€¢ PDPè·³è·ƒä¸“é¡¹æ£€æµ‹ (ä¸“é—¨è§£å†³å¼‚å¸¸è·³è·ƒé—®é¢˜)\n",
      "  â€¢ å®Œæ•´å¯è§†åŒ–æŠ¥å‘Š (å¼‚å¸¸å€¼åˆ†å¸ƒ+æŠ•ç¥¨+æ¸…æ´—æ•ˆæœ)\n",
      "  â€¢ æ¸…æ´—å‰åå®Œæ•´å¯¹æ¯” (PDPæ›²çº¿+åˆ†å¸ƒ+æ–œç‡+ç»Ÿè®¡å¯¹æ¯”)\n",
      "\n",
      "é’ˆå¯¹Î©ç‰¹å¾å¼‚å¸¸è·³è·ƒé—®é¢˜çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ:\n",
      "  â€¢ Î©ç‰¹å¾å·²åŒ…å«åœ¨ç»Ÿä¸€å¼‚å¸¸å€¼æ£€æµ‹èŒƒå›´å†…\n",
      "  â€¢ 6ç§å•ç‰¹å¾+2ç§å¤šç‰¹å¾æ–¹æ³•ä¼šç»¼åˆè¯†åˆ«Î©çš„å¼‚å¸¸å€¼\n",
      "  â€¢ PDPè·³è·ƒä¸“é¡¹æ£€æµ‹ä¼šæ‰¾åˆ°å¯¼è‡´è·³è·ƒçš„å…·ä½“æ•°æ®ç‚¹\n",
      "  â€¢ ç»Ÿä¸€æ¸…æ´—ä¼šåœ¨æ‰€æœ‰åˆ†æä¸­ç§»é™¤å¼‚å¸¸æ•°æ®ç‚¹\n",
      "  â€¢ æ¸…æ´—å‰åå¯¹æ¯”ä¼šæ˜¾ç¤ºÎ©ç‰¹å¾çš„æ”¹è¿›æ•ˆæœ\n",
      "  â€¢ å…¨å±€å¼‚å¸¸å€¼æŠ¥å‘Šä¼šè¯¦ç»†è®°å½•Î©ç‰¹å¾çš„æ£€æµ‹ç»“æœ\n",
      "  â€¢ æ‰€æœ‰PDPå’Œäº¤äº’åˆ†æéƒ½å°†ä½¿ç”¨æ¸…æ´—åçš„Î©ç‰¹å¾æ•°æ®\n",
      "\n",
      "æ‰€æœ‰èµ„æºå·²æ¸…ç†ï¼Œç¨‹åºæ‰§è¡Œå®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# é¦–å…ˆè®¾ç½®matplotlibä¸ºéäº¤äº’æ¨¡å¼ï¼Œç¡®ä¿ä¸æ˜¾ç¤ºä»»ä½•å›¾è¡¨\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # ä½¿ç”¨éGUIåç«¯ï¼Œç¡®ä¿ä¸æ˜¾ç¤ºå›¾è¡¨\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()  # å…³é—­äº¤äº’æ¨¡å¼\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®matplotlibä¸ºéäº¤äº’æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºå›¾è¡¨\n",
    "plt.ioff()  # å…³é—­äº¤äº’æ¨¡å¼\n",
    "matplotlib.use('Agg')  # ä½¿ç”¨éGUIåç«¯\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œç»˜å›¾é£æ ¼\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['figure.max_open_warning'] = 0  # ç¦ç”¨å›¾å½¢æ•°é‡è­¦å‘Š\n",
    "plt.rcParams['interactive'] = False  # ç¦ç”¨äº¤äº’æ¨¡å¼\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# é…ç½®å‚æ•° - ä¿®æ”¹ä¸ºGBDTæ¨¡å‹ä¸“ç”¨é…ç½®\n",
    "CONFIG = {\n",
    "    # ä¿®æ”¹ï¼šæŒ‡å‘GBDTæ¨¡å‹çš„è¾“å‡ºç›®å½•\n",
    "    'gbdt_model_dir': \"D:\\\\åšä¸€ä¸‹\\\\manuscript5-å˜Siç»„\\\\æœ€ä¼˜æ¨¡å‹GBDTä¸“ç”¨è®­ç»ƒ\",\n",
    "    'data_file': \"D:\\\\åšä¸€ä¸‹\\\\Nb-Siæ•°æ®åº“8.28ç‰¹å¾å€¼.xlsx\",\n",
    "    'pdp_output_dir': \"D:\\\\åšä¸€ä¸‹\\\\manuscript5-å˜Siç»„\\\\GBDTæ¨¡å‹PDPåˆ†æç»“æœ\",\n",
    "    \n",
    "    # GBDTæ¨¡å‹ç›¸å…³ä¿¡æ¯\n",
    "    'selected_features': [\n",
    "        'mean_C4 enthalpy melting', \n",
    "        'solubility_limit_factor', \n",
    "        'mean_S10 Lattice Constants a', \n",
    "        'Î›', \n",
    "        'Î©'\n",
    "    ],\n",
    "    'target_col': 'KQ',\n",
    "    'model_timestamp': '1756353666',  # ä¿®æ”¹ï¼šä½¿ç”¨ä½ çš„æ¨¡å‹æ—¶é—´æˆ³\n",
    "    'random_state': 2023,\n",
    "    \n",
    "    'grid_resolution': 200,  # ä»100æ”¹ä¸º200ï¼Œæ›²çº¿æ›´å…‰æ»‘\n",
    "    'percentiles': (0.01, 0.99),  # PDPè®¡ç®—çš„ç™¾åˆ†ä½æ•°èŒƒå›´\n",
    "    'figure_size': (12, 8),  # å›¾å½¢å°ºå¯¸\n",
    "    'dpi': 300,  # å›¾åƒåˆ†è¾¨ç‡\n",
    "    \n",
    "    # å­¦æœ¯æ ‡å‡†çš„Bootstrapè®¾ç½®\n",
    "    'bootstrap_samples': 1000,  # æå‡åˆ°1000æ¬¡ï¼ˆç¬¦åˆå­¦æœ¯æ ‡å‡†ï¼‰\n",
    "    'bootstrap_samples_small': 2000,  # å°æ•°æ®é›†ä½¿ç”¨æ›´å¤šæ¬¡æ•°\n",
    "    'bootstrap_samples_large': 500,   # å¤§æ•°æ®é›†å¯ä»¥å‡å°‘æ¬¡æ•°\n",
    "    \n",
    "    'confidence_level': 0.95,   # ç½®ä¿¡æ°´å¹³\n",
    "    'small_dataset_threshold': 1000,   # å°æ•°æ®é›†é˜ˆå€¼\n",
    "    'large_dataset_threshold': 10000,  # å¤§æ•°æ®é›†é˜ˆå€¼\n",
    "    \n",
    "    # æ–°å¢ï¼šæŸ±çŠ¶å›¾é…ç½®\n",
    "    'histogram_bins': 50,  # æŸ±çŠ¶å›¾ç®±æ•°\n",
    "    \n",
    "    # äº¤äº’åˆ†æé…ç½®\n",
    "    'interaction_2d_resolution': 50,  # 2Däº¤äº’åˆ†æç½‘æ ¼åˆ†è¾¨ç‡\n",
    "    'interaction_3d_resolution': 30,  # 3Däº¤äº’åˆ†æç½‘æ ¼åˆ†è¾¨ç‡\n",
    "    'max_3d_interactions': 10,  # æœ€å¤§3Däº¤äº’åˆ†ææ•°é‡\n",
    "    \n",
    "    # å¢å¼ºï¼šå¼‚å¸¸å€¼æ£€æµ‹é…ç½®\n",
    "    'outlier_detection': {\n",
    "        'enable': True,  # æ˜¯å¦å¯ç”¨å¼‚å¸¸å€¼æ£€æµ‹\n",
    "        'auto_clean': True,  # æ˜¯å¦è‡ªåŠ¨æ¸…æ´—å¼‚å¸¸å€¼\n",
    "        'contamination': 0.05,  # å¼‚å¸¸å€¼æ¯”ä¾‹å‡è®¾ (5%)\n",
    "        'min_votes': 2,  # å¼‚å¸¸å€¼æ£€æµ‹æœ€å°‘æŠ•ç¥¨æ•°\n",
    "        'z_score_threshold': 3.0,  # Z-Scoreé˜ˆå€¼\n",
    "        'iqr_multiplier': 1.5,  # IQRå€æ•°\n",
    "        'target_features': ['Î©'],  # é‡ç‚¹æ£€æµ‹çš„ç‰¹å¾ï¼ˆå¯ä»¥ä¸ºç©ºè¡¨ç¤ºæ£€æµ‹æ‰€æœ‰ç‰¹å¾ï¼‰\n",
    "        'save_outlier_reports': True,  # æ˜¯å¦ä¿å­˜å¼‚å¸¸å€¼æ£€æµ‹æŠ¥å‘Š\n",
    "        'comparison_analysis': True,  # æ˜¯å¦è¿›è¡Œæ¸…æ´—å‰åå¯¹æ¯”åˆ†æ\n",
    "        'unified_cleaning': True,  # æ˜¯å¦ç»Ÿä¸€æ¸…æ´—ï¼ˆç¡®ä¿æ‰€æœ‰åˆ†æä½¿ç”¨ç›¸åŒçš„æ¸…æ´—åæ•°æ®ï¼‰\n",
    "        'multi_feature_detection': True,  # æ˜¯å¦å¯ç”¨å¤šç‰¹å¾è”åˆå¼‚å¸¸å€¼æ£€æµ‹\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_adaptive_config(n_samples):\n",
    "    \"\"\"æ ¹æ®æ•°æ®é›†å¤§å°è‡ªé€‚åº”è°ƒæ•´å‚æ•°\"\"\"\n",
    "    if n_samples <= CONFIG['small_dataset_threshold']:\n",
    "        return {\n",
    "            'bootstrap_samples': CONFIG['bootstrap_samples_small'],\n",
    "            'description': 'å°æ•°æ®é›†-é«˜ç²¾åº¦Bootstrap'\n",
    "        }\n",
    "    elif n_samples <= CONFIG['large_dataset_threshold']:\n",
    "        return {\n",
    "            'bootstrap_samples': CONFIG['bootstrap_samples'],\n",
    "            'description': 'ä¸­ç­‰æ•°æ®é›†-æ ‡å‡†Bootstrap'\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'bootstrap_samples': CONFIG['bootstrap_samples_large'],\n",
    "            'description': 'å¤§æ•°æ®é›†-é«˜æ•ˆBootstrap'\n",
    "        }\n",
    "\n",
    "def cleanup_matplotlib():\n",
    "    \"\"\"æ¸…ç†matplotlibèµ„æº\"\"\"\n",
    "    plt.close('all')\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "def calculate_histogram_data(feature_data, bins=None):\n",
    "    \"\"\"è®¡ç®—ç‰¹å¾æ•°æ®çš„ç›´æ–¹å›¾åˆ†å¸ƒæ•°æ®\"\"\"\n",
    "    if bins is None:\n",
    "        bins = CONFIG['histogram_bins']\n",
    "    \n",
    "    # è®¡ç®—ç›´æ–¹å›¾æ•°æ®\n",
    "    counts, bin_edges = np.histogram(feature_data, bins=bins)\n",
    "    \n",
    "    # è®¡ç®—ç®±ä¸­å¿ƒç‚¹\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # è®¡ç®—å¯†åº¦ï¼ˆå½’ä¸€åŒ–é¢‘ç‡ï¼‰\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "    densities = counts / (len(feature_data) * bin_width)\n",
    "    \n",
    "    # è®¡ç®—ç›¸å¯¹é¢‘ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰\n",
    "    relative_frequencies = counts / len(feature_data) * 100\n",
    "    \n",
    "    return {\n",
    "        'bin_centers': bin_centers,\n",
    "        'bin_edges': bin_edges,\n",
    "        'counts': counts,\n",
    "        'densities': densities,\n",
    "        'relative_frequencies': relative_frequencies,\n",
    "        'bin_width': bin_width,\n",
    "        'total_samples': len(feature_data)\n",
    "    }\n",
    "\n",
    "class UnifiedOutlierDetector:\n",
    "    \"\"\"ç»Ÿä¸€å¼‚å¸¸å€¼æ£€æµ‹å™¨ç±» - ç”¨äºå…¨å±€æ•°æ®æ¸…æ´—\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.global_outlier_mask = None\n",
    "        self.global_outlier_report = {}\n",
    "        \n",
    "    def detect_global_outliers(self, X_data, y_data, features, model=None):\n",
    "        \"\"\"å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹ - ç»Ÿä¸€å¤„ç†æ‰€æœ‰ç‰¹å¾\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ” å¼€å§‹å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆç»Ÿä¸€æ¸…æ´—æ¨¡å¼ï¼‰...\")\n",
    "        print(f\"  æ•°æ®é›†å¤§å°: {X_data.shape[0]} æ ·æœ¬ Ã— {X_data.shape[1]} ç‰¹å¾\")\n",
    "        \n",
    "        n_samples, n_features = X_data.shape\n",
    "        all_outlier_methods = {}\n",
    "        feature_outlier_reports = {}\n",
    "        \n",
    "        # 1. é€ç‰¹å¾å¼‚å¸¸å€¼æ£€æµ‹\n",
    "        print(f\"  æ­¥éª¤ 1/3: é€ç‰¹å¾å¼‚å¸¸å€¼æ£€æµ‹...\")\n",
    "        for i, feature in enumerate(features):\n",
    "            feature_data = X_data[:, i]\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦ä¸ºé‡ç‚¹æ£€æµ‹ç‰¹å¾\n",
    "            target_features = self.config.get('target_features', [])\n",
    "            should_detect = (not target_features) or (feature in target_features)\n",
    "            \n",
    "            if should_detect:\n",
    "                print(f\"    ğŸ¯ æ£€æµ‹ç‰¹å¾: {feature}\")\n",
    "                outlier_methods = self._detect_single_feature_outliers(feature_data, feature)\n",
    "                \n",
    "                # æ·»åŠ PDPè·³è·ƒæ£€æµ‹ï¼ˆå¦‚æœæä¾›äº†æ¨¡å‹ï¼‰\n",
    "                if model is not None:\n",
    "                    jump_outliers = self._detect_pdp_jump_outliers(model, X_data, i, feature_data, feature)\n",
    "                    if np.sum(jump_outliers) > 0:\n",
    "                        outlier_methods['pdp_jump'] = jump_outliers\n",
    "                \n",
    "                # ä¿å­˜å•ç‰¹å¾æ£€æµ‹ç»“æœ\n",
    "                for method, mask in outlier_methods.items():\n",
    "                    method_key = f\"{feature}_{method}\"\n",
    "                    all_outlier_methods[method_key] = mask\n",
    "                \n",
    "                feature_outlier_reports[feature] = outlier_methods\n",
    "            else:\n",
    "                print(f\"    â­ï¸ è·³è¿‡ç‰¹å¾: {feature} (ä¸åœ¨é‡ç‚¹æ£€æµ‹åˆ—è¡¨ä¸­)\")\n",
    "        \n",
    "        # 2. å¤šç‰¹å¾è”åˆå¼‚å¸¸å€¼æ£€æµ‹\n",
    "        if self.config.get('multi_feature_detection', False):\n",
    "            print(f\"  æ­¥éª¤ 2/3: å¤šç‰¹å¾è”åˆå¼‚å¸¸å€¼æ£€æµ‹...\")\n",
    "            multi_feature_outliers = self._detect_multi_feature_outliers(X_data, features)\n",
    "            for method, mask in multi_feature_outliers.items():\n",
    "                all_outlier_methods[f\"multi_{method}\"] = mask\n",
    "        else:\n",
    "            print(f\"  æ­¥éª¤ 2/3: è·³è¿‡å¤šç‰¹å¾æ£€æµ‹ (å·²ç¦ç”¨)\")\n",
    "        \n",
    "        # 3. åˆ›å»ºå…¨å±€å¼‚å¸¸å€¼æ©ç \n",
    "        print(f\"  æ­¥éª¤ 3/3: åˆ›å»ºå…¨å±€å¼‚å¸¸å€¼æ©ç ...\")\n",
    "        if all_outlier_methods:\n",
    "            # è®¡ç®—æ¯ä¸ªæ ·æœ¬è¢«æ ‡è®°ä¸ºå¼‚å¸¸å€¼çš„æ¬¡æ•°ï¼ˆè·¨æ‰€æœ‰æ–¹æ³•å’Œç‰¹å¾ï¼‰\n",
    "            method_names = list(all_outlier_methods.keys())\n",
    "            vote_matrix = np.column_stack([all_outlier_methods[method] for method in method_names])\n",
    "            vote_counts = np.sum(vote_matrix, axis=1)\n",
    "            \n",
    "            # ä½¿ç”¨æŠ•ç¥¨æœºåˆ¶ç¡®å®šæœ€ç»ˆå¼‚å¸¸å€¼\n",
    "            self.global_outlier_mask = vote_counts >= self.config['min_votes']\n",
    "            outlier_count = np.sum(self.global_outlier_mask)\n",
    "            \n",
    "            print(f\"    âœ… å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹å®Œæˆ:\")\n",
    "            print(f\"      - æ£€æµ‹æ–¹æ³•æ€»æ•°: {len(all_outlier_methods)}\")\n",
    "            print(f\"      - å¼‚å¸¸å€¼æ ·æœ¬æ•°: {outlier_count}/{n_samples} ({outlier_count/n_samples*100:.2f}%)\")\n",
    "            print(f\"      - æŠ•ç¥¨é˜ˆå€¼: {self.config['min_votes']}\")\n",
    "            \n",
    "            # ä¿å­˜å…¨å±€å¼‚å¸¸å€¼æŠ¥å‘Š\n",
    "            self.global_outlier_report = {\n",
    "                'all_methods': all_outlier_methods,\n",
    "                'feature_reports': feature_outlier_reports,\n",
    "                'vote_counts': vote_counts,\n",
    "                'vote_matrix': vote_matrix,\n",
    "                'method_names': method_names,\n",
    "                'outlier_mask': self.global_outlier_mask,\n",
    "                'outlier_count': outlier_count,\n",
    "                'outlier_percentage': outlier_count/n_samples*100\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            print(f\"    âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•å¼‚å¸¸å€¼æ–¹æ³•ç»“æœ\")\n",
    "            self.global_outlier_mask = np.zeros(n_samples, dtype=bool)\n",
    "            self.global_outlier_report = {}\n",
    "        \n",
    "        return self.global_outlier_mask, self.global_outlier_report\n",
    "    \n",
    "    def _detect_single_feature_outliers(self, feature_data, feature_name):\n",
    "        \"\"\"å•ç‰¹å¾å¼‚å¸¸å€¼æ£€æµ‹\"\"\"\n",
    "        \n",
    "        outlier_methods = {}\n",
    "        n_samples = len(feature_data)\n",
    "        \n",
    "        # æ–¹æ³•1: Z-Scoreæ–¹æ³•\n",
    "        z_scores = np.abs(stats.zscore(feature_data))\n",
    "        outlier_methods['z_score'] = z_scores > self.config['z_score_threshold']\n",
    "        \n",
    "        # æ–¹æ³•2: æ”¹è¿›çš„IQRæ–¹æ³•\n",
    "        Q1 = np.percentile(feature_data, 25)\n",
    "        Q3 = np.percentile(feature_data, 75)\n",
    "        IQR = Q3 - Q1\n",
    "        iqr_lower = Q1 - self.config['iqr_multiplier'] * IQR\n",
    "        iqr_upper = Q3 + self.config['iqr_multiplier'] * IQR\n",
    "        outlier_methods['iqr'] = (feature_data < iqr_lower) | (feature_data > iqr_upper)\n",
    "        \n",
    "        # æ–¹æ³•3: åŸºäºåˆ†ä½æ•°çš„æ–¹æ³•\n",
    "        p1, p99 = np.percentile(feature_data, [1, 99])\n",
    "        p0_5, p99_5 = np.percentile(feature_data, [0.5, 99.5])\n",
    "        outlier_methods['percentile_1_99'] = (feature_data < p1) | (feature_data > p99)\n",
    "        outlier_methods['percentile_0.5_99.5'] = (feature_data < p0_5) | (feature_data > p99_5)\n",
    "        \n",
    "        # æ–¹æ³•4: Isolation Forest\n",
    "        try:\n",
    "            iso_forest = IsolationForest(\n",
    "                contamination=self.config['contamination'], \n",
    "                random_state=42\n",
    "            )\n",
    "            outlier_methods['isolation_forest'] = iso_forest.fit_predict(feature_data.reshape(-1, 1)) == -1\n",
    "        except:\n",
    "            outlier_methods['isolation_forest'] = np.zeros(n_samples, dtype=bool)\n",
    "        \n",
    "        # ç»Ÿè®¡å„æ–¹æ³•ç»“æœ\n",
    "        for method, mask in outlier_methods.items():\n",
    "            count = np.sum(mask)\n",
    "            percentage = count / n_samples * 100\n",
    "            print(f\"      {method}: {count} ä¸ªå¼‚å¸¸å€¼ ({percentage:.2f}%)\")\n",
    "        \n",
    "        return outlier_methods\n",
    "    \n",
    "    def _detect_multi_feature_outliers(self, X_data, features):\n",
    "        \"\"\"å¤šç‰¹å¾è”åˆå¼‚å¸¸å€¼æ£€æµ‹\"\"\"\n",
    "        \n",
    "        multi_outlier_methods = {}\n",
    "        n_samples = X_data.shape[0]\n",
    "        \n",
    "        try:\n",
    "            # å¤šç»´Isolation Forest\n",
    "            iso_forest_multi = IsolationForest(\n",
    "                contamination=self.config['contamination'],\n",
    "                random_state=42\n",
    "            )\n",
    "            multi_outlier_methods['isolation_forest_multi'] = iso_forest_multi.fit_predict(X_data) == -1\n",
    "            \n",
    "            # é©¬æ°è·ç¦»å¼‚å¸¸å€¼æ£€æµ‹\n",
    "            try:\n",
    "                from scipy.spatial.distance import mahalanobis\n",
    "                mean = np.mean(X_data, axis=0)\n",
    "                cov = np.cov(X_data.T)\n",
    "                cov_inv = np.linalg.pinv(cov)  # ä½¿ç”¨ä¼ªé€†å¤„ç†å¥‡å¼‚çŸ©é˜µ\n",
    "                \n",
    "                mahal_distances = []\n",
    "                for i in range(n_samples):\n",
    "                    try:\n",
    "                        distance = mahalanobis(X_data[i], mean, cov_inv)\n",
    "                        mahal_distances.append(distance)\n",
    "                    except:\n",
    "                        mahal_distances.append(np.nan)\n",
    "                \n",
    "                mahal_distances = np.array(mahal_distances)\n",
    "                # ç§»é™¤NaNå€¼\n",
    "                valid_distances = mahal_distances[~np.isnan(mahal_distances)]\n",
    "                if len(valid_distances) > 0:\n",
    "                    threshold = np.percentile(valid_distances, 95)  # 95%åˆ†ä½æ•°\n",
    "                    multi_outlier_methods['mahalanobis'] = (mahal_distances > threshold) & (~np.isnan(mahal_distances))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      âš ï¸ é©¬æ°è·ç¦»è®¡ç®—å¤±è´¥: {str(e)}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"      âš ï¸ å¤šç‰¹å¾å¼‚å¸¸å€¼æ£€æµ‹å¤±è´¥: {str(e)}\")\n",
    "        \n",
    "        # ç»Ÿè®¡å¤šç‰¹å¾æ£€æµ‹ç»“æœ\n",
    "        for method, mask in multi_outlier_methods.items():\n",
    "            count = np.sum(mask)\n",
    "            percentage = count / n_samples * 100\n",
    "            print(f\"      {method}: {count} ä¸ªå¼‚å¸¸å€¼ ({percentage:.2f}%)\")\n",
    "        \n",
    "        return multi_outlier_methods\n",
    "    \n",
    "    def _detect_pdp_jump_outliers(self, model, X_data, feature_idx, feature_data, feature_name):\n",
    "        \"\"\"æ£€æµ‹å¯¼è‡´PDPå¼‚å¸¸è·³è·ƒçš„æ•°æ®ç‚¹\"\"\"\n",
    "        \n",
    "        print(f\"      ğŸ” PDPè·³è·ƒæ£€æµ‹: {feature_name}\")\n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®—ç²—ç•¥PDPæ¥æ‰¾åˆ°å¼‚å¸¸è·³è·ƒç‚¹\n",
    "            temp_grid = np.linspace(np.min(feature_data), np.max(feature_data), 50)\n",
    "            temp_pdp = []\n",
    "            \n",
    "            for grid_val in temp_grid:\n",
    "                X_modified = X_data.copy()\n",
    "                X_modified[:, feature_idx] = grid_val\n",
    "                temp_pred = model.predict(X_modified)\n",
    "                temp_pdp.append(np.mean(temp_pred))\n",
    "            \n",
    "            temp_pdp = np.array(temp_pdp)\n",
    "            \n",
    "            # è®¡ç®—PDPçš„æ¢¯åº¦æ¥æ‰¾åˆ°å¼‚å¸¸è·³è·ƒ\n",
    "            if len(temp_grid) > 1:\n",
    "                temp_gradient = np.gradient(temp_pdp, temp_grid)\n",
    "                \n",
    "                # æ‰¾åˆ°æ¢¯åº¦ç»å¯¹å€¼æœ€å¤§çš„ç‚¹\n",
    "                max_gradient_idx = np.argmax(np.abs(temp_gradient))\n",
    "                problem_value = temp_grid[max_gradient_idx]\n",
    "                max_gradient = temp_gradient[max_gradient_idx]\n",
    "                \n",
    "                print(f\"        æœ€å¤§æ¢¯åº¦ä½ç½®: {problem_value:.4f}, æ¢¯åº¦å€¼: {max_gradient:.4f}\")\n",
    "                \n",
    "                # å¦‚æœæ¢¯åº¦è¿‡å¤§ï¼Œæ ‡è®°é™„è¿‘çš„æ ·æœ¬ä¸ºæ½œåœ¨å¼‚å¸¸å€¼\n",
    "                if np.abs(max_gradient) > np.std(temp_gradient) * 3:\n",
    "                    tolerance = np.std(feature_data) * 0.1  # ä½¿ç”¨ç‰¹å¾æ ‡å‡†å·®çš„10%ä½œä¸ºå®¹å¿åº¦\n",
    "                    jump_outliers = np.abs(feature_data - problem_value) < tolerance\n",
    "                    print(f\"        æ ‡è®° {np.sum(jump_outliers)} ä¸ªè·³è·ƒé™„è¿‘çš„æ ·æœ¬\")\n",
    "                    return jump_outliers\n",
    "            \n",
    "            return np.zeros(len(feature_data), dtype=bool)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"        PDPè·³è·ƒæ£€æµ‹å¤±è´¥: {str(e)}\")\n",
    "            return np.zeros(len(feature_data), dtype=bool)\n",
    "    \n",
    "    def apply_cleaning(self, X_data, y_data):\n",
    "        \"\"\"åº”ç”¨æ•°æ®æ¸…æ´—\"\"\"\n",
    "        \n",
    "        if self.global_outlier_mask is None:\n",
    "            print(\"âš ï¸ å°šæœªæ‰§è¡Œå¼‚å¸¸å€¼æ£€æµ‹ï¼Œè¿”å›åŸå§‹æ•°æ®\")\n",
    "            return X_data, y_data, np.array([])\n",
    "        \n",
    "        if not self.config.get('auto_clean', False):\n",
    "            print(\"âš ï¸ è‡ªåŠ¨æ¸…æ´—å·²ç¦ç”¨ï¼Œè¿”å›åŸå§‹æ•°æ®\")\n",
    "            return X_data, y_data, np.array([])\n",
    "        \n",
    "        outlier_count = np.sum(self.global_outlier_mask)\n",
    "        if outlier_count == 0:\n",
    "            print(\"âœ… æœªæ£€æµ‹åˆ°å¼‚å¸¸å€¼ï¼Œè¿”å›åŸå§‹æ•°æ®\")\n",
    "            return X_data, y_data, np.array([])\n",
    "        \n",
    "        # åˆ›å»ºæ¸…æ´—åçš„æ•°æ®\n",
    "        clean_mask = ~self.global_outlier_mask\n",
    "        X_cleaned = X_data[clean_mask]\n",
    "        y_cleaned = y_data[clean_mask]\n",
    "        removed_indices = np.where(self.global_outlier_mask)[0]\n",
    "        \n",
    "        print(f\"ğŸ§¹ æ•°æ®æ¸…æ´—å®Œæˆ:\")\n",
    "        print(f\"  åŸå§‹æ ·æœ¬æ•°: {len(X_data)}\")\n",
    "        print(f\"  ç§»é™¤å¼‚å¸¸å€¼: {outlier_count}\")\n",
    "        print(f\"  æ¸…æ´—åæ ·æœ¬æ•°: {len(X_cleaned)}\")\n",
    "        print(f\"  æ•°æ®ä¿ç•™ç‡: {len(X_cleaned)/len(X_data)*100:.2f}%\")\n",
    "        \n",
    "        return X_cleaned, y_cleaned, removed_indices\n",
    "    \n",
    "    def save_global_outlier_report(self, model_dir, features):\n",
    "        \"\"\"ä¿å­˜å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹æŠ¥å‘Š\"\"\"\n",
    "        \n",
    "        if not self.global_outlier_report:\n",
    "            print(\"âš ï¸ æ²¡æœ‰å…¨å±€å¼‚å¸¸å€¼æŠ¥å‘Šå¯ä¿å­˜\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            report = self.global_outlier_report\n",
    "            \n",
    "            # ä¿å­˜å…¨å±€å¼‚å¸¸å€¼æ±‡æ€»\n",
    "            global_summary = []\n",
    "            for i, feature in enumerate(features):\n",
    "                outlier_count = np.sum(report['outlier_mask'])\n",
    "                global_summary.append({\n",
    "                    'ç‰¹å¾': feature,\n",
    "                    'æ˜¯å¦å¼‚å¸¸å€¼': report['outlier_mask'],\n",
    "                    'æŠ•ç¥¨è®¡æ•°': report['vote_counts'],\n",
    "                    'å¼‚å¸¸å€¼æ€»æ•°': outlier_count,\n",
    "                    'å¼‚å¸¸å€¼æ¯”ä¾‹': outlier_count / len(report['vote_counts']) * 100\n",
    "                })\n",
    "            \n",
    "            # ä¿å­˜è¯¦ç»†çš„æ–¹æ³•ç»“æœ\n",
    "            methods_results = {}\n",
    "            for method_name, mask in report['all_methods'].items():\n",
    "                methods_results[method_name] = mask\n",
    "            \n",
    "            # ä¿å­˜åˆ°Excel\n",
    "            excel_path = os.path.join(model_dir, 'outlier_detection', 'Global_Outlier_Report.xlsx')\n",
    "            \n",
    "            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "                # å…¨å±€æ±‡æ€»\n",
    "                global_df = pd.DataFrame({\n",
    "                    'sample_index': np.arange(len(report['vote_counts'])),\n",
    "                    'is_global_outlier': report['outlier_mask'],\n",
    "                    'vote_counts': report['vote_counts']\n",
    "                })\n",
    "                global_df.to_excel(writer, sheet_name='Global_Summary', index=False)\n",
    "                \n",
    "                # å„æ–¹æ³•è¯¦ç»†ç»“æœ\n",
    "                methods_df = pd.DataFrame(methods_results)\n",
    "                methods_df.insert(0, 'sample_index', np.arange(len(report['vote_counts'])))\n",
    "                methods_df.to_excel(writer, sheet_name='All_Methods', index=False)\n",
    "                \n",
    "                # ç»Ÿè®¡ä¿¡æ¯\n",
    "                stats_data = []\n",
    "                for method_name, mask in report['all_methods'].items():\n",
    "                    stats_data.append({\n",
    "                        'method': method_name,\n",
    "                        'outliers_detected': np.sum(mask),\n",
    "                        'percentage': np.sum(mask) / len(mask) * 100\n",
    "                    })\n",
    "                \n",
    "                stats_data.append({\n",
    "                    'method': 'GLOBAL_CONSENSUS',\n",
    "                    'outliers_detected': np.sum(report['outlier_mask']),\n",
    "                    'percentage': report['outlier_percentage']\n",
    "                })\n",
    "                \n",
    "                pd.DataFrame(stats_data).to_excel(writer, sheet_name='Statistics', index=False)\n",
    "            \n",
    "            print(f\"âœ… å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹æŠ¥å‘Šå·²ä¿å­˜: {excel_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹æŠ¥å‘Šä¿å­˜å¤±è´¥: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "def load_gbdt_model_and_data():\n",
    "    \"\"\"åŠ è½½GBDTæ¨¡å‹å’Œæ•°æ® - ä¿®æ”¹ï¼šä»è®­ç»ƒç¨‹åºçš„è¾“å‡ºåŠ è½½\"\"\"\n",
    "    print(\"=== åŠ è½½GBDTæ¨¡å‹å’Œæ•°æ® ===\")\n",
    "    \n",
    "    # ç¡®ä¿matplotlibè®¾ç½®æ­£ç¡®\n",
    "    plt.ioff()\n",
    "    matplotlib.use('Agg')\n",
    "    \n",
    "    try:\n",
    "        # åŠ è½½è®­ç»ƒå¥½çš„GBDTæ¨¡å‹\n",
    "        model_file = os.path.join(CONFIG['gbdt_model_dir'], f\"optimal_GBDT_model_{CONFIG['model_timestamp']}.pkl\")\n",
    "        with open(model_file, 'rb') as f:\n",
    "            gbdt_model = pickle.load(f)\n",
    "        print(f\"âœ“ æˆåŠŸåŠ è½½GBDTæ¨¡å‹: {model_file}\")\n",
    "        \n",
    "        # åŠ è½½æ¨¡å‹ä¿¡æ¯\n",
    "        info_file = os.path.join(CONFIG['gbdt_model_dir'], f\"model_info_{CONFIG['model_timestamp']}.pkl\")\n",
    "        with open(info_file, 'rb') as f:\n",
    "            model_info_data = pickle.load(f)\n",
    "        print(f\"âœ“ æˆåŠŸåŠ è½½æ¨¡å‹ä¿¡æ¯: {info_file}\")\n",
    "        \n",
    "        # åŠ è½½åŸå§‹æ•°æ®\n",
    "        df = pd.read_excel(CONFIG['data_file'])\n",
    "        print(f\"âœ“ æˆåŠŸåŠ è½½åŸå§‹æ•°æ®: {df.shape}\")\n",
    "        \n",
    "        # æå–ç‰¹å¾å’Œç›®æ ‡å˜é‡ - ç¡®ä¿ä¸è®­ç»ƒæ—¶ä¸€è‡´\n",
    "        X_data = df[CONFIG['selected_features']].copy()\n",
    "        y_data = df[CONFIG['target_col']].copy()\n",
    "        \n",
    "        # æ£€æŸ¥å¹¶ç§»é™¤åŒ…å«NaNçš„è¡Œ - ä¸è®­ç»ƒç¨‹åºä¿æŒä¸€è‡´\n",
    "        nan_mask_x = ~X_data.isnull().any(axis=1)\n",
    "        nan_mask_y = ~y_data.isnull()\n",
    "        valid_mask = nan_mask_x & nan_mask_y\n",
    "        \n",
    "        X_clean = X_data[valid_mask].values\n",
    "        y_clean = y_data[valid_mask].values\n",
    "        \n",
    "        print(f\"âœ“ æ•°æ®æ¸…ç†å®Œæˆ: {X_clean.shape}\")\n",
    "        print(f\"âœ“ ç‰¹å¾: {CONFIG['selected_features']}\")\n",
    "        print(f\"âœ“ ç›®æ ‡å˜é‡: {CONFIG['target_col']}\")\n",
    "        \n",
    "        # æ„é€ æ¨¡å‹ä¿¡æ¯å­—å…¸ - é€‚é…åŸPDPç¨‹åºçš„æ ¼å¼\n",
    "        model_info = {\n",
    "            'model_key': 'GBDT_PCC_k5_Optimal',\n",
    "            'method': 'PCC',\n",
    "            'k': 5,\n",
    "            'model': 'GradientBoosting',\n",
    "            'features': CONFIG['selected_features'],\n",
    "            'model_instance': gbdt_model,\n",
    "            'X_data': X_clean,\n",
    "            'y_data': y_clean,\n",
    "            'train_r2': model_info_data['metrics']['Train RÂ²'],\n",
    "            'test_r2': model_info_data['metrics']['Test RÂ²'],\n",
    "            'train_mae': model_info_data['metrics']['Train MAE'],\n",
    "            'test_mae': model_info_data['metrics']['Test MAE'],\n",
    "            'feature_importance': model_info_data['feature_importance'],\n",
    "            'cv_scores': np.array(model_info_data['cv_scores']),\n",
    "            'model_params': model_info_data['model_parameters']\n",
    "        }\n",
    "        \n",
    "        # è¿”å›å•ä¸ªæ¨¡å‹çš„åˆ—è¡¨ï¼ˆé€‚é…åŸç¨‹åºå¤„ç†å¤šä¸ªæ¨¡å‹çš„ç»“æ„ï¼‰\n",
    "        best_models = [model_info]\n",
    "        \n",
    "        # æ„é€ å…¶ä»–å¿…è¦çš„è¿”å›å€¼\n",
    "        all_features_name = CONFIG['selected_features']\n",
    "        scaler = None  # GBDTä¸éœ€è¦æ ‡å‡†åŒ–\n",
    "        data_info = {\n",
    "            'original_shape': df.shape,\n",
    "            'cleaned_shape': X_clean.shape,\n",
    "            'features': CONFIG['selected_features'],\n",
    "            'target': CONFIG['target_col']\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ“ æˆåŠŸæ„é€ æ¨¡å‹ä¿¡æ¯ï¼Œå‡†å¤‡è¿›è¡ŒPDPåˆ†æ\")\n",
    "        \n",
    "        return best_models, all_features_name, scaler, data_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½GBDTæ¨¡å‹å’Œæ•°æ®å¤±è´¥: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "def create_model_directory(model_info):\n",
    "    \"\"\"ä¸ºGBDTæ¨¡å‹åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•\"\"\"\n",
    "    model_name = f\"{model_info['method']}_k{model_info['k']}_{model_info['model']}_Optimal\"\n",
    "    # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦\n",
    "    model_name = model_name.replace(' ', '_').replace('-', '_').replace('/', '_')\n",
    "    \n",
    "    model_dir = os.path.join(CONFIG['pdp_output_dir'], model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # åˆ›å»ºå­ç›®å½•\n",
    "    subdirs = [\n",
    "        'combined_pdp',           # åˆå¹¶çš„PDPåˆ†æï¼ˆindividual + confidenceï¼‰\n",
    "        'interaction_2d_pdp',     # 2Dç‰¹å¾äº¤äº’PDP\n",
    "        'interaction_3d_pdp',     # 3Dç‰¹å¾äº¤äº’PDP\n",
    "        'outlier_detection',      # å¼‚å¸¸å€¼æ£€æµ‹æŠ¥å‘Š\n",
    "        'cleaned_analysis',       # æ¸…æ´—åçš„åˆ†æç»“æœ\n",
    "    ]\n",
    "    for subdir in subdirs:\n",
    "        os.makedirs(os.path.join(model_dir, subdir), exist_ok=True)\n",
    "    \n",
    "    return model_dir, model_name\n",
    "\n",
    "def safe_filename(name):\n",
    "    \"\"\"ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼Œç§»é™¤æ‰€æœ‰å¯èƒ½çš„éæ³•å­—ç¬¦\"\"\"\n",
    "    # æ›¿æ¢å¸¸è§çš„éæ³•å­—ç¬¦\n",
    "    replacements = {\n",
    "        ' ': '_',\n",
    "        '/': '_',\n",
    "        '\\\\': '_',\n",
    "        ':': '_',\n",
    "        '*': '_',\n",
    "        '?': '_',\n",
    "        '\"': '_',\n",
    "        '<': '_',\n",
    "        '>': '_',\n",
    "        '|': '_',\n",
    "        '(': '',\n",
    "        ')': '',\n",
    "        '[': '',\n",
    "        ']': '',\n",
    "        '{': '',\n",
    "        '}': '',\n",
    "        'Ã—': 'x',\n",
    "        'Ã·': 'div',\n",
    "        '+': 'plus',\n",
    "        '=': 'eq',\n",
    "        '%': 'percent',\n",
    "        '#': 'num',\n",
    "        '&': 'and',\n",
    "        '@': 'at',\n",
    "        '$': 'dollar',\n",
    "        '!': 'excl',\n",
    "        '^': 'caret',\n",
    "        '~': 'tilde',\n",
    "        '`': 'grave',\n",
    "        \"'\": '',\n",
    "        ',': '_',\n",
    "        ';': '_',\n",
    "        '.': '_',\n",
    "        '-': '_'\n",
    "    }\n",
    "    \n",
    "    safe_name = name\n",
    "    for old, new in replacements.items():\n",
    "        safe_name = safe_name.replace(old, new)\n",
    "    \n",
    "    # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿\n",
    "    while '__' in safe_name:\n",
    "        safe_name = safe_name.replace('__', '_')\n",
    "    \n",
    "    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ä¸‹åˆ’çº¿\n",
    "    safe_name = safe_name.strip('_')\n",
    "    \n",
    "    # é™åˆ¶é•¿åº¦ï¼ˆExcelå·¥ä½œè¡¨åç§°é™åˆ¶31å­—ç¬¦ï¼‰\n",
    "    if len(safe_name) > 25:\n",
    "        safe_name = safe_name[:25]\n",
    "    \n",
    "    return safe_name\n",
    "\n",
    "def save_data_to_excel(data, filepath, sheet_name='Data', additional_info=None):\n",
    "    \"\"\"æ”¹è¿›çš„Excelä¿å­˜å‡½æ•°ï¼Œæ›´å¥½åœ°å¤„ç†å„ç§æ•°æ®ç±»å‹\"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "            # ä¿å­˜ä¸»è¦æ•°æ®\n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    safe_sheet_name = safe_filename(key)[:31]  # Excelå·¥ä½œè¡¨åç§°é™åˆ¶31å­—ç¬¦\n",
    "                    \n",
    "                    if isinstance(value, np.ndarray):\n",
    "                        if value.ndim == 1:\n",
    "                            # 1Dæ•°ç»„ç›´æ¥ä¿å­˜\n",
    "                            df = pd.DataFrame({safe_filename(key): value})\n",
    "                            df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
    "                        elif value.ndim == 2:\n",
    "                            # 2Dæ•°ç»„ä¿å­˜ä¸ºçŸ©é˜µå½¢å¼\n",
    "                            df = pd.DataFrame(value)\n",
    "                            df.to_excel(writer, sheet_name=safe_sheet_name, index=True)\n",
    "                        else:\n",
    "                            # é«˜ç»´æ•°ç»„å±•å¹³åä¿å­˜\n",
    "                            df = pd.DataFrame({safe_filename(key): value.flatten()})\n",
    "                            df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
    "                    elif isinstance(value, (list, tuple)):\n",
    "                        # åˆ—è¡¨æˆ–å…ƒç»„\n",
    "                        try:\n",
    "                            df = pd.DataFrame({safe_filename(key): value})\n",
    "                            df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
    "                        except:\n",
    "                            # å¦‚æœç›´æ¥è½¬æ¢å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•\n",
    "                            df = pd.DataFrame({'data': [str(v) for v in value]})\n",
    "                            df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
    "                    elif isinstance(value, pd.DataFrame):\n",
    "                        # DataFrameç›´æ¥ä¿å­˜\n",
    "                        value.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
    "                    else:\n",
    "                        # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
    "                        df = pd.DataFrame({safe_filename(key): [str(value)]})\n",
    "                        df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
    "                        \n",
    "            elif isinstance(data, pd.DataFrame):\n",
    "                safe_sheet_name = safe_filename(sheet_name)[:31]\n",
    "                data.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
    "            elif isinstance(data, (list, np.ndarray)):\n",
    "                safe_sheet_name = safe_filename(sheet_name)[:31]\n",
    "                if isinstance(data, np.ndarray) and data.ndim > 1:\n",
    "                    df = pd.DataFrame(data)\n",
    "                else:\n",
    "                    df = pd.DataFrame({safe_filename(sheet_name): data})\n",
    "                df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
    "            \n",
    "            # ä¿å­˜é¢å¤–ä¿¡æ¯\n",
    "            if additional_info:\n",
    "                try:\n",
    "                    info_df = pd.DataFrame(list(additional_info.items()), columns=['é¡¹ç›®', 'å€¼'])\n",
    "                    info_df.to_excel(writer, sheet_name='ä¿¡æ¯', index=False)\n",
    "                except:\n",
    "                    # å¦‚æœé¢å¤–ä¿¡æ¯ä¿å­˜å¤±è´¥ï¼Œè·³è¿‡ä½†ä¸å½±å“ä¸»è¦æ•°æ®\n",
    "                    pass\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Excelä¿å­˜å¤±è´¥: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def save_2d_interaction_data(feature1_values, feature2_values, Z, feature1_name, feature2_name, excel_path, additional_info=None):\n",
    "    \"\"\"ä¸“é—¨ç”¨äºä¿å­˜2Däº¤äº’æ•°æ®çš„å‡½æ•°\"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "            # ä¿å­˜ç‰¹å¾1çš„å€¼\n",
    "            df1 = pd.DataFrame({'feature1_values': feature1_values})\n",
    "            df1.to_excel(writer, sheet_name='Feature1_Values', index=False)\n",
    "            \n",
    "            # ä¿å­˜ç‰¹å¾2çš„å€¼\n",
    "            df2 = pd.DataFrame({'feature2_values': feature2_values})\n",
    "            df2.to_excel(writer, sheet_name='Feature2_Values', index=False)\n",
    "            \n",
    "            # ä¿å­˜2D PDPçŸ©é˜µ\n",
    "            df_matrix = pd.DataFrame(Z)\n",
    "            df_matrix.columns = [f'F1_{i:.4f}' for i in range(len(feature1_values))]\n",
    "            df_matrix.index = [f'F2_{i:.4f}' for i in range(len(feature2_values))]\n",
    "            df_matrix.to_excel(writer, sheet_name='PDP_Matrix', index=True)\n",
    "            \n",
    "            # ä¿å­˜ä¸ºé•¿æ ¼å¼æ•°æ®ï¼ˆä¾¿äºåç»­åˆ†æï¼‰\n",
    "            XX, YY = np.meshgrid(feature1_values, feature2_values)\n",
    "            long_data = []\n",
    "            for i in range(Z.shape[0]):\n",
    "                for j in range(Z.shape[1]):\n",
    "                    long_data.append({\n",
    "                        'feature1_value': XX[i, j],\n",
    "                        'feature2_value': YY[i, j],\n",
    "                        'pdp_value': Z[i, j],\n",
    "                        'feature1_index': j,\n",
    "                        'feature2_index': i\n",
    "                    })\n",
    "            \n",
    "            df_long = pd.DataFrame(long_data)\n",
    "            df_long.to_excel(writer, sheet_name='Long_Format', index=False)\n",
    "            \n",
    "            # ä¿å­˜é¢å¤–ä¿¡æ¯\n",
    "            if additional_info:\n",
    "                info_df = pd.DataFrame(list(additional_info.items()), columns=['é¡¹ç›®', 'å€¼'])\n",
    "                info_df.to_excel(writer, sheet_name='ä¿¡æ¯', index=False)\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ 2Däº¤äº’æ•°æ®Excelä¿å­˜å¤±è´¥: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def save_3d_interaction_data(feature1_values, feature2_values, feature3_values, Z_3d, feature1_name, feature2_name, feature3_name, excel_path, additional_info=None):\n",
    "    \"\"\"ä¸“é—¨ç”¨äºä¿å­˜3Däº¤äº’æ•°æ®çš„å‡½æ•°\"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "            # ä¿å­˜ç‰¹å¾å€¼\n",
    "            df1 = pd.DataFrame({'feature1_values': feature1_values})\n",
    "            df1.to_excel(writer, sheet_name='Feature1_Values', index=False)\n",
    "            \n",
    "            df2 = pd.DataFrame({'feature2_values': feature2_values})\n",
    "            df2.to_excel(writer, sheet_name='Feature2_Values', index=False)\n",
    "            \n",
    "            df3 = pd.DataFrame({'feature3_values': feature3_values})\n",
    "            df3.to_excel(writer, sheet_name='Feature3_Values', index=False)\n",
    "            \n",
    "            # ä¿å­˜3Dæ•°æ®ä¸ºæ‰å¹³åŒ–æ ¼å¼\n",
    "            flat_3d_data = []\n",
    "            for i, val1 in enumerate(feature1_values):\n",
    "                for j, val2 in enumerate(feature2_values):\n",
    "                    for k, val3 in enumerate(feature3_values):\n",
    "                        flat_3d_data.append({\n",
    "                            'feature1_value': val1,\n",
    "                            'feature2_value': val2,\n",
    "                            'feature3_value': val3,\n",
    "                            'pdp_value': Z_3d[i, j, k],\n",
    "                            'feature1_index': i,\n",
    "                            'feature2_index': j,\n",
    "                            'feature3_index': k\n",
    "                        })\n",
    "            \n",
    "            df_3d_flat = pd.DataFrame(flat_3d_data)\n",
    "            df_3d_flat.to_excel(writer, sheet_name='3D_Flat_Data', index=False)\n",
    "            \n",
    "            # ä¿å­˜åˆ‡ç‰‡æ•°æ®ï¼ˆå›ºå®šä¸€ä¸ªç‰¹å¾ç»´åº¦ï¼‰\n",
    "            for slice_idx in [0, len(feature3_values)//2, -1]:\n",
    "                if slice_idx == -1:\n",
    "                    slice_idx = len(feature3_values) - 1\n",
    "                \n",
    "                slice_data = Z_3d[:, :, slice_idx]\n",
    "                df_slice = pd.DataFrame(slice_data)\n",
    "                df_slice.columns = [f'F2_{val:.4f}' for val in feature2_values]\n",
    "                df_slice.index = [f'F1_{val:.4f}' for val in feature1_values]\n",
    "                \n",
    "                sheet_name = f'Slice_F3_{feature3_values[slice_idx]:.4f}'[:31]\n",
    "                df_slice.to_excel(writer, sheet_name=sheet_name, index=True)\n",
    "            \n",
    "            # ä¿å­˜é¢å¤–ä¿¡æ¯\n",
    "            if additional_info:\n",
    "                info_df = pd.DataFrame(list(additional_info.items()), columns=['é¡¹ç›®', 'å€¼'])\n",
    "                info_df.to_excel(writer, sheet_name='ä¿¡æ¯', index=False)\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ 3Däº¤äº’æ•°æ®Excelä¿å­˜å¤±è´¥: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def calculate_feature_importance(model, X_data, y_data, feature_names):\n",
    "    \"\"\"è®¡ç®—ç‰¹å¾é‡è¦æ€§\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        importance_type = \"Built-in Feature Importance\"\n",
    "    else:\n",
    "        # ä½¿ç”¨ç½®æ¢é‡è¦æ€§\n",
    "        perm_importance = permutation_importance(model, X_data, y_data, n_repeats=5, random_state=42)\n",
    "        importances = perm_importance.importances_mean\n",
    "        importance_type = \"Permutation Importance\"\n",
    "    \n",
    "    return importances, importance_type\n",
    "\n",
    "def compare_pdp_before_after_cleaning(model, X_original, X_cleaned, feature_idx, feature_name, \n",
    "                                     model_dir, outliers_removed):\n",
    "    \"\"\"å¯¹æ¯”æ¸…æ´—å‰åçš„PDPåˆ†æç»“æœ\"\"\"\n",
    "    \n",
    "    print(f\"      ç”Ÿæˆæ¸…æ´—å‰åPDPå¯¹æ¯”åˆ†æ...\")\n",
    "    \n",
    "    try:\n",
    "        # åŸå§‹æ•°æ®çš„PDPåˆ†æ\n",
    "        feature_data_original = X_original[:, feature_idx]\n",
    "        feature_min_orig = np.percentile(feature_data_original, 1)\n",
    "        feature_max_orig = np.percentile(feature_data_original, 99)\n",
    "        fixed_grid_orig = np.linspace(feature_min_orig, feature_max_orig, CONFIG['grid_resolution'])\n",
    "        \n",
    "        base_pdp_original = []\n",
    "        for grid_val in fixed_grid_orig:\n",
    "            X_modified = X_original.copy()\n",
    "            X_modified[:, feature_idx] = grid_val\n",
    "            predictions = model.predict(X_modified)\n",
    "            base_pdp_original.append(np.mean(predictions))\n",
    "        \n",
    "        base_pdp_original = np.array(base_pdp_original)\n",
    "        \n",
    "        # æ¸…æ´—åæ•°æ®çš„PDPåˆ†æ\n",
    "        feature_data_cleaned = X_cleaned[:, feature_idx]\n",
    "        feature_min_clean = np.percentile(feature_data_cleaned, 1)\n",
    "        feature_max_clean = np.percentile(feature_data_cleaned, 99)\n",
    "        fixed_grid_clean = np.linspace(feature_min_clean, feature_max_clean, CONFIG['grid_resolution'])\n",
    "        \n",
    "        base_pdp_cleaned = []\n",
    "        for grid_val in fixed_grid_clean:\n",
    "            X_modified = X_cleaned.copy()\n",
    "            X_modified[:, feature_idx] = grid_val\n",
    "            predictions = model.predict(X_modified)\n",
    "            base_pdp_cleaned.append(np.mean(predictions))\n",
    "        \n",
    "        base_pdp_cleaned = np.array(base_pdp_cleaned)\n",
    "        \n",
    "        # åˆ›å»ºå¯¹æ¯”å›¾\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. PDPæ›²çº¿å¯¹æ¯”\n",
    "        ax = axes[0, 0]\n",
    "        ax.plot(fixed_grid_orig, base_pdp_original, 'red', linewidth=3, alpha=0.8, \n",
    "                label=f'æ¸…æ´—å‰PDP (n={len(X_original)})')\n",
    "        ax.plot(fixed_grid_clean, base_pdp_cleaned, 'blue', linewidth=3, \n",
    "                label=f'æ¸…æ´—åPDP (n={len(X_cleaned)})')\n",
    "        \n",
    "        ax.set_xlabel(feature_name, fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('åä¾èµ–å€¼', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('æ¸…æ´—å‰åPDPæ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”\n",
    "        ax = axes[0, 1]\n",
    "        ax.hist(feature_data_original, bins=50, alpha=0.6, color='red', \n",
    "                label=f'æ¸…æ´—å‰ (n={len(feature_data_original)})', density=True)\n",
    "        ax.hist(feature_data_cleaned, bins=50, alpha=0.6, color='blue', \n",
    "                label=f'æ¸…æ´—å (n={len(feature_data_cleaned)})', density=True)\n",
    "        \n",
    "        ax.set_xlabel(feature_name, fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('å¯†åº¦', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. PDPæ–œç‡å¯¹æ¯”\n",
    "        ax = axes[1, 0]\n",
    "        if len(fixed_grid_orig) > 1:\n",
    "            grad_orig = np.gradient(base_pdp_original, fixed_grid_orig)\n",
    "            ax.plot(fixed_grid_orig, grad_orig, 'red', linewidth=2, alpha=0.8, label='æ¸…æ´—å‰æ–œç‡')\n",
    "        \n",
    "        if len(fixed_grid_clean) > 1:\n",
    "            grad_clean = np.gradient(base_pdp_cleaned, fixed_grid_clean)\n",
    "            ax.plot(fixed_grid_clean, grad_clean, 'blue', linewidth=2, label='æ¸…æ´—åæ–œç‡')\n",
    "        \n",
    "        ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        ax.set_xlabel(feature_name, fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('PDPæ–œç‡', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('PDPæ–œç‡å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. ç»Ÿè®¡å¯¹æ¯”è¡¨\n",
    "        ax = axes[1, 1]\n",
    "        \n",
    "        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡\n",
    "        pdp_range_orig = np.max(base_pdp_original) - np.min(base_pdp_original)\n",
    "        pdp_range_clean = np.max(base_pdp_cleaned) - np.min(base_pdp_cleaned)\n",
    "        \n",
    "        feature_range_orig = np.max(feature_data_original) - np.min(feature_data_original)\n",
    "        feature_range_clean = np.max(feature_data_cleaned) - np.min(feature_data_cleaned)\n",
    "        \n",
    "        max_grad_orig = np.max(np.abs(grad_orig)) if 'grad_orig' in locals() else 0\n",
    "        max_grad_clean = np.max(np.abs(grad_clean)) if 'grad_clean' in locals() else 0\n",
    "        \n",
    "        comparison_stats = [\n",
    "            ['ç»Ÿè®¡æŒ‡æ ‡', 'æ¸…æ´—å‰', 'æ¸…æ´—å', 'å˜åŒ–'],\n",
    "            ['æ ·æœ¬æ•°é‡', f\"{len(feature_data_original)}\", f\"{len(feature_data_cleaned)}\", f\"-{outliers_removed}\"],\n",
    "            ['PDPå˜åŒ–å¹…åº¦', f\"{pdp_range_orig:.4f}\", f\"{pdp_range_clean:.4f}\", f\"{pdp_range_clean-pdp_range_orig:+.4f}\"],\n",
    "            ['ç‰¹å¾èŒƒå›´', f\"{feature_range_orig:.4f}\", f\"{feature_range_clean:.4f}\", f\"{feature_range_clean-feature_range_orig:+.4f}\"],\n",
    "            ['æœ€å¤§æ–œç‡', f\"{max_grad_orig:.4f}\", f\"{max_grad_clean:.4f}\", f\"{max_grad_clean-max_grad_orig:+.4f}\"],\n",
    "            ['ç‰¹å¾å‡å€¼', f\"{np.mean(feature_data_original):.4f}\", f\"{np.mean(feature_data_cleaned):.4f}\", \n",
    "             f\"{np.mean(feature_data_cleaned)-np.mean(feature_data_original):+.4f}\"],\n",
    "            ['ç‰¹å¾æ ‡å‡†å·®', f\"{np.std(feature_data_original):.4f}\", f\"{np.std(feature_data_cleaned):.4f}\",\n",
    "             f\"{np.std(feature_data_cleaned)-np.std(feature_data_original):+.4f}\"]\n",
    "        ]\n",
    "        \n",
    "        table = ax.table(cellText=comparison_stats[1:],\n",
    "                        colLabels=comparison_stats[0],\n",
    "                        cellLoc='center',\n",
    "                        loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1.2, 1.8)\n",
    "        ax.axis('off')\n",
    "        ax.set_title('æ¸…æ´—å‰åç»Ÿè®¡å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f'{feature_name} å¼‚å¸¸å€¼æ¸…æ´—å‰åPDPå¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # ä¿å­˜å¯¹æ¯”å›¾\n",
    "        safe_name = safe_filename(feature_name)\n",
    "        img_path = os.path.join(model_dir, 'cleaned_analysis', f'PDP_Comparison_{safe_name}.png')\n",
    "        plt.savefig(img_path, dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # ä¿å­˜å¯¹æ¯”æ•°æ®\n",
    "        comparison_data = {\n",
    "            'original_grid': fixed_grid_orig,\n",
    "            'original_pdp': base_pdp_original,\n",
    "            'cleaned_grid': fixed_grid_clean,\n",
    "            'cleaned_pdp': base_pdp_cleaned,\n",
    "            'original_feature_data': feature_data_original,\n",
    "            'cleaned_feature_data': feature_data_cleaned\n",
    "        }\n",
    "        \n",
    "        if 'grad_orig' in locals():\n",
    "            comparison_data['original_gradient'] = grad_orig\n",
    "        if 'grad_clean' in locals():\n",
    "            comparison_data['cleaned_gradient'] = grad_clean\n",
    "        \n",
    "        excel_path = os.path.join(model_dir, 'cleaned_analysis', f'PDP_Comparison_Data_{safe_name}.xlsx')\n",
    "        save_data_to_excel(comparison_data, excel_path, 'PDPå¯¹æ¯”æ•°æ®')\n",
    "        \n",
    "        print(f\"        âœ“ PDPæ¸…æ´—å‰åå¯¹æ¯”åˆ†æå·²ä¿å­˜\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"        âŒ PDPæ¸…æ´—å‰åå¯¹æ¯”åˆ†æå¤±è´¥: {str(e)}\")\n",
    "        plt.close('all')\n",
    "        return False\n",
    "\n",
    "def generate_combined_pdp_analysis(model_info, model_dir, data_export):\n",
    "    \"\"\"ç”Ÿæˆåˆå¹¶çš„PDPåˆ†æ - ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—åçš„æ•°æ®\"\"\"\n",
    "    print(f\"  ç”Ÿæˆç»Ÿä¸€æ¸…æ´—PDPåˆ†æï¼ˆIndividual + ç½®ä¿¡åŒºé—´ï¼‰...\")\n",
    "    \n",
    "    model = model_info['model_instance']\n",
    "    X_data = model_info['X_data_cleaned']  # ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®\n",
    "    y_data = model_info['y_data_cleaned']  # ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®\n",
    "    features = model_info['features']\n",
    "    \n",
    "    # è·å–è‡ªé€‚åº”é…ç½®\n",
    "    adaptive_config = get_adaptive_config(X_data.shape[0])\n",
    "    n_bootstrap = adaptive_config['bootstrap_samples']\n",
    "    \n",
    "    print(f\"    æ•°æ®é›†å¤§å°: {X_data.shape[0]} (å·²æ¸…æ´—)\")\n",
    "    print(f\"    ç½‘æ ¼åˆ†è¾¨ç‡: {CONFIG['grid_resolution']}ç‚¹ (å…‰æ»‘æ›²çº¿)\")\n",
    "    print(f\"    Bootstrapæ¬¡æ•°: {n_bootstrap} (å­¦æœ¯æ ‡å‡†)\")\n",
    "    \n",
    "    # è®¡ç®—ç‰¹å¾é‡è¦æ€§\n",
    "    importances, importance_type = calculate_feature_importance(model, X_data, y_data, features)\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªç‰¹å¾ç”Ÿæˆåˆå¹¶çš„PDPåˆ†æ\n",
    "    combined_data_all = {}\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        print(f\"    å¤„ç†ç‰¹å¾: {feature}\")\n",
    "        \n",
    "        try:\n",
    "            # è·å–æ¸…æ´—åçš„ç‰¹å¾æ•°æ®\n",
    "            feature_data = X_data[:, i]\n",
    "            \n",
    "            # è®¡ç®—ç‰¹å¾çš„ç›´æ–¹å›¾åˆ†å¸ƒæ•°æ®\n",
    "            histogram_data = calculate_histogram_data(feature_data)\n",
    "            \n",
    "            # è®¡ç®—ç‰¹å¾çš„å®é™…èŒƒå›´\n",
    "            feature_min = np.percentile(feature_data, CONFIG['percentiles'][0] * 100)\n",
    "            feature_max = np.percentile(feature_data, CONFIG['percentiles'][1] * 100)\n",
    "            \n",
    "            # åˆ›å»ºå›ºå®šçš„ç‰¹å¾å€¼ç½‘æ ¼\n",
    "            fixed_grid = np.linspace(feature_min, feature_max, CONFIG['grid_resolution'])\n",
    "            \n",
    "            print(f\"      å¼€å§‹ {n_bootstrap} æ¬¡Bootstrapé‡‡æ ·...\")\n",
    "            \n",
    "            # Bootstrapé‡‡æ ·è®¡ç®—ç½®ä¿¡åŒºé—´\n",
    "            bootstrap_pdps = []\n",
    "            \n",
    "            for b in range(n_bootstrap):\n",
    "                try:\n",
    "                    # åˆ›å»ºbootstrapæ ·æœ¬\n",
    "                    boot_indices = np.random.choice(X_data.shape[0], X_data.shape[0], replace=True)\n",
    "                    X_boot = X_data[boot_indices]\n",
    "                    \n",
    "                    # æ‰‹åŠ¨è®¡ç®—PDPï¼Œä½¿ç”¨å›ºå®šç½‘æ ¼\n",
    "                    boot_pdp_values = []\n",
    "                    for grid_val in fixed_grid:\n",
    "                        # åˆ›å»ºä¿®æ”¹åçš„æ ·æœ¬\n",
    "                        X_modified = X_boot.copy()\n",
    "                        X_modified[:, i] = grid_val\n",
    "                        \n",
    "                        # é¢„æµ‹å¹¶å–å¹³å‡\n",
    "                        predictions = model.predict(X_modified)\n",
    "                        boot_pdp_values.append(np.mean(predictions))\n",
    "                    \n",
    "                    bootstrap_pdps.append(boot_pdp_values)\n",
    "                    \n",
    "                    # è¿›åº¦æç¤ºï¼ˆæ¯200æ¬¡ï¼‰\n",
    "                    if (b + 1) % 200 == 0:\n",
    "                        print(f\"        Bootstrapè¿›åº¦: {b + 1}/{n_bootstrap}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"      âš ï¸ Bootstrap {b+1} è®¡ç®—å¤±è´¥: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if len(bootstrap_pdps) < max(10, n_bootstrap // 10):\n",
    "                print(f\"      âŒ ç‰¹å¾ {feature} Bootstrapæ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨åŸºç¡€PDP\")\n",
    "                bootstrap_pdps = []\n",
    "            \n",
    "            # é‡æ–°è®¡ç®—åŸºç¡€PDPä½¿ç”¨ç›¸åŒçš„å›ºå®šç½‘æ ¼\n",
    "            base_pdp_fixed = []\n",
    "            for grid_val in fixed_grid:\n",
    "                X_modified = X_data.copy()\n",
    "                X_modified[:, i] = grid_val\n",
    "                predictions = model.predict(X_modified)\n",
    "                base_pdp_fixed.append(np.mean(predictions))\n",
    "            \n",
    "            base_pdp_fixed = np.array(base_pdp_fixed)\n",
    "            \n",
    "            # è®¡ç®—ç½®ä¿¡åŒºé—´\n",
    "            if len(bootstrap_pdps) > 0:\n",
    "                bootstrap_pdps = np.array(bootstrap_pdps)\n",
    "                success_rate = len(bootstrap_pdps) / n_bootstrap * 100\n",
    "                \n",
    "                # è®¡ç®—å¤šç§ç½®ä¿¡åŒºé—´\n",
    "                confidence_intervals = {}\n",
    "                for conf_level in [0.90, 0.95, 0.99]:\n",
    "                    alpha = 1 - conf_level\n",
    "                    lower_percentile = (alpha/2) * 100\n",
    "                    upper_percentile = (1 - alpha/2) * 100\n",
    "                    \n",
    "                    ci_lower = np.percentile(bootstrap_pdps, lower_percentile, axis=0)\n",
    "                    ci_upper = np.percentile(bootstrap_pdps, upper_percentile, axis=0)\n",
    "                    \n",
    "                    confidence_intervals[f'{int(conf_level*100)}%'] = {\n",
    "                        'lower': ci_lower,\n",
    "                        'upper': ci_upper,\n",
    "                        'width': ci_upper - ci_lower\n",
    "                    }\n",
    "                \n",
    "                # ä½¿ç”¨ä¸»è¦ç½®ä¿¡æ°´å¹³\n",
    "                main_ci = confidence_intervals[f'{int(CONFIG[\"confidence_level\"]*100)}%']\n",
    "                ci_lower = main_ci['lower']\n",
    "                ci_upper = main_ci['upper']\n",
    "                \n",
    "                bootstrap_mean = np.mean(bootstrap_pdps, axis=0)\n",
    "                bootstrap_std = np.std(bootstrap_pdps, axis=0)\n",
    "                \n",
    "                print(f\"      æˆåŠŸBootstrap: {len(bootstrap_pdps)}/{n_bootstrap} ({success_rate:.1f}%)\")\n",
    "            else:\n",
    "                confidence_intervals = {}\n",
    "                ci_lower = None\n",
    "                ci_upper = None\n",
    "                bootstrap_mean = None\n",
    "                bootstrap_std = None\n",
    "                success_rate = 0\n",
    "                print(f\"      æœªä½¿ç”¨Bootstrapï¼Œä»…æ˜¾ç¤ºåŸºç¡€PDP\")\n",
    "            \n",
    "            # ä¿å­˜åˆå¹¶æ•°æ®\n",
    "            combined_data_all[feature] = {\n",
    "                'feature_values': fixed_grid,\n",
    "                'base_pdp': base_pdp_fixed,\n",
    "                'feature_importance': importances[i],\n",
    "                'importance_type': importance_type,\n",
    "                'bootstrap_count': len(bootstrap_pdps) if len(bootstrap_pdps) > 0 else 0,\n",
    "                'success_rate': success_rate,\n",
    "                'confidence_intervals': confidence_intervals,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                'bootstrap_mean': bootstrap_mean,\n",
    "                'bootstrap_std': bootstrap_std,\n",
    "                'cleaned_feature_data': feature_data,  # æ¸…æ´—åç‰¹å¾æ•°æ®\n",
    "                'histogram_data': histogram_data,  # ç›´æ–¹å›¾åˆ†å¸ƒæ•°æ®\n",
    "                'data_suffix': \"_cleaned\"\n",
    "            }\n",
    "            \n",
    "            # åˆ›å»ºå¢å¼ºçš„å¯è§†åŒ–å›¾\n",
    "            fig, axes = plt.subplots(3, 1, figsize=(CONFIG['figure_size'][0], CONFIG['figure_size'][1]*1.5))\n",
    "            \n",
    "            # ç¬¬ä¸€ä¸ªå­å›¾ï¼šåˆå¹¶çš„PDP + ç½®ä¿¡åŒºé—´ + ç‰¹å¾åˆ†å¸ƒ\n",
    "            ax1 = axes[0]\n",
    "            \n",
    "            # ç»˜åˆ¶ç½®ä¿¡åŒºé—´ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "            if ci_lower is not None and ci_upper is not None:\n",
    "                # ç»˜åˆ¶å¤šç§ç½®ä¿¡åŒºé—´ï¼ˆä¸åŒé€æ˜åº¦ï¼‰\n",
    "                colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "                alphas = [0.6, 0.4, 0.2]\n",
    "                for idx, (level, ci_data) in enumerate(confidence_intervals.items()):\n",
    "                    ax1.fill_between(fixed_grid, ci_data['lower'], ci_data['upper'], \n",
    "                                   alpha=alphas[idx], color=colors[idx], \n",
    "                                   label=f'{level} ç½®ä¿¡åŒºé—´')\n",
    "                \n",
    "                # ç»˜åˆ¶Bootstrapå‡å€¼\n",
    "                if bootstrap_mean is not None:\n",
    "                    ax1.plot(fixed_grid, bootstrap_mean, 'red', linewidth=2, linestyle='--', \n",
    "                            alpha=0.8, label='Bootstrapå‡å€¼')\n",
    "            \n",
    "            # ç»˜åˆ¶ä¸»PDPæ›²çº¿\n",
    "            ax1.plot(fixed_grid, base_pdp_fixed, 'blue', linewidth=4, label='PDPæ›²çº¿', zorder=10)\n",
    "            \n",
    "            # æ·»åŠ ç‰¹å¾åˆ†å¸ƒï¼ˆç›´æ–¹å›¾ï¼ŒåŒYè½´ï¼‰\n",
    "            ax1_twin = ax1.twinx()\n",
    "            ax1_twin.hist(feature_data, bins=CONFIG['histogram_bins'], alpha=0.3, \n",
    "                         color='gray', density=True, label='ç‰¹å¾åˆ†å¸ƒ')\n",
    "            ax1_twin.set_ylabel('ç‰¹å¾å¯†åº¦', fontsize=12, color='gray')\n",
    "            ax1_twin.tick_params(axis='y', labelcolor='gray')\n",
    "            \n",
    "            ax1.set_xlabel(feature, fontsize=14, fontweight='bold')\n",
    "            ax1.set_ylabel('åä¾èµ–å€¼', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            title_text = f'ç»Ÿä¸€æ¸…æ´—PDPåˆ†æ: {feature}\\né‡è¦æ€§: {importances[i]:.4f} ({importance_type})'\n",
    "            if len(bootstrap_pdps) > 0:\n",
    "                title_text += f'\\nBootstrap: {len(bootstrap_pdps)}æ¬¡ (æˆåŠŸç‡: {success_rate:.1f}%)'\n",
    "            title_text += f'\\nä½¿ç”¨ç»Ÿä¸€æ¸…æ´—åæ•°æ®'\n",
    "            \n",
    "            ax1.set_title(title_text, fontsize=14, fontweight='bold')\n",
    "            ax1.legend(loc='upper left')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # ç¬¬äºŒä¸ªå­å›¾ï¼šPDPä¸€é˜¶å¯¼æ•°ï¼ˆæ–œç‡å˜åŒ–ï¼‰\n",
    "            ax2 = axes[1]\n",
    "            if len(fixed_grid) > 1:\n",
    "                pdp_gradient = np.gradient(base_pdp_fixed, fixed_grid)\n",
    "                ax2.plot(fixed_grid, pdp_gradient, 'red', linewidth=3, label='PDPæ–œç‡')\n",
    "                ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "                ax2.set_xlabel(feature, fontsize=12, fontweight='bold')\n",
    "                ax2.set_ylabel('PDPæ–œç‡', fontsize=12, fontweight='bold')\n",
    "                ax2.set_title('ç‰¹å¾æ•ˆåº”å˜åŒ–ç‡', fontsize=12, fontweight='bold')\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "                ax2.legend()\n",
    "                \n",
    "                max_gradient = np.max(np.abs(pdp_gradient))\n",
    "            else:\n",
    "                ax2.text(0.5, 0.5, 'æ•°æ®ç‚¹ä¸è¶³\\næ— æ³•è®¡ç®—æ–œç‡', ha='center', va='center', \n",
    "                        transform=ax2.transAxes, fontsize=14)\n",
    "                max_gradient = 0\n",
    "                pdp_gradient = np.array([])\n",
    "            \n",
    "            # ç¬¬ä¸‰ä¸ªå­å›¾ï¼šä¸ç¡®å®šæ€§åˆ†æï¼ˆå¦‚æœæœ‰ç½®ä¿¡åŒºé—´ï¼‰\n",
    "            ax3 = axes[2]\n",
    "            if ci_lower is not None and ci_upper is not None:\n",
    "                ci_width = ci_upper - ci_lower\n",
    "                ax3.plot(fixed_grid, ci_width, 'purple', linewidth=3, label='ç½®ä¿¡åŒºé—´å®½åº¦')\n",
    "                if bootstrap_std is not None:\n",
    "                    ax3.plot(fixed_grid, bootstrap_std, 'orange', linewidth=3, label='Bootstrapæ ‡å‡†å·®')\n",
    "                ax3.set_xlabel(feature, fontsize=12, fontweight='bold')\n",
    "                ax3.set_ylabel('ä¸ç¡®å®šæ€§æŒ‡æ ‡', fontsize=12, fontweight='bold')\n",
    "                ax3.set_title('PDPä¸ç¡®å®šæ€§åˆ†æ', fontsize=12, fontweight='bold')\n",
    "                ax3.legend()\n",
    "                ax3.grid(True, alpha=0.3)\n",
    "                \n",
    "                avg_ci_width = np.mean(ci_width)\n",
    "                max_uncertainty = np.max(ci_width)\n",
    "            else:\n",
    "                ax3.text(0.5, 0.5, 'æ— Bootstrapæ•°æ®\\næ— æ³•åˆ†æä¸ç¡®å®šæ€§', ha='center', va='center', \n",
    "                        transform=ax3.transAxes, fontsize=14)\n",
    "                ax3.set_title('PDPä¸ç¡®å®šæ€§åˆ†æ', fontsize=12, fontweight='bold')\n",
    "                avg_ci_width = 0\n",
    "                max_uncertainty = 0\n",
    "            \n",
    "            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†\n",
    "            stats_text = f'ç½‘æ ¼åˆ†è¾¨ç‡: {CONFIG[\"grid_resolution\"]}ç‚¹\\n' + \\\n",
    "                        f'åˆ†ææ ·æœ¬æ•°: {len(feature_data)} (æ¸…æ´—å)\\n' + \\\n",
    "                        f'ç‰¹å¾å‡å€¼: {np.mean(feature_data):.4f}\\n' + \\\n",
    "                        f'ç‰¹å¾æ ‡å‡†å·®: {np.std(feature_data):.4f}\\n' + \\\n",
    "                        f'PDPå˜åŒ–å¹…åº¦: {np.max(base_pdp_fixed) - np.min(base_pdp_fixed):.4f}\\n' + \\\n",
    "                        f'æœ€å¤§æ–œç‡: {max_gradient:.4f}\\n' + \\\n",
    "                        f'å¹³å‡ä¸ç¡®å®šæ€§: {avg_ci_width:.4f}'\n",
    "            \n",
    "            ax1.text(0.02, 0.98, stats_text, transform=ax1.transAxes, \n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcyan\", alpha=0.8),\n",
    "                    verticalalignment='top', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # ä¿å­˜å¢å¼ºå›¾ç‰‡\n",
    "            safe_name = safe_filename(feature)\n",
    "            filename = f\"Unified_Cleaned_PDP_Analysis_{safe_name}.png\"\n",
    "            img_path = os.path.join(model_dir, 'combined_pdp', filename)\n",
    "            plt.savefig(img_path, dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # ä¿å­˜å¯¹åº”çš„Excelæ•°æ®\n",
    "            excel_data = {\n",
    "                'feature_values': fixed_grid,\n",
    "                'base_pdp': base_pdp_fixed,\n",
    "                'pdp_gradient': pdp_gradient if len(pdp_gradient) > 0 else np.full_like(fixed_grid, np.nan),\n",
    "                \n",
    "                # æ¸…æ´—åç‰¹å¾åˆ†å¸ƒæ•°æ®\n",
    "                'cleaned_feature_data': feature_data,\n",
    "                'histogram_bin_centers': histogram_data['bin_centers'],\n",
    "                'histogram_bin_edges': histogram_data['bin_edges'],\n",
    "                'histogram_counts': histogram_data['counts'],\n",
    "                'histogram_densities': histogram_data['densities'],\n",
    "                'histogram_relative_frequencies': histogram_data['relative_frequencies'],\n",
    "            }\n",
    "            \n",
    "            # æ·»åŠ ç½®ä¿¡åŒºé—´æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "            if ci_lower is not None and ci_upper is not None:\n",
    "                excel_data.update({\n",
    "                    'bootstrap_mean': bootstrap_mean,\n",
    "                    'bootstrap_std': bootstrap_std,\n",
    "                    'ci_width': ci_width\n",
    "                })\n",
    "                \n",
    "                # æ·»åŠ æ‰€æœ‰ç½®ä¿¡åŒºé—´\n",
    "                for level, ci_data in confidence_intervals.items():\n",
    "                    excel_data[f'ci_{level}_lower'] = ci_data['lower']\n",
    "                    excel_data[f'ci_{level}_upper'] = ci_data['upper']\n",
    "                    excel_data[f'ci_{level}_width'] = ci_data['width']\n",
    "            \n",
    "            excel_info = {\n",
    "                'ç‰¹å¾åç§°': feature,\n",
    "                'ç‰¹å¾é‡è¦æ€§': importances[i],\n",
    "                'é‡è¦æ€§ç±»å‹': importance_type,\n",
    "                'ç½‘æ ¼åˆ†è¾¨ç‡': CONFIG['grid_resolution'],\n",
    "                'åˆ†ææ ·æœ¬æ•°é‡': len(feature_data),\n",
    "                'æ•°æ®çŠ¶æ€': 'ç»Ÿä¸€æ¸…æ´—å',\n",
    "                'ç‰¹å¾å‡å€¼': np.mean(feature_data),\n",
    "                'ç‰¹å¾æ ‡å‡†å·®': np.std(feature_data),\n",
    "                'ç‰¹å¾æœ€å°å€¼': np.min(feature_data),\n",
    "                'ç‰¹å¾æœ€å¤§å€¼': np.max(feature_data),\n",
    "                'ç‰¹å¾ä¸­ä½æ•°': np.median(feature_data),\n",
    "                'PDPå˜åŒ–å¹…åº¦': np.max(base_pdp_fixed) - np.min(base_pdp_fixed),\n",
    "                'æœ€å¤§æ–œç‡': max_gradient,\n",
    "                'Bootstrapæ¬¡æ•°': len(bootstrap_pdps) if len(bootstrap_pdps) > 0 else 0,\n",
    "                'BootstrapæˆåŠŸç‡': success_rate,\n",
    "                'å¹³å‡ä¸ç¡®å®šæ€§': avg_ci_width,\n",
    "                'æœ€å¤§ä¸ç¡®å®šæ€§': max_uncertainty,\n",
    "                'ç›´æ–¹å›¾ç®±æ•°': CONFIG['histogram_bins'],\n",
    "                'ç›´æ–¹å›¾ç®±å®½åº¦': histogram_data['bin_width'],\n",
    "                'æ•°æ®è¦†ç›–èŒƒå›´': f\"{feature_min:.4f} - {feature_max:.4f}\",\n",
    "                'ç»Ÿä¸€æ¸…æ´—': 'âœ… æ‰€æœ‰åˆ†æä½¿ç”¨ç›¸åŒçš„æ¸…æ´—åæ•°æ®'\n",
    "            }\n",
    "            \n",
    "            excel_path = os.path.join(model_dir, 'combined_pdp', f\"Unified_Cleaned_PDP_Analysis_{safe_name}.xlsx\")\n",
    "            save_data_to_excel(excel_data, excel_path, 'ç»Ÿä¸€æ¸…æ´—PDPæ•°æ®', excel_info)\n",
    "            \n",
    "            print(f\"      âœ“ ç‰¹å¾ {feature} ç»Ÿä¸€æ¸…æ´—PDPåˆ†æå’Œå®Œæ•´æ•°æ®å·²ä¿å­˜\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ ç‰¹å¾ {feature} ç»Ÿä¸€æ¸…æ´—PDPåˆ†æå¤±è´¥: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "        finally:\n",
    "            plt.close('all')\n",
    "    \n",
    "    if not combined_data_all:\n",
    "        print(f\"    âŒ æ‰€æœ‰ç‰¹å¾ç»Ÿä¸€æ¸…æ´—PDPè®¡ç®—éƒ½å¤±è´¥äº†\")\n",
    "        data_export['combined_pdp'] = {}\n",
    "        return {}\n",
    "    \n",
    "    print(f\"    âœ“ æˆåŠŸå¤„ç† {len(combined_data_all)}/{len(features)} ä¸ªç‰¹å¾\")\n",
    "    \n",
    "    # ç”Ÿæˆç‰¹å¾PDPç»¼åˆå¯¹æ¯”å›¾\n",
    "    try:\n",
    "        plt.figure(figsize=(18, 14))\n",
    "        \n",
    "        successful_features = list(combined_data_all.keys())\n",
    "        n_features = len(successful_features)\n",
    "        \n",
    "        if n_features <= 4:\n",
    "            rows, cols = 2, 2\n",
    "        elif n_features <= 6:\n",
    "            rows, cols = 2, 3\n",
    "        elif n_features <= 9:\n",
    "            rows, cols = 3, 3\n",
    "        else:\n",
    "            rows, cols = 4, 3\n",
    "        \n",
    "        comprehensive_data = {}\n",
    "        \n",
    "        for i, feature in enumerate(successful_features):\n",
    "            if i >= rows * cols:\n",
    "                break\n",
    "                \n",
    "            plt.subplot(rows, cols, i + 1)\n",
    "            \n",
    "            feature_values = combined_data_all[feature]['feature_values']\n",
    "            base_pdp = combined_data_all[feature]['base_pdp']\n",
    "            importance = combined_data_all[feature]['feature_importance']\n",
    "            cleaned_data = combined_data_all[feature]['cleaned_feature_data']\n",
    "            \n",
    "            # ç»˜åˆ¶ç½®ä¿¡åŒºé—´ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "            ci_lower = combined_data_all[feature]['ci_lower']\n",
    "            ci_upper = combined_data_all[feature]['ci_upper']\n",
    "            \n",
    "            if ci_lower is not None and ci_upper is not None:\n",
    "                plt.fill_between(feature_values, ci_lower, ci_upper, alpha=0.3, color='lightblue', label='95%ç½®ä¿¡åŒºé—´')\n",
    "            \n",
    "            # ç»˜åˆ¶ä¸»PDPæ›²çº¿\n",
    "            plt.plot(feature_values, base_pdp, 'blue', linewidth=2, label='PDPæ›²çº¿')\n",
    "            \n",
    "            # æ·»åŠ æ¸…æ´—åæ•°æ®åˆ†å¸ƒï¼ˆå³yè½´ï¼‰\n",
    "            ax_twin = plt.gca().twinx()\n",
    "            ax_twin.hist(cleaned_data, bins=20, alpha=0.2, color='gray', density=True)\n",
    "            ax_twin.set_ylabel('å¯†åº¦', fontsize=8, color='gray')\n",
    "            ax_twin.tick_params(axis='y', labelcolor='gray', labelsize=8)\n",
    "            \n",
    "            plt.xlabel(feature, fontsize=10)\n",
    "            plt.ylabel('PDP', fontsize=10)\n",
    "            \n",
    "            title = f'{feature}\\né‡è¦æ€§: {importance:.4f}'\n",
    "            title += f'\\n(ç»Ÿä¸€æ¸…æ´—)'\n",
    "            \n",
    "            plt.title(title, fontsize=9, fontweight='bold')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # ä¿å­˜ç»¼åˆå¯¹æ¯”æ•°æ®\n",
    "            comprehensive_data[f'{safe_filename(feature)}_values'] = feature_values\n",
    "            comprehensive_data[f'{safe_filename(feature)}_pdp'] = base_pdp\n",
    "            comprehensive_data[f'{safe_filename(feature)}_cleaned_data'] = cleaned_data\n",
    "        \n",
    "        plt.suptitle(f'æ‰€æœ‰ç‰¹å¾ç»Ÿä¸€æ¸…æ´—PDPç»¼åˆå¯¹æ¯”\\n{model_info[\"method\"]} + {model_info[\"model\"]} (K={model_info[\"k\"]})', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # ä¿å­˜ç»¼åˆå¯¹æ¯”å›¾\n",
    "        comp_img_path = os.path.join(model_dir, 'combined_pdp', 'All_Features_Unified_Cleaned_PDP_Comprehensive.png')\n",
    "        plt.savefig(comp_img_path, dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "        \n",
    "        # ä¿å­˜ç»¼åˆå¯¹æ¯”æ•°æ®\n",
    "        comp_excel_path = os.path.join(model_dir, 'combined_pdp', 'All_Features_Unified_Cleaned_PDP_Comprehensive.xlsx')\n",
    "        save_data_to_excel(comprehensive_data, comp_excel_path, 'ç»¼åˆç»Ÿä¸€æ¸…æ´—PDPæ•°æ®')\n",
    "        \n",
    "        print(f\"    âœ“ ç»Ÿä¸€æ¸…æ´—PDPç»¼åˆå¯¹æ¯”å›¾å’Œæ•°æ®å·²ä¿å­˜\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ ç»Ÿä¸€æ¸…æ´—PDPç»¼åˆå¯¹æ¯”å›¾ç”Ÿæˆå¤±è´¥: {str(e)}\")\n",
    "        plt.close('all')\n",
    "    \n",
    "    data_export['combined_pdp'] = combined_data_all\n",
    "    return combined_data_all\n",
    "\n",
    "def generate_complete_2d_interaction_pdp(model_info, model_dir, data_export):\n",
    "    \"\"\"ç”Ÿæˆå®Œæ•´çš„2Dç‰¹å¾äº¤äº’PDPåˆ†æ - ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—åçš„æ•°æ®\"\"\"\n",
    "    print(f\"  ç”Ÿæˆå®Œæ•´çš„2Dç‰¹å¾äº¤äº’PDPåˆ†æï¼ˆä½¿ç”¨ç»Ÿä¸€æ¸…æ´—æ•°æ®ï¼Œæ‰€æœ‰ç‰¹å¾å¯¹ï¼‰...\")\n",
    "    \n",
    "    model = model_info['model_instance']\n",
    "    X_data = model_info['X_data_cleaned']  # ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®\n",
    "    features = model_info['features']\n",
    "    \n",
    "    interaction_data = {}\n",
    "    \n",
    "    # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç‰¹å¾å¯¹ç»„åˆ\n",
    "    feature_combinations = list(combinations(range(len(features)), 2))\n",
    "    total_combinations = len(feature_combinations)\n",
    "    \n",
    "    print(f\"    æ€»å…±éœ€è¦åˆ†æ {total_combinations} ä¸ªç‰¹å¾äº¤äº’å¯¹\")\n",
    "    print(f\"    2Dç½‘æ ¼åˆ†è¾¨ç‡: {CONFIG['interaction_2d_resolution']}Ã—{CONFIG['interaction_2d_resolution']}\")\n",
    "    print(f\"    ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—åæ•°æ®: {X_data.shape[0]} æ ·æœ¬\")\n",
    "    \n",
    "    successful_interactions = 0\n",
    "    \n",
    "    for comb_idx, (i, j) in enumerate(feature_combinations):\n",
    "        feature1_name = features[i]\n",
    "        feature2_name = features[j]\n",
    "        \n",
    "        print(f\"    [{comb_idx+1}/{total_combinations}] å¤„ç†äº¤äº’: {feature1_name} Ã— {feature2_name}\")\n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®—2D PDP\n",
    "            pd_result = partial_dependence(\n",
    "                model, X_data, features=[i, j], \n",
    "                grid_resolution=CONFIG['interaction_2d_resolution'],\n",
    "                percentiles=CONFIG['percentiles']\n",
    "            )\n",
    "            \n",
    "            # å…¼å®¹æ–°ç‰ˆsklearnçš„è¿”å›æ ¼å¼\n",
    "            if hasattr(pd_result, 'grid_values'):\n",
    "                feature1_values = pd_result.grid_values[0]\n",
    "                feature2_values = pd_result.grid_values[1]\n",
    "                Z_raw = pd_result.average\n",
    "                \n",
    "                if len(Z_raw.shape) == 3:\n",
    "                    Z = Z_raw[0]\n",
    "                elif len(Z_raw.shape) == 2:\n",
    "                    Z = Z_raw\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected Z shape: {Z_raw.shape}\")\n",
    "            else:\n",
    "                feature1_values = pd_result[1][0]\n",
    "                feature2_values = pd_result[1][1]\n",
    "                Z_raw = pd_result[0]\n",
    "                \n",
    "                if len(Z_raw.shape) == 3:\n",
    "                    Z = Z_raw[0]\n",
    "                else:\n",
    "                    Z = Z_raw\n",
    "            \n",
    "            # ç¡®ä¿Zçš„å½¢çŠ¶æ­£ç¡®\n",
    "            expected_shape = (len(feature2_values), len(feature1_values))\n",
    "            if Z.shape != expected_shape:\n",
    "                if Z.shape == (len(feature1_values), len(feature2_values)):\n",
    "                    Z = Z.T\n",
    "                else:\n",
    "                    raise ValueError(f\"æ— æ³•åŒ¹é…ZçŸ©é˜µå½¢çŠ¶: {Z.shape}\")\n",
    "            \n",
    "            XX, YY = np.meshgrid(feature1_values, feature2_values)\n",
    "            \n",
    "            # ä¿å­˜äº¤äº’æ•°æ®\n",
    "            interaction_key = f\"{feature1_name}_Ã—_{feature2_name}\"\n",
    "            interaction_data[interaction_key] = {\n",
    "                'feature1_values': feature1_values,\n",
    "                'feature2_values': feature2_values, \n",
    "                'feature1_name': feature1_name,\n",
    "                'feature2_name': feature2_name,\n",
    "                'partial_dependence_2d': Z,\n",
    "                'interaction_strength': np.var(Z)  # ä½¿ç”¨æ–¹å·®è¡¡é‡äº¤äº’å¼ºåº¦\n",
    "            }\n",
    "            \n",
    "            # åˆ›å»ºå¢å¼ºçš„2Dçƒ­åŠ›å›¾\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(CONFIG['figure_size'][0]*1.5, CONFIG['figure_size'][1]))\n",
    "            \n",
    "            # å·¦å›¾ï¼šçƒ­åŠ›å›¾\n",
    "            ax1 = axes[0]\n",
    "            im1 = ax1.contourf(XX, YY, Z, levels=50, cmap='RdYlBu_r')\n",
    "            fig.colorbar(im1, ax=ax1, label='åä¾èµ–å€¼')\n",
    "            \n",
    "            # æ·»åŠ ç­‰é«˜çº¿\n",
    "            contours = ax1.contour(XX, YY, Z, levels=10, colors='black', alpha=0.6, linewidths=0.8)\n",
    "            ax1.clabel(contours, inline=True, fontsize=8, fmt='%.3f')\n",
    "            \n",
    "            ax1.set_xlabel(feature1_name, fontsize=12, fontweight='bold')\n",
    "            ax1.set_ylabel(feature2_name, fontsize=12, fontweight='bold')\n",
    "            ax1.set_title(f'2Däº¤äº’çƒ­åŠ›å›¾ï¼ˆç»Ÿä¸€æ¸…æ´—æ•°æ®ï¼‰\\n{feature1_name} Ã— {feature2_name}', fontsize=12, fontweight='bold')\n",
    "            \n",
    "            # æ·»åŠ æ¸…æ´—åæ•°æ®ç‚¹åˆ†å¸ƒ\n",
    "            feature1_data = X_data[:, i]\n",
    "            feature2_data = X_data[:, j]\n",
    "            ax1.scatter(feature1_data, feature2_data, c='white', s=0.5, alpha=0.3, label='æ¸…æ´—åæ•°æ®ç‚¹')\n",
    "            ax1.legend()\n",
    "            \n",
    "            # å³å›¾ï¼š3Dè¡¨é¢å›¾\n",
    "            ax2 = fig.add_subplot(122, projection='3d')\n",
    "            surf = ax2.plot_surface(XX, YY, Z, cmap='RdYlBu_r', alpha=0.9, \n",
    "                                 linewidth=0, antialiased=True)\n",
    "            fig.colorbar(surf, ax=ax2, label='åä¾èµ–å€¼', shrink=0.5, aspect=20)\n",
    "            \n",
    "            ax2.set_xlabel(feature1_name, fontsize=10, fontweight='bold')\n",
    "            ax2.set_ylabel(feature2_name, fontsize=10, fontweight='bold')\n",
    "            ax2.set_zlabel('åä¾èµ–å€¼', fontsize=10, fontweight='bold')\n",
    "            ax2.set_title(f'3Däº¤äº’è¡¨é¢å›¾ï¼ˆç»Ÿä¸€æ¸…æ´—æ•°æ®ï¼‰\\n{feature1_name} Ã— {feature2_name}', fontsize=10, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # ä¿å­˜2Däº¤äº’å›¾\n",
    "            safe_name1 = safe_filename(feature1_name)\n",
    "            safe_name2 = safe_filename(feature2_name)\n",
    "            filename = f\"2D_Interaction_PDP_Cleaned_{safe_name1}_{safe_name2}.png\"\n",
    "            img_path = os.path.join(model_dir, 'interaction_2d_pdp', filename)\n",
    "            plt.savefig(img_path, dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "            plt.close('all')\n",
    "            \n",
    "            # ä¿å­˜2Däº¤äº’æ•°æ®åˆ°Excel\n",
    "            interaction_excel_info = {\n",
    "                'ç‰¹å¾1': feature1_name,\n",
    "                'ç‰¹å¾2': feature2_name,\n",
    "                'ç½‘æ ¼åˆ†è¾¨ç‡': f\"{Z.shape[0]}Ã—{Z.shape[1]}\",\n",
    "                'æœ€å°PDPå€¼': np.min(Z),\n",
    "                'æœ€å¤§PDPå€¼': np.max(Z),\n",
    "                'PDPå˜åŒ–å¹…åº¦': np.max(Z) - np.min(Z),\n",
    "                'äº¤äº’å¼ºåº¦(æ–¹å·®)': np.var(Z),\n",
    "                'äº¤äº’æ’å': comb_idx + 1,\n",
    "                'æ•°æ®çŠ¶æ€': 'ç»Ÿä¸€æ¸…æ´—å',\n",
    "                'æ ·æœ¬æ•°é‡': X_data.shape[0]\n",
    "            }\n",
    "            \n",
    "            excel_path = os.path.join(model_dir, 'interaction_2d_pdp', f\"2D_Interaction_PDP_Cleaned_{safe_name1}_{safe_name2}.xlsx\")\n",
    "            success = save_2d_interaction_data(feature1_values, feature2_values, Z, feature1_name, feature2_name, excel_path, interaction_excel_info)\n",
    "            \n",
    "            if success:\n",
    "                print(f\"      âœ“ 2Däº¤äº’ {feature1_name} Ã— {feature2_name} åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\")\n",
    "            else:\n",
    "                print(f\"      âš ï¸ 2Däº¤äº’ {feature1_name} Ã— {feature2_name} å›¾ç‰‡ä¿å­˜æˆåŠŸï¼Œæ•°æ®ä¿å­˜å¤±è´¥\")\n",
    "            \n",
    "            successful_interactions += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ äº¤äº’ {feature1_name} Ã— {feature2_name} åˆ†æå¤±è´¥: {str(e)}\")\n",
    "            plt.close('all')\n",
    "            continue\n",
    "    \n",
    "    # ç”Ÿæˆäº¤äº’å¼ºåº¦æ’åºæŠ¥å‘Š\n",
    "    if interaction_data:\n",
    "        try:\n",
    "            print(f\"    ç”Ÿæˆäº¤äº’å¼ºåº¦æ’åºæŠ¥å‘Š...\")\n",
    "            \n",
    "            # æŒ‰äº¤äº’å¼ºåº¦æ’åº\n",
    "            sorted_interactions = sorted(interaction_data.items(), \n",
    "                                       key=lambda x: x[1]['interaction_strength'], \n",
    "                                       reverse=True)\n",
    "            \n",
    "            # åˆ›å»ºäº¤äº’å¼ºåº¦å¯¹æ¯”å›¾\n",
    "            plt.figure(figsize=(16, 10))\n",
    "            \n",
    "            # å–å‰10ä¸ªæœ€å¼ºäº¤äº’\n",
    "            top_interactions = sorted_interactions[:min(10, len(sorted_interactions))]\n",
    "            \n",
    "            interaction_names = [name.replace('_Ã—_', ' Ã— ') for name, _ in top_interactions]\n",
    "            interaction_strengths = [data['interaction_strength'] for _, data in top_interactions]\n",
    "            \n",
    "            bars = plt.bar(range(len(interaction_names)), interaction_strengths, \n",
    "                          color=plt.cm.viridis(np.linspace(0, 1, len(interaction_names))))\n",
    "            \n",
    "            plt.xlabel('ç‰¹å¾äº¤äº’å¯¹', fontsize=14, fontweight='bold')\n",
    "            plt.ylabel('äº¤äº’å¼ºåº¦ (PDPæ–¹å·®)', fontsize=14, fontweight='bold')\n",
    "            plt.title(f'Top {len(top_interactions)} æœ€å¼º2Dç‰¹å¾äº¤äº’ï¼ˆç»Ÿä¸€æ¸…æ´—æ•°æ®ï¼‰\\næ€»å…±åˆ†æäº† {len(interaction_data)} ä¸ªäº¤äº’å¯¹', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "            plt.xticks(range(len(interaction_names)), interaction_names, rotation=45, ha='right')\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "            for i, (bar, strength) in enumerate(zip(bars, interaction_strengths)):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(interaction_strengths)*0.01, \n",
    "                        f'{strength:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # ä¿å­˜äº¤äº’å¼ºåº¦æ’åºå›¾\n",
    "            strength_img_path = os.path.join(model_dir, 'interaction_2d_pdp', 'Interaction_Strength_Ranking_Cleaned.png')\n",
    "            plt.savefig(strength_img_path, dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "            plt.close('all')\n",
    "            \n",
    "            # ä¿å­˜äº¤äº’å¼ºåº¦æ’åºæ•°æ®\n",
    "            ranking_data = []\n",
    "            for rank, (name, data) in enumerate(sorted_interactions, 1):\n",
    "                ranking_data.append({\n",
    "                    'æ’å': rank,\n",
    "                    'ç‰¹å¾äº¤äº’å¯¹': name.replace('_Ã—_', ' Ã— '),\n",
    "                    'ç‰¹å¾1': data['feature1_name'],\n",
    "                    'ç‰¹å¾2': data['feature2_name'],\n",
    "                    'äº¤äº’å¼ºåº¦': data['interaction_strength'],\n",
    "                    'æœ€å°PDPå€¼': np.min(data['partial_dependence_2d']),\n",
    "                    'æœ€å¤§PDPå€¼': np.max(data['partial_dependence_2d']),\n",
    "                    'PDPå˜åŒ–å¹…åº¦': np.max(data['partial_dependence_2d']) - np.min(data['partial_dependence_2d']),\n",
    "                    'æ•°æ®çŠ¶æ€': 'ç»Ÿä¸€æ¸…æ´—å'\n",
    "                })\n",
    "            \n",
    "            ranking_df = pd.DataFrame(ranking_data)\n",
    "            ranking_excel_path = os.path.join(model_dir, 'interaction_2d_pdp', 'All_2D_Interactions_Ranking_Cleaned.xlsx')\n",
    "            ranking_df.to_excel(ranking_excel_path, index=False)\n",
    "            \n",
    "            print(f\"      âœ“ äº¤äº’å¼ºåº¦æ’åºæŠ¥å‘Šå·²ä¿å­˜\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ äº¤äº’å¼ºåº¦æ’åºæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}\")\n",
    "            plt.close('all')\n",
    "    \n",
    "    print(f\"    âœ“ å®Œæ•´2Däº¤äº’åˆ†æå®Œæˆï¼šæˆåŠŸå¤„ç† {successful_interactions}/{total_combinations} ä¸ªäº¤äº’å¯¹\")\n",
    "    data_export['interaction_2d_pdp'] = interaction_data\n",
    "    return interaction_data\n",
    "\n",
    "def generate_selected_3d_interaction_pdp(model_info, model_dir, data_export, max_3d_interactions=None):\n",
    "    \"\"\"ç”Ÿæˆé€‰å®šçš„3Dç‰¹å¾äº¤äº’PDPåˆ†æ - ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—åçš„æ•°æ®\"\"\"\n",
    "    print(f\"  ç”Ÿæˆé€‰å®šçš„3Dç‰¹å¾äº¤äº’PDPåˆ†æï¼ˆä½¿ç”¨ç»Ÿä¸€æ¸…æ´—æ•°æ®ï¼‰...\")\n",
    "    \n",
    "    model = model_info['model_instance']\n",
    "    X_data = model_info['X_data_cleaned']  # ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®\n",
    "    y_data = model_info['y_data_cleaned']  # ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®\n",
    "    features = model_info['features']\n",
    "    \n",
    "    if max_3d_interactions is None:\n",
    "        max_3d_interactions = CONFIG['max_3d_interactions']\n",
    "    \n",
    "    # è®¡ç®—ç‰¹å¾é‡è¦æ€§æ¥é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾è¿›è¡Œ3Dåˆ†æ\n",
    "    importances, _ = calculate_feature_importance(model, X_data, y_data, features)\n",
    "    \n",
    "    # é€‰æ‹©é‡è¦æ€§æœ€é«˜çš„ç‰¹å¾è¿›è¡Œ3Däº¤äº’åˆ†æ\n",
    "    top_feature_indices = np.argsort(importances)[-min(6, len(features)):]  # æœ€å¤šå–6ä¸ªæœ€é‡è¦çš„ç‰¹å¾\n",
    "    top_features = [features[i] for i in top_feature_indices]\n",
    "    \n",
    "    # ç”Ÿæˆ3Dç»„åˆï¼ˆé™åˆ¶æ•°é‡ï¼‰\n",
    "    feature_3d_combinations = list(combinations(top_feature_indices, 3))\n",
    "    \n",
    "    # é™åˆ¶3Däº¤äº’æ•°é‡\n",
    "    if len(feature_3d_combinations) > max_3d_interactions:\n",
    "        # ä¼˜å…ˆé€‰æ‹©åŒ…å«æœ€é‡è¦ç‰¹å¾çš„ç»„åˆ\n",
    "        feature_3d_combinations = feature_3d_combinations[:max_3d_interactions]\n",
    "    \n",
    "    total_3d_combinations = len(feature_3d_combinations)\n",
    "    \n",
    "    print(f\"    é€‰æ‹©äº†å‰ {len(top_features)} ä¸ªæœ€é‡è¦ç‰¹å¾è¿›è¡Œ3Dåˆ†æ\")\n",
    "    print(f\"    æ€»å…±éœ€è¦åˆ†æ {total_3d_combinations} ä¸ª3Dç‰¹å¾äº¤äº’\")\n",
    "    print(f\"    3Dç½‘æ ¼åˆ†è¾¨ç‡: {CONFIG['interaction_3d_resolution']}Â³\")\n",
    "    print(f\"    ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—åæ•°æ®: {X_data.shape[0]} æ ·æœ¬\")\n",
    "    \n",
    "    if total_3d_combinations == 0:\n",
    "        print(f\"    âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„ç‰¹å¾è¿›è¡Œ3Däº¤äº’åˆ†æ\")\n",
    "        data_export['interaction_3d_pdp'] = {}\n",
    "        return {}\n",
    "    \n",
    "    interaction_3d_data = {}\n",
    "    successful_3d_interactions = 0\n",
    "    \n",
    "    for comb_idx, (i, j, k) in enumerate(feature_3d_combinations):\n",
    "        feature1_name = features[i]\n",
    "        feature2_name = features[j]\n",
    "        feature3_name = features[k]\n",
    "        \n",
    "        print(f\"    [{comb_idx+1}/{total_3d_combinations}] å¤„ç†3Däº¤äº’: {feature1_name} Ã— {feature2_name} Ã— {feature3_name}\")\n",
    "        \n",
    "        try:\n",
    "            # æ‰‹åŠ¨è®¡ç®—3D PDPï¼ˆsklearnçš„partial_dependenceåªæ”¯æŒ1Då’Œ2Dï¼‰\n",
    "            feature1_data = X_data[:, i]\n",
    "            feature2_data = X_data[:, j]\n",
    "            feature3_data = X_data[:, k]\n",
    "            \n",
    "            # åˆ›å»º3Dç½‘æ ¼\n",
    "            feature1_range = np.linspace(np.percentile(feature1_data, 1), np.percentile(feature1_data, 99), CONFIG['interaction_3d_resolution'])\n",
    "            feature2_range = np.linspace(np.percentile(feature2_data, 1), np.percentile(feature2_data, 99), CONFIG['interaction_3d_resolution'])\n",
    "            feature3_range = np.linspace(np.percentile(feature3_data, 1), np.percentile(feature3_data, 99), CONFIG['interaction_3d_resolution'])\n",
    "            \n",
    "            # è®¡ç®—3D PDP\n",
    "            Z_3d = np.zeros((len(feature1_range), len(feature2_range), len(feature3_range)))\n",
    "            \n",
    "            total_grid_points = len(feature1_range) * len(feature2_range) * len(feature3_range)\n",
    "            print(f\"      è®¡ç®— {total_grid_points} ä¸ª3Dç½‘æ ¼ç‚¹...\")\n",
    "            \n",
    "            for idx1, val1 in enumerate(feature1_range):\n",
    "                for idx2, val2 in enumerate(feature2_range):\n",
    "                    for idx3, val3 in enumerate(feature3_range):\n",
    "                        X_modified = X_data.copy()\n",
    "                        X_modified[:, i] = val1\n",
    "                        X_modified[:, j] = val2\n",
    "                        X_modified[:, k] = val3\n",
    "                        \n",
    "                        predictions = model.predict(X_modified)\n",
    "                        Z_3d[idx1, idx2, idx3] = np.mean(predictions)\n",
    "                \n",
    "                # è¿›åº¦æç¤º\n",
    "                if (idx1 + 1) % 5 == 0:\n",
    "                    progress = ((idx1 + 1) * len(feature2_range) * len(feature3_range)) / total_grid_points * 100\n",
    "                    print(f\"        3Dè®¡ç®—è¿›åº¦: {progress:.1f}%\")\n",
    "            \n",
    "            # ä¿å­˜3Däº¤äº’æ•°æ®\n",
    "            interaction_key = f\"{feature1_name}_Ã—_{feature2_name}_Ã—_{feature3_name}\"\n",
    "            interaction_3d_data[interaction_key] = {\n",
    "                'feature1_values': feature1_range,\n",
    "                'feature2_values': feature2_range,\n",
    "                'feature3_values': feature3_range,\n",
    "                'feature1_name': feature1_name,\n",
    "                'feature2_name': feature2_name,\n",
    "                'feature3_name': feature3_name,\n",
    "                'partial_dependence_3d': Z_3d,\n",
    "                'interaction_strength_3d': np.var(Z_3d)\n",
    "            }\n",
    "            \n",
    "            # åˆ›å»º3Då¯è§†åŒ–ï¼ˆå¤šä¸ªåˆ‡ç‰‡å›¾ï¼‰\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "            axes = axes.ravel()\n",
    "            \n",
    "            # ç”Ÿæˆ6ä¸ªä¸åŒçš„åˆ‡ç‰‡\n",
    "            slice_indices = [0, len(feature3_range)//4, len(feature3_range)//2, \n",
    "                           3*len(feature3_range)//4, len(feature3_range)-1]\n",
    "            slice_indices = slice_indices[:6]  # ç¡®ä¿åªæœ‰6ä¸ªåˆ‡ç‰‡\n",
    "            \n",
    "            for slice_idx, ax in enumerate(axes):\n",
    "                if slice_idx < len(slice_indices):\n",
    "                    f3_idx = slice_indices[slice_idx]\n",
    "                    f3_val = feature3_range[f3_idx]\n",
    "                    \n",
    "                    # åˆ›å»º2Dç½‘æ ¼ç”¨äºè¿™ä¸ªåˆ‡ç‰‡\n",
    "                    XX, YY = np.meshgrid(feature2_range, feature1_range)\n",
    "                    Z_slice = Z_3d[:, :, f3_idx]\n",
    "                    \n",
    "                    # ç»˜åˆ¶çƒ­åŠ›å›¾\n",
    "                    im = ax.contourf(XX, YY, Z_slice, levels=20, cmap='RdYlBu_r')\n",
    "                    fig.colorbar(im, ax=ax)\n",
    "                    \n",
    "                    # æ·»åŠ ç­‰é«˜çº¿\n",
    "                    contours = ax.contour(XX, YY, Z_slice, levels=8, colors='black', alpha=0.6, linewidths=0.5)\n",
    "                    \n",
    "                    ax.set_xlabel(feature2_name, fontsize=10)\n",
    "                    ax.set_ylabel(feature1_name, fontsize=10)\n",
    "                    ax.set_title(f'{feature3_name}={f3_val:.3f}', fontsize=10, fontweight='bold')\n",
    "                else:\n",
    "                    ax.axis('off')\n",
    "            \n",
    "            plt.suptitle(f'3Däº¤äº’PDPåˆ‡ç‰‡å›¾ï¼ˆç»Ÿä¸€æ¸…æ´—æ•°æ®ï¼‰: {feature1_name} Ã— {feature2_name} Ã— {feature3_name}', \n",
    "                        fontsize=16, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # ä¿å­˜3Däº¤äº’å›¾\n",
    "            safe_name1 = safe_filename(feature1_name)\n",
    "            safe_name2 = safe_filename(feature2_name)\n",
    "            safe_name3 = safe_filename(feature3_name)\n",
    "            filename = f\"3D_Interaction_PDP_Cleaned_{safe_name1}_{safe_name2}_{safe_name3}.png\"\n",
    "            img_path = os.path.join(model_dir, 'interaction_3d_pdp', filename)\n",
    "            plt.savefig(img_path, dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "            plt.close('all')\n",
    "            \n",
    "            # ä¿å­˜3Däº¤äº’æ•°æ®åˆ°Excel\n",
    "            interaction_3d_excel_info = {\n",
    "                'ç‰¹å¾1': feature1_name,\n",
    "                'ç‰¹å¾2': feature2_name,\n",
    "                'ç‰¹å¾3': feature3_name,\n",
    "                '3Dç½‘æ ¼åˆ†è¾¨ç‡': f\"{Z_3d.shape[0]}Ã—{Z_3d.shape[1]}Ã—{Z_3d.shape[2]}\",\n",
    "                'æœ€å°PDPå€¼': np.min(Z_3d),\n",
    "                'æœ€å¤§PDPå€¼': np.max(Z_3d),\n",
    "                'PDPå˜åŒ–å¹…åº¦': np.max(Z_3d) - np.min(Z_3d),\n",
    "                '3Däº¤äº’å¼ºåº¦': np.var(Z_3d),\n",
    "                '3Däº¤äº’æ’å': comb_idx + 1,\n",
    "                'æ•°æ®ç‚¹æ€»æ•°': Z_3d.size,\n",
    "                'æ•°æ®çŠ¶æ€': 'ç»Ÿä¸€æ¸…æ´—å',\n",
    "                'æ ·æœ¬æ•°é‡': X_data.shape[0]\n",
    "            }\n",
    "            \n",
    "            excel_path = os.path.join(model_dir, 'interaction_3d_pdp', f\"3D_Interaction_PDP_Cleaned_{safe_name1}_{safe_name2}_{safe_name3}.xlsx\")\n",
    "            success = save_3d_interaction_data(feature1_range, feature2_range, feature3_range, Z_3d, \n",
    "                                             feature1_name, feature2_name, feature3_name, excel_path, interaction_3d_excel_info)\n",
    "            \n",
    "            if success:\n",
    "                print(f\"      âœ“ 3Däº¤äº’ {feature1_name} Ã— {feature2_name} Ã— {feature3_name} åˆ†æå’Œæ•°æ®ä¿å­˜æˆåŠŸ\")\n",
    "            else:\n",
    "                print(f\"      âš ï¸ 3Däº¤äº’ {feature1_name} Ã— {feature2_name} Ã— {feature3_name} å›¾ç‰‡ä¿å­˜æˆåŠŸï¼Œæ•°æ®ä¿å­˜å¤±è´¥\")\n",
    "            \n",
    "            successful_3d_interactions += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ 3Däº¤äº’ {feature1_name} Ã— {feature2_name} Ã— {feature3_name} åˆ†æå¤±è´¥: {str(e)}\")\n",
    "            plt.close('all')\n",
    "            continue\n",
    "    \n",
    "    # ç”Ÿæˆ3Däº¤äº’å¼ºåº¦æ’åºæŠ¥å‘Š\n",
    "    if interaction_3d_data:\n",
    "        try:\n",
    "            print(f\"    ç”Ÿæˆ3Däº¤äº’å¼ºåº¦æ’åºæŠ¥å‘Š...\")\n",
    "            \n",
    "            # æŒ‰3Däº¤äº’å¼ºåº¦æ’åº\n",
    "            sorted_3d_interactions = sorted(interaction_3d_data.items(), \n",
    "                                          key=lambda x: x[1]['interaction_strength_3d'], \n",
    "                                          reverse=True)\n",
    "            \n",
    "            # åˆ›å»º3Däº¤äº’å¼ºåº¦å¯¹æ¯”å›¾\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            \n",
    "            interaction_3d_names = [name.replace('_Ã—_', ' Ã— ') for name, _ in sorted_3d_interactions]\n",
    "            interaction_3d_strengths = [data['interaction_strength_3d'] for _, data in sorted_3d_interactions]\n",
    "            \n",
    "            bars = plt.bar(range(len(interaction_3d_names)), interaction_3d_strengths, \n",
    "                          color=plt.cm.plasma(np.linspace(0, 1, len(interaction_3d_names))))\n",
    "            \n",
    "            plt.xlabel('3Dç‰¹å¾äº¤äº’ç»„åˆ', fontsize=14, fontweight='bold')\n",
    "            plt.ylabel('3Däº¤äº’å¼ºåº¦ (PDPæ–¹å·®)', fontsize=14, fontweight='bold')\n",
    "            plt.title(f'æ‰€æœ‰3Dç‰¹å¾äº¤äº’å¼ºåº¦æ’åºï¼ˆç»Ÿä¸€æ¸…æ´—æ•°æ®ï¼‰\\næˆåŠŸåˆ†æäº† {len(interaction_3d_data)} ä¸ª3Däº¤äº’', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "            plt.xticks(range(len(interaction_3d_names)), interaction_3d_names, rotation=45, ha='right')\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "            for i, (bar, strength) in enumerate(zip(bars, interaction_3d_strengths)):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(interaction_3d_strengths)*0.01, \n",
    "                        f'{strength:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # ä¿å­˜3Däº¤äº’å¼ºåº¦æ’åºå›¾\n",
    "            strength_3d_img_path = os.path.join(model_dir, 'interaction_3d_pdp', '3D_Interaction_Strength_Ranking_Cleaned.png')\n",
    "            plt.savefig(strength_3d_img_path, dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "            plt.close('all')\n",
    "            \n",
    "            # ä¿å­˜3Däº¤äº’å¼ºåº¦æ’åºæ•°æ®\n",
    "            ranking_3d_data = []\n",
    "            for rank, (name, data) in enumerate(sorted_3d_interactions, 1):\n",
    "                ranking_3d_data.append({\n",
    "                    'æ’å': rank,\n",
    "                    '3Dç‰¹å¾äº¤äº’ç»„åˆ': name.replace('_Ã—_', ' Ã— '),\n",
    "                    'ç‰¹å¾1': data['feature1_name'],\n",
    "                    'ç‰¹å¾2': data['feature2_name'],\n",
    "                    'ç‰¹å¾3': data['feature3_name'],\n",
    "                    '3Däº¤äº’å¼ºåº¦': data['interaction_strength_3d'],\n",
    "                    'æœ€å°PDPå€¼': np.min(data['partial_dependence_3d']),\n",
    "                    'æœ€å¤§PDPå€¼': np.max(data['partial_dependence_3d']),\n",
    "                    'PDPå˜åŒ–å¹…åº¦': np.max(data['partial_dependence_3d']) - np.min(data['partial_dependence_3d']),\n",
    "                    'æ•°æ®ç‚¹æ€»æ•°': data['partial_dependence_3d'].size,\n",
    "                    'æ•°æ®çŠ¶æ€': 'ç»Ÿä¸€æ¸…æ´—å'\n",
    "                })\n",
    "            \n",
    "            ranking_3d_df = pd.DataFrame(ranking_3d_data)\n",
    "            ranking_3d_excel_path = os.path.join(model_dir, 'interaction_3d_pdp', 'All_3D_Interactions_Ranking_Cleaned.xlsx')\n",
    "            ranking_3d_df.to_excel(ranking_3d_excel_path, index=False)\n",
    "            \n",
    "            print(f\"      âœ“ 3Däº¤äº’å¼ºåº¦æ’åºæŠ¥å‘Šå·²ä¿å­˜\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ 3Däº¤äº’å¼ºåº¦æ’åºæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}\")\n",
    "            plt.close('all')\n",
    "    \n",
    "    print(f\"    âœ“ 3Däº¤äº’åˆ†æå®Œæˆï¼šæˆåŠŸå¤„ç† {successful_3d_interactions}/{total_3d_combinations} ä¸ª3Däº¤äº’\")\n",
    "    data_export['interaction_3d_pdp'] = interaction_3d_data\n",
    "    return interaction_3d_data\n",
    "\n",
    "def analyze_gbdt_model():\n",
    "    \"\"\"åˆ†æGBDTæ¨¡å‹ - ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†ç‰ˆæœ¬\"\"\"\n",
    "    print(f\"\\n[1/1] åˆ†æGBDTæ¨¡å‹: PCC_k=5_GradientBoosting\")\n",
    "    print(f\"  ç‰¹å¾: {', '.join(CONFIG['selected_features'])}\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹å’Œæ•°æ®\n",
    "    best_models, all_features_name, scaler, data_info = load_gbdt_model_and_data()\n",
    "    model_info = best_models[0]  # åªæœ‰ä¸€ä¸ªæ¨¡å‹\n",
    "    \n",
    "    print(f\"  æ€§èƒ½: RÂ²={model_info['test_r2']:.6f}, MAE={model_info['test_mae']:.6f}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºè‡ªé€‚åº”é…ç½®\n",
    "    adaptive_config = get_adaptive_config(model_info['X_data'].shape[0])\n",
    "    print(f\"  æ•°æ®é›†å¤§å°: {model_info['X_data'].shape[0]}\")\n",
    "    print(f\"  ç½‘æ ¼åˆ†è¾¨ç‡: {CONFIG['grid_resolution']}ç‚¹ (é«˜ç²¾åº¦å…‰æ»‘æ›²çº¿)\")\n",
    "    print(f\"  è‡ªé€‚åº”é…ç½®: Bootstrap {adaptive_config['bootstrap_samples']}æ¬¡\")\n",
    "    print(f\"  ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†: {'å¯ç”¨' if CONFIG['outlier_detection']['enable'] else 'ç¦ç”¨'}\")\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å‹ç›®å½•\n",
    "    model_dir, model_name = create_model_directory(model_info)\n",
    "    print(f\"  è¾“å‡ºç›®å½•: {model_dir}\")\n",
    "    \n",
    "    # å­˜å‚¨æ‰€æœ‰å¯¼å‡ºæ•°æ®\n",
    "    data_export = {}\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nå¼€å§‹ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†å’ŒPDPåˆ†æ...\")\n",
    "        \n",
    "        # === æ­¥éª¤1: ç»Ÿä¸€å¼‚å¸¸å€¼æ£€æµ‹å’Œæ•°æ®æ¸…æ´— ===\n",
    "        X_original = model_info['X_data']\n",
    "        y_original = model_info['y_data']\n",
    "        features = model_info['features']\n",
    "        model = model_info['model_instance']\n",
    "        \n",
    "        if CONFIG['outlier_detection']['enable']:\n",
    "            print(f\"\\n1ï¸âƒ£ ç»Ÿä¸€å¼‚å¸¸å€¼æ£€æµ‹å’Œæ•°æ®æ¸…æ´—\")\n",
    "            \n",
    "            # åˆ›å»ºç»Ÿä¸€å¼‚å¸¸å€¼æ£€æµ‹å™¨\n",
    "            outlier_detector = UnifiedOutlierDetector(CONFIG['outlier_detection'])\n",
    "            \n",
    "            # æ‰§è¡Œå…¨å±€å¼‚å¸¸å€¼æ£€æµ‹\n",
    "            global_outlier_mask, global_outlier_report = outlier_detector.detect_global_outliers(\n",
    "                X_original, y_original, features, model\n",
    "            )\n",
    "            \n",
    "            # åº”ç”¨æ•°æ®æ¸…æ´—\n",
    "            X_cleaned, y_cleaned, removed_indices = outlier_detector.apply_cleaning(X_original, y_original)\n",
    "            \n",
    "            # ä¿å­˜å…¨å±€å¼‚å¸¸å€¼æŠ¥å‘Š\n",
    "            outlier_detector.save_global_outlier_report(model_dir, features)\n",
    "            \n",
    "            # æ›´æ–°æ¨¡å‹ä¿¡æ¯ï¼Œæ·»åŠ æ¸…æ´—åçš„æ•°æ®\n",
    "            model_info['X_data_cleaned'] = X_cleaned\n",
    "            model_info['y_data_cleaned'] = y_cleaned\n",
    "            model_info['removed_indices'] = removed_indices\n",
    "            model_info['outlier_detector'] = outlier_detector\n",
    "            \n",
    "            outliers_removed = len(removed_indices)\n",
    "            print(f\"    ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†å®Œæˆï¼šç§»é™¤ {outliers_removed} ä¸ªå¼‚å¸¸æ ·æœ¬\")\n",
    "            \n",
    "            # ç”Ÿæˆæ¸…æ´—å‰åæ•°æ®å¯¹æ¯”\n",
    "            if outliers_removed > 0 and CONFIG['outlier_detection']['comparison_analysis']:\n",
    "                print(f\"    ç”Ÿæˆæ¸…æ´—å‰åæ•°æ®å¯¹æ¯”åˆ†æ...\")\n",
    "                for i, feature in enumerate(features):\n",
    "                    compare_pdp_before_after_cleaning(\n",
    "                        model, X_original, X_cleaned, i, feature, model_dir, outliers_removed\n",
    "                    )\n",
    "        else:\n",
    "            print(f\"\\n1ï¸âƒ£ è·³è¿‡å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆå·²ç¦ç”¨ï¼‰\")\n",
    "            # å¦‚æœç¦ç”¨å¼‚å¸¸å€¼æ£€æµ‹ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®\n",
    "            model_info['X_data_cleaned'] = X_original\n",
    "            model_info['y_data_cleaned'] = y_original\n",
    "            model_info['removed_indices'] = np.array([])\n",
    "            model_info['outlier_detector'] = None\n",
    "            outliers_removed = 0\n",
    "        \n",
    "        # === æ­¥éª¤2: ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—åæ•°æ®è¿›è¡Œæ‰€æœ‰PDPåˆ†æ ===\n",
    "        print(f\"\\n2ï¸âƒ£ ç»Ÿä¸€æ¸…æ´—æ•°æ®PDPåˆ†æï¼ˆIndividual + ç½®ä¿¡åŒºé—´ï¼‰\")\n",
    "        combined_data = generate_combined_pdp_analysis(model_info, model_dir, data_export)\n",
    "        \n",
    "        if not combined_data:\n",
    "            print(f\"ç»Ÿä¸€æ¸…æ´—PDPåˆ†æå¤±è´¥ï¼Œè·³è¿‡åç»­åˆ†æ\")\n",
    "            return False\n",
    "        \n",
    "        # === æ­¥éª¤3: ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—åæ•°æ®è¿›è¡Œ2Däº¤äº’åˆ†æ ===\n",
    "        print(f\"\\n3ï¸âƒ£ ç»Ÿä¸€æ¸…æ´—æ•°æ®2Dç‰¹å¾äº¤äº’PDPåˆ†æï¼ˆæ‰€æœ‰ç‰¹å¾å¯¹ï¼‰\")\n",
    "        interaction_2d_data = generate_complete_2d_interaction_pdp(model_info, model_dir, data_export)\n",
    "        \n",
    "        # === æ­¥éª¤4: ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—åæ•°æ®è¿›è¡Œ3Däº¤äº’åˆ†æ ===\n",
    "        print(f\"\\n4ï¸âƒ£ ç»Ÿä¸€æ¸…æ´—æ•°æ®3Dç‰¹å¾äº¤äº’PDPåˆ†æï¼ˆé‡è¦ç‰¹å¾ç»„åˆï¼‰\")\n",
    "        interaction_3d_data = generate_selected_3d_interaction_pdp(model_info, model_dir, data_export)\n",
    "        \n",
    "        print(f\"\\nGBDTæ¨¡å‹ {model_name} ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†åˆ†æå®Œæˆï¼\")\n",
    "        \n",
    "        # è¾“å‡ºåˆ†æç»“æœç»Ÿè®¡\n",
    "        print(f\"\\nåˆ†æç»“æœç»Ÿè®¡:\")\n",
    "        print(f\"â€¢ åŸå§‹æ ·æœ¬æ•°: {X_original.shape[0]}\")\n",
    "        print(f\"â€¢ å¼‚å¸¸å€¼æ£€æµ‹: {'å·²æ‰§è¡Œ' if CONFIG['outlier_detection']['enable'] else 'æœªæ‰§è¡Œ'}\")\n",
    "        print(f\"â€¢ å¼‚å¸¸å€¼æ¸…æ´—: ç§»é™¤ {outliers_removed} ä¸ªæ ·æœ¬\")\n",
    "        print(f\"â€¢ æ¸…æ´—åæ ·æœ¬æ•°: {model_info['X_data_cleaned'].shape[0]}\")\n",
    "        print(f\"â€¢ æ•°æ®ä¿ç•™ç‡: {model_info['X_data_cleaned'].shape[0]/X_original.shape[0]*100:.2f}%\")\n",
    "        print(f\"â€¢ åˆ†æç‰¹å¾æ•°: {len(combined_data)}\")\n",
    "        print(f\"â€¢ ç½‘æ ¼åˆ†è¾¨ç‡: {CONFIG['grid_resolution']}ç‚¹ (é«˜ç²¾åº¦)\")\n",
    "        print(f\"â€¢ 2Däº¤äº’åˆ†æ: {len(interaction_2d_data)} å¯¹ (ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—æ•°æ®)\")\n",
    "        print(f\"â€¢ 3Däº¤äº’åˆ†æ: {len(interaction_3d_data)} ç»„ (ä½¿ç”¨ç»Ÿä¸€æ¸…æ´—æ•°æ®)\")\n",
    "        print(f\"â€¢ æ•°æ®ä¸€è‡´æ€§: æ‰€æœ‰åˆ†æä½¿ç”¨ç›¸åŒçš„æ¸…æ´—åæ•°æ®\")\n",
    "        print(f\"â€¢ æ¸…æ´—å‰åå¯¹æ¯”: {'å·²ç”Ÿæˆ' if CONFIG['outlier_detection']['comparison_analysis'] else 'æœªç”Ÿæˆ'}\")\n",
    "        print(f\"â€¢ å…¨å±€å¼‚å¸¸å€¼æŠ¥å‘Š: {'å·²ç”Ÿæˆ' if CONFIG['outlier_detection']['save_outlier_reports'] else 'æœªç”Ÿæˆ'}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nGBDTæ¨¡å‹ {model_name} ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†åˆ†æå¤±è´¥: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GBDTæ¨¡å‹PDPåˆ†æç¨‹åºï¼ˆç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†ç‰ˆï¼‰\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # åˆå§‹æ¸…ç†\n",
    "    cleanup_matplotlib()\n",
    "    \n",
    "    print(f\"è¾“å‡ºç›®å½•: {CONFIG['pdp_output_dir']}\")\n",
    "    print(f\"åˆ†æç±»å‹: GBDTæ¨¡å‹ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç† + å®Œæ•´PDPå’Œäº¤äº’åˆ†æ\")\n",
    "    print(f\"matplotlibåç«¯: {matplotlib.get_backend()}\")\n",
    "    print(f\"äº¤äº’æ¨¡å¼: {'å¼€å¯' if matplotlib.is_interactive() else 'å…³é—­'}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†é…ç½®\n",
    "    print(f\"\\nç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†é…ç½®:\")\n",
    "    print(f\"â€¢ ç½‘æ ¼åˆ†è¾¨ç‡: {CONFIG['grid_resolution']}ç‚¹ (Individual PDP)\")\n",
    "    print(f\"â€¢ 2Däº¤äº’åˆ†è¾¨ç‡: {CONFIG['interaction_2d_resolution']}Ã—{CONFIG['interaction_2d_resolution']} (æ‰€æœ‰ç‰¹å¾å¯¹)\")\n",
    "    print(f\"â€¢ 3Däº¤äº’åˆ†è¾¨ç‡: {CONFIG['interaction_3d_resolution']}Â³ (é‡è¦ç‰¹å¾ç»„åˆ)\")\n",
    "    print(f\"â€¢ æœ€å¤§3Däº¤äº’æ•°: {CONFIG['max_3d_interactions']} (è®¡ç®—æ•ˆç‡è€ƒè™‘)\")\n",
    "    print(f\"â€¢ Bootstrapæ¬¡æ•°: å°æ•°æ®é›†{CONFIG['bootstrap_samples_small']}, ä¸­ç­‰{CONFIG['bootstrap_samples']}, å¤§å‹{CONFIG['bootstrap_samples_large']}\")\n",
    "    print(f\"â€¢ ç½®ä¿¡æ°´å¹³: {CONFIG['confidence_level']*100}%\")\n",
    "    print(f\"â€¢ ç›´æ–¹å›¾ç®±æ•°: {CONFIG['histogram_bins']} (ç‰¹å¾åˆ†å¸ƒ)\")\n",
    "    \n",
    "    # ç»Ÿä¸€å¼‚å¸¸å€¼æ£€æµ‹é…ç½®\n",
    "    outlier_config = CONFIG['outlier_detection']\n",
    "    print(f\"\\nç»Ÿä¸€å¼‚å¸¸å€¼æ£€æµ‹é…ç½®:\")\n",
    "    print(f\"â€¢ å¼‚å¸¸å€¼æ£€æµ‹: {'å¯ç”¨' if outlier_config['enable'] else 'ç¦ç”¨'}\")\n",
    "    if outlier_config['enable']:\n",
    "        print(f\"â€¢ ç»Ÿä¸€æ¸…æ´—: {'å¯ç”¨' if outlier_config['unified_cleaning'] else 'ç¦ç”¨'}\")\n",
    "        print(f\"â€¢ è‡ªåŠ¨æ¸…æ´—: {'æ˜¯' if outlier_config['auto_clean'] else 'å¦'}\")\n",
    "        print(f\"â€¢ å¤šç‰¹å¾è”åˆæ£€æµ‹: {'å¯ç”¨' if outlier_config['multi_feature_detection'] else 'ç¦ç”¨'}\")\n",
    "        print(f\"â€¢ å¼‚å¸¸å€¼æ¯”ä¾‹å‡è®¾: {outlier_config['contamination']*100}%\")\n",
    "        print(f\"â€¢ æœ€å°‘æŠ•ç¥¨æ•°: {outlier_config['min_votes']}\")\n",
    "        print(f\"â€¢ Z-Scoreé˜ˆå€¼: {outlier_config['z_score_threshold']}\")\n",
    "        print(f\"â€¢ IQRå€æ•°: {outlier_config['iqr_multiplier']}\")\n",
    "        \n",
    "        target_features = outlier_config.get('target_features', [])\n",
    "        if target_features:\n",
    "            print(f\"â€¢ é‡ç‚¹æ£€æµ‹ç‰¹å¾: {', '.join(target_features)}\")\n",
    "        else:\n",
    "            print(f\"â€¢ é‡ç‚¹æ£€æµ‹ç‰¹å¾: æ‰€æœ‰ç‰¹å¾\")\n",
    "        \n",
    "        print(f\"â€¢ ä¿å­˜å¼‚å¸¸å€¼æŠ¥å‘Š: {'æ˜¯' if outlier_config['save_outlier_reports'] else 'å¦'}\")\n",
    "        print(f\"â€¢ å¯¹æ¯”åˆ†æ: {'æ˜¯' if outlier_config['comparison_analysis'] else 'å¦'}\")\n",
    "    \n",
    "    # åˆ›å»ºä¸»è¾“å‡ºç›®å½•\n",
    "    os.makedirs(CONFIG['pdp_output_dir'], exist_ok=True)\n",
    "    \n",
    "    # åˆ†æGBDTæ¨¡å‹\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    success = analyze_gbdt_model()\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"GBDTæ¨¡å‹PDPåˆ†æå®Œæˆï¼\")\n",
    "        print(f\"=\"*80)\n",
    "        print(f\"è¾“å‡ºç›®å½•: {CONFIG['pdp_output_dir']}\")\n",
    "        \n",
    "        print(f\"\\nè¯¦ç»†ç»“æœä¿å­˜åœ¨ä»¥ä¸‹ç›®å½•ä¸­:\")\n",
    "        print(f\"  â€¢ combined_pdp/ (ç»Ÿä¸€æ¸…æ´—PDPåˆ†æ)\")\n",
    "        print(f\"  â€¢ interaction_2d_pdp/ (ç»Ÿä¸€æ¸…æ´—2Däº¤äº’åˆ†æ)\")\n",
    "        print(f\"  â€¢ interaction_3d_pdp/ (ç»Ÿä¸€æ¸…æ´—3Däº¤äº’åˆ†æ)\")\n",
    "        print(f\"  â€¢ outlier_detection/ (å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹æŠ¥å‘Š)\")\n",
    "        print(f\"  â€¢ cleaned_analysis/ (æ¸…æ´—å‰åå¯¹æ¯”)\")\n",
    "        \n",
    "        print(f\"\\nç”Ÿæˆçš„ç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†åˆ†æç±»å‹:\")\n",
    "        print(f\"  â€¢ ç»Ÿä¸€æ¸…æ´—Individual PDPå›¾ï¼š{CONFIG['grid_resolution']}ç½‘æ ¼ç‚¹ + Excelæ•°æ®\")\n",
    "        print(f\"  â€¢ ç»Ÿä¸€æ¸…æ´—2Däº¤äº’å›¾ï¼š{CONFIG['interaction_2d_resolution']}Ã—{CONFIG['interaction_2d_resolution']} æ‰€æœ‰ç‰¹å¾å¯¹ + Excelæ•°æ®\")\n",
    "        print(f\"  â€¢ ç»Ÿä¸€æ¸…æ´—3Däº¤äº’å›¾ï¼š{CONFIG['interaction_3d_resolution']}Â³ é‡è¦ç‰¹å¾ç»„åˆ + åˆ‡ç‰‡å¯è§†åŒ– + Excelæ•°æ®\")\n",
    "        print(f\"  â€¢ å…¨å±€å¼‚å¸¸å€¼æ£€æµ‹æŠ¥å‘Šï¼šå¤šç§æ£€æµ‹æ–¹æ³• + å¤šç‰¹å¾è”åˆ + æŠ•ç¥¨æœºåˆ¶ + Excelæ•°æ®\")\n",
    "        print(f\"  â€¢ æ¸…æ´—å‰åå¯¹æ¯”ï¼šPDPæ›²çº¿+åˆ†å¸ƒ+æ–œç‡+ç»Ÿè®¡å¯¹æ¯” + Excelæ•°æ®\")\n",
    "        print(f\"  â€¢ äº¤äº’å¼ºåº¦æ’åºï¼š2Då’Œ3Däº¤äº’çš„å¼ºåº¦åˆ†æå’Œæ’åºï¼ˆåŸºäºç»Ÿä¸€æ¸…æ´—æ•°æ®ï¼‰\")\n",
    "        \n",
    "        print(f\"\\nç»Ÿä¸€å¼‚å¸¸å€¼å¤„ç†æ ¸å¿ƒæ”¹è¿›:\")\n",
    "        print(f\"  â€¢ å…¨å±€ç»Ÿä¸€å¼‚å¸¸å€¼æ£€æµ‹ (ä¸€æ¬¡æ£€æµ‹ï¼Œå…¨å±€åº”ç”¨)\")\n",
    "        print(f\"  â€¢ å¤šç‰¹å¾è”åˆå¼‚å¸¸å€¼æ£€æµ‹ (é©¬æ°è·ç¦»+å¤šç»´IsolationForest)\")\n",
    "        print(f\"  â€¢ æ™ºèƒ½æŠ•ç¥¨æœºåˆ¶ (å¤šç§æ£€æµ‹æ–¹æ³•ç»¼åˆå†³ç­–)\")\n",
    "        print(f\"  â€¢ æ•°æ®ä¸€è‡´æ€§ä¿è¯ (æ‰€æœ‰PDPå’Œäº¤äº’åˆ†æä½¿ç”¨ç›¸åŒçš„æ¸…æ´æ•°æ®)\")\n",
    "        print(f\"  â€¢ é’ˆå¯¹æ€§Î©ç‰¹å¾å¤„ç† (é‡ç‚¹æ£€æµ‹åˆ—è¡¨å¯é…ç½®)\")\n",
    "        print(f\"  â€¢ PDPè·³è·ƒä¸“é¡¹æ£€æµ‹ (ä¸“é—¨è§£å†³å¼‚å¸¸è·³è·ƒé—®é¢˜)\")\n",
    "        print(f\"  â€¢ å®Œæ•´å¯è§†åŒ–æŠ¥å‘Š (å¼‚å¸¸å€¼åˆ†å¸ƒ+æŠ•ç¥¨+æ¸…æ´—æ•ˆæœ)\")\n",
    "        print(f\"  â€¢ æ¸…æ´—å‰åå®Œæ•´å¯¹æ¯” (PDPæ›²çº¿+åˆ†å¸ƒ+æ–œç‡+ç»Ÿè®¡å¯¹æ¯”)\")\n",
    "        \n",
    "        print(f\"\\né’ˆå¯¹Î©ç‰¹å¾å¼‚å¸¸è·³è·ƒé—®é¢˜çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ:\")\n",
    "        if 'Î©' in CONFIG['outlier_detection'].get('target_features', []) or not CONFIG['outlier_detection'].get('target_features', []):\n",
    "            print(f\"  â€¢ Î©ç‰¹å¾å·²åŒ…å«åœ¨ç»Ÿä¸€å¼‚å¸¸å€¼æ£€æµ‹èŒƒå›´å†…\")\n",
    "            print(f\"  â€¢ 6ç§å•ç‰¹å¾+2ç§å¤šç‰¹å¾æ–¹æ³•ä¼šç»¼åˆè¯†åˆ«Î©çš„å¼‚å¸¸å€¼\")\n",
    "            print(f\"  â€¢ PDPè·³è·ƒä¸“é¡¹æ£€æµ‹ä¼šæ‰¾åˆ°å¯¼è‡´è·³è·ƒçš„å…·ä½“æ•°æ®ç‚¹\")\n",
    "            print(f\"  â€¢ ç»Ÿä¸€æ¸…æ´—ä¼šåœ¨æ‰€æœ‰åˆ†æä¸­ç§»é™¤å¼‚å¸¸æ•°æ®ç‚¹\")\n",
    "            print(f\"  â€¢ æ¸…æ´—å‰åå¯¹æ¯”ä¼šæ˜¾ç¤ºÎ©ç‰¹å¾çš„æ”¹è¿›æ•ˆæœ\")\n",
    "            print(f\"  â€¢ å…¨å±€å¼‚å¸¸å€¼æŠ¥å‘Šä¼šè¯¦ç»†è®°å½•Î©ç‰¹å¾çš„æ£€æµ‹ç»“æœ\")\n",
    "            print(f\"  â€¢ æ‰€æœ‰PDPå’Œäº¤äº’åˆ†æéƒ½å°†ä½¿ç”¨æ¸…æ´—åçš„Î©ç‰¹å¾æ•°æ®\")\n",
    "        else:\n",
    "            print(f\"  â€¢ âš ï¸ Î©ç‰¹å¾ä¸åœ¨é‡ç‚¹æ£€æµ‹åˆ—è¡¨ä¸­\")\n",
    "            print(f\"  â€¢ å»ºè®®å°†'Î©'æ·»åŠ åˆ°CONFIG['outlier_detection']['target_features']ä¸­\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nGBDTæ¨¡å‹PDPåˆ†æå¤±è´¥\")\n",
    "    \n",
    "    # æœ€ç»ˆæ¸…ç†\n",
    "    cleanup_matplotlib()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(f\"\\næ‰€æœ‰èµ„æºå·²æ¸…ç†ï¼Œç¨‹åºæ‰§è¡Œå®Œæ¯•\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # æ‰§è¡Œä¸»ç¨‹åº\n",
    "    success = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
